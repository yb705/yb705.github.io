<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="数据预处理与缩放"><meta name="keywords" content="python,SVM,数据缩放"><meta name="author" content="Yyb,undefined"><meta name="copyright" content="Yyb"><title>数据预处理与缩放【Yyb的花园】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"VN5QYUXW8S","apiKey":"7bb2817029d8aaa580ce39e2aef50ce7","indexName":"search","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {"appId":"S3SM93qxXfn8olHkcUQnJuIp-gzGzoHsz","appKey":"dJ07LxI5XAsyjJ3DB7zmCUL3","placeholder":"走过路过不要错过,买不买的瞧一瞧啊!","pageSize":10},
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yyb的花园" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE"><span class="toc-number">1.</span> <span class="toc-text">机器学习-数据预处理与缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F"><span class="toc-number">1.1.</span> <span class="toc-text">序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">1.2.</span> <span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">预处理的四种类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.4.</span> <span class="toc-text">应用数据转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">替代方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Yyb</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://mp.csdn.net/console/article?spm=1010.2135.3001.5416" target="_blank">CSDN<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="1994yybsr@sina.com" target="_blank">E-Mail<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="https://github.com/yb705" target="_blank">GitHub<i class="icon-dot bg-color8"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">19</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">20</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">13</span></a></div><div class="friend-link"><a class="friend-link-text" href="https://github.com/yb705" target="_blank">不怎么用的github</a><a class="friend-link-text" href="https://i.csdn.net/#/user-center/profile?spm=1010.2135.3001.5111" target="_blank">定期更新的CSDN</a><a class="friend-link-text" href="1994yybsr@sina.com" target="_blank">只用来接收消息的邮箱</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">Yyb的花园</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">数据预处理与缩放</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2021-06-15 | 更新于 2021-06-15</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">无监督学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/">数据预处理</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/python/">python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/SVM/">SVM</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E6%95%B0%E6%8D%AE%E7%BC%A9%E6%94%BE/">数据缩放</a></div></div></div><div class="main-content"><h1 id="机器学习-数据预处理与缩放"><a href="#机器学习-数据预处理与缩放" class="headerlink" title="机器学习-数据预处理与缩放"></a>机器学习-数据预处理与缩放</h1><h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们在接触监督学习时了解到，有一些算法（譬如<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>）对于数据的缩放非常敏感。因此，通常的做法是对数据集进行调节，使得数据表示更适合于这些算法。通常来说，这是对数据特征的一种简单的缩放和移动。</p>
<p>机器学习的理论实际上是起源于概率论与数理统计，接下来，我们来简单提几个相关概念，来帮助大家更好地理解接下来的要说的几种处理方法。</p>
<span id="more"></span>

<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>中位数</strong>——对于一组数字来说，中位数指的是这样的数值x：有一半的数值小于x，另一半的数值大于x。如果数据集的数据个数是偶数，就取中间两个数值的平均数。</p>
<p><strong>四分位数</strong>——按照四分之一的数据个数来划分数据集。较小四分位数指的是这样的数值x：有四分之一的数值小于x。较大四分位数指的是这样的数值x：有四分之一的数值大于x。</p>
<p><strong>方差</strong>——衡量随机变量或一组数据的离散程度的度量。</p>
<p><strong>异常值</strong>——在数据集之中的那些与众不同的数据点。</p>
<p>需要注意的是<strong>异常值并不一定是误差值或者错误值</strong>。譬如说想要统计某辆公交车上20名乘客的平均年龄，有一种情况就是其中19名乘客是处于20岁到30岁之间，但是有一名乘客的年龄是70岁，那么这名乘客的年龄就是属于与众不同的数据点，是异常值，而不是误差值。并且这个数值会影响最终的统计计算结果——拉高公交车上乘客的平均年龄。</p>
<h2 id="预处理的四种类型"><a href="#预处理的四种类型" class="headerlink" title="预处理的四种类型"></a>预处理的四种类型</h2><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610115711512.jpg" alt="在这里插入图片描述"><br>scikit-learn中一共提供了4种预处理方法，变换效果分别如上图所示，接下来，结合图像，我们来详细说一下这四种变换。<br><del>PS：如果大家不喜欢下面的枯燥理论就直接跳过吧，说实话，我觉得只要会使用，知道用于什么地方就够了。</del> </p>
<p><strong>StandardScaler（标准化）</strong>：确保每个特征的<strong>平均值</strong>为0，<strong>方差</strong>为1，使特征值都位于同一量级。但这种缩放不能保证特征任何特定的最大值和最小值。（我曾经在讲解<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>的时候进行过标准化的人工处理，有源代码，感兴趣的朋友可以去看下，可以帮助大家更好地理解。）</p>
<p><strong>RobustScaler（剔除异常值）</strong>：RobustScaler也是一种标准化，工作原理与StandardScaler类似，确保每个特征的统计属性都处于同一范围。但是RobustScaler使用的是<strong>中位数</strong>和<strong>四分位数</strong>，而不是平均值和方差。这样RobustScaler会忽略与其它点有很大不同的数据点（<strong>异常值</strong>），减少异常值造成的麻烦。</p>
<p><strong>MinMaxScaler（归一化）</strong>：MinMaxScaler移动数据，使得所有特征都刚好位于0到1之间。对于二维数据集来说，所有的数据都包含在x轴0到1与y轴0到1组成的矩形中。（同样，我在讲解<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>的时候进行过归一化的人工处理，也有相关源代码。）</p>
<p><strong>Normalizer（正则化，有些地方也叫做归一化）</strong>：Normalizer用到的是一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于1。通过上面的第四幅小图可以看出：它<strong>将一个数据点投射到半径为1的圆上（对于更高维度的情况是球面）</strong>。这意味着每个数据点的缩放比例都不相同。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。</p>
<h2 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h2><p>接下来我们用数据集<a target="_blank" rel="noopener" href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115350097">SVM</a>算法来实际使用下，看看效果。（数据以及相关算法我们都在之前讲解的SVM中详细地讲过了，感兴趣的朋友可以点击超链接去看下，这里就不在赘述了。）</p>
<p>首先是导入数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> winreg</span><br><span class="line"><span class="comment">###################</span></span><br><span class="line">real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)</span><br><span class="line">file_address=winreg.QueryValueEx(real_address, <span class="string">&quot;Desktop&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">file_address+=<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">file_origin=file_address+<span class="string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span></span><br><span class="line">health=pd.read_csv(file_origin)</span><br><span class="line"><span class="comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span></span><br><span class="line"><span class="comment">###################</span></span><br></pre></td></tr></table></figure>

<p>划分训练集和测试集，并进行建模，精度评分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">train=health.drop([<span class="string">&quot;fetal_health&quot;</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="string">&quot;fetal_health&quot;</span>],random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span></span><br><span class="line">svm=svm.SVC(C=<span class="number">1</span>,kernel=<span class="string">&quot;rbf&quot;</span>,decision_function_shape=<span class="string">&quot;ovr&quot;</span>)</span><br><span class="line">svm.fit(X_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SVM待测模型评分：&quot;</span>+<span class="built_in">str</span>(accuracy_score(y_test,svm.predict(X_test))))</span><br></pre></td></tr></table></figure>

<p>结果如下所示：<br><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140210590.jpg" alt="在这里插入图片描述"></p>
<p>接下来我们对数据进行缩放再进行建模评分，看看模型精度有什么变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler<span class="comment">###标准化</span></span><br><span class="line">standard=StandardScaler()</span><br><span class="line">standard.fit(X_train)<span class="comment">###使用fit方法拟合缩放器，并将其应用于训练数据，其实就和之前学过的算法一样，先用fit去训练数据，适应数据</span></span><br><span class="line">X_train_scaled=standard.transform(X_train)<span class="comment">####对训练数据进行实际缩放，也是类似之前学习的训练过程，在，fit适用数据之后，再用transfrom去同等变换X_train,X_test</span></span><br><span class="line">X_test_scaled=standard.transform(X_test)<span class="comment">###注意测试集相对训练集来说移动必须是一致的，因为变换后数量级是不同的，但是要保证数据的分布形状要完全相同</span></span><br><span class="line">svm.fit(X_train_scaled,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标准化后SVM模型评分：&quot;</span>+<span class="built_in">str</span>(accuracy_score(y_test,svm.predict(X_test_scaled))))</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140415448.jpg" alt="在这里插入图片描述"><br>可以看到模型精度较之前有了提升。</p>
<p>整个处理过程与之前算法训练模型类似。首先使用<strong>fit</strong>方法拟合缩放器（scaler），并将其应用于训练数据。然后为了应用刚刚学习的数据（即对训练数据进行实际缩放），我们使用缩放器的<strong>transform</strong>方法。最后为了将SVM应用到缩放后的数据上，还需要对测试集进行变换。</p>
<p>需要注意的是，为了让监督模型能够在测试集上运行，<strong>对训练集和测试集应用完全相同的变换是很重要的</strong>。因为<strong>刻度数值可以不一样，但是必须要保证测试集与训练集的数据分布是一样的（可以结合上面的散点图来看一下）</strong>。</p>
<h2 id="替代方法"><a href="#替代方法" class="headerlink" title="替代方法"></a>替代方法</h2><p>通常来说，想要在某个数据集上fit一个模型，然后再将其transform，是一个非常常见的过程。但是可以用比先调fit再调transform更高效的方法来计算。对于这种使用场景，所有具有transform方法的模型也都有一个fit_transform方法，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standard=StandardScaler()</span><br><span class="line">X_train_scaled=standard.fit(X_train).transform(X_train)<span class="comment">###原方法先fit后transform</span></span><br><span class="line">X_train_scaled=standard.fit_transform(X_train)<span class="comment">###结果相同，但计算更加高效</span></span><br></pre></td></tr></table></figure>

<p>虽然fit_transform不一定对所有模型都更加高效，但在尝试变换训练集时，使用这一方法仍然是很少的。<br><del>PS：最主要是看起来上了点档次</del></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>对于任何一种类型的数据集或者是一个算法来说，没有绝对正确的预处理算法。<strong>预处理方法，数据集与建模算法这三者之间永远都不会存在绑定关系</strong>。 譬如我们分别用SVM和神经网络来测试上述四种预处理算法，代码及结果如下所示：</p>
<p><strong>SVM：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:</span><br><span class="line">    scaled=i</span><br><span class="line">    i.fit(X_train)</span><br><span class="line">    X_train_scaled=i.transform(X_train)</span><br><span class="line">    X_test_scaled=i.transform(X_test)</span><br><span class="line">    svm.fit(X_train_scaled,y_train)</span><br><span class="line">    score=accuracy_score(y_test,svm.predict(X_test_scaled))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(i)+<span class="string">&quot;处理后得分：&quot;</span>+<span class="built_in">str</span>(score))</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014311582.png" alt="在这里插入图片描述"><br><strong>神经网络（MLP）：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier<span class="comment">#多层感知机-MLP/神经网络</span></span><br><span class="line">mlp=MLPClassifier(solver=<span class="string">&quot;lbfgs&quot;</span>,random_state=<span class="number">1</span>,max_iter=<span class="number">100000</span>).fit(X_train,y_train)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:</span><br><span class="line">    scaled=i</span><br><span class="line">    i.fit(X_train)</span><br><span class="line">    X_train_scaled=i.transform(X_train)</span><br><span class="line">    X_test_scaled=i.transform(X_test)</span><br><span class="line">    mlp.fit(X_train_scaled,y_train)</span><br><span class="line">    score=accuracy_score(y_test,mlp.predict(X_test_scaled))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(i)+<span class="string">&quot;处理后得分：&quot;</span>+<span class="built_in">str</span>(score))</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014321875.png" alt="在这里插入图片描述"><br>可以看出，对于同一个数据集应用不同的算法，选择的处理方法是不一样的。对于SVM来说，用<strong>标准化</strong>处理精度会高一些，而对于神经网络来说，用<strong>归一化</strong>处理效果会更好。而如果我们对其它数据集进行机器学习的话，那么就会存在其它的选择。所以对于一份没有接触过的数据集来说，如果时间允许的话，可以尝试各种各样的组合，来去搭建精度最高的模型。</p>
<p>最后，虽然数据缩放不涉及任何复杂的数学，但良好的做法仍是使用scikit_learn提供的缩放机制，而不是自己人工实现它们，因为即使在这些简单的计算中也容易犯错。</p>
<p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yyb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://yb705.github.io/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/">https://yb705.github.io/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb705.github.io">Yyb的花园</a>！</span></div></div><div class="post-copyright valine" id="comments-container"><script src="//unpkg.com/valine@1.4.14/dist/Valine.min.js"></script><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2];
}
var flag = false;
var gitFun = function () {
    try {
        var valineObj = window.GLOBAL_CONFIG.valine;
        new Valine({
            el: "#comments-container",
            ...valineObj
        });
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/"><i class="fas fa-angle-left">&nbsp;</i><span>决策树集成-随机森林随机森林</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"><span>最小二乘法</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By Yyb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>