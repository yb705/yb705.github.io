<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="决策树集成-随机森林"><meta name="keywords" content="python,随机森林"><meta name="author" content="Yyb,undefined"><meta name="copyright" content="Yyb"><title>决策树集成-随机森林【Yyb的花园】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"VN5QYUXW8S","apiKey":"7bb2817029d8aaa580ce39e2aef50ce7","indexName":"search","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {"appId":"S3SM93qxXfn8olHkcUQnJuIp-gzGzoHsz","appKey":"dJ07LxI5XAsyjJ3DB7zmCUL3","placeholder":"走过路过不要错过,买不买的瞧一瞧啊!","pageSize":10},
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yyb的花园" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">决策树集成-随机森林算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E6%83%B3%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">思想简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.</span> <span class="toc-text">构造原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%93%8D%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.4.</span> <span class="toc-text">实操建模</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">1.6.</span> <span class="toc-text">分析特征重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.7.</span> <span class="toc-text">优缺点</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Yyb</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://mp.csdn.net/console/article?spm=1010.2135.3001.5416" target="_blank">CSDN<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="1994yybsr@sina.com" target="_blank">E-Mail<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="https://github.com/yb705" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">19</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">20</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">13</span></a></div><div class="friend-link"><a class="friend-link-text" href="https://github.com/yb705" target="_blank">不怎么用的github</a><a class="friend-link-text" href="https://i.csdn.net/#/user-center/profile?spm=1010.2135.3001.5111" target="_blank">定期更新的CSDN</a><a class="friend-link-text" href="1994yybsr@sina.com" target="_blank">只用来接收消息的邮箱</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">Yyb的花园</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">决策树集成-随机森林</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2021-06-15 | 更新于 2021-06-15</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90/">决策树集成</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/python/">python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">随机森林</a></div></div></div><div class="main-content"><h1 id="决策树集成-随机森林算法"><a href="#决策树集成-随机森林算法" class="headerlink" title="决策树集成-随机森林算法"></a>决策树集成-随机森林算法</h1><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>集成</strong></p>
<p>集成是合并多个机器学习模型来构建更强大模型的方法。在机器学习算法中有许多模型属于这一类，但已证明有两种集成模型对大量分类和回归的数据集都是有效的，二者都以决策树为基础，分别是<strong>随机森林（random forest）</strong>和<strong>梯度提升决策树决策（gradiet boosted decision tree）</strong>。</p>
<p>本片文章先讲解一下随机森林。在了解随机森林之前建议先去看一下我的另外两篇讲解<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树的文章决策树算法之讲解实操（上）</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115939923">决策树算法之讲解实操（下）</a>，重复的东西，我这里就不在赘述了。<br>ps：接下来会花费很长的篇幅来讲解随机森林的思想和构造原理，已经有所了解的小伙伴可以直接跳过。</p>
<h2 id="思想简介"><a href="#思想简介" class="headerlink" title="思想简介"></a>思想简介</h2><p>在之前的一篇文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树的文章决策树算法之讲解实操（上）</a>中我们提到过，决策树的一个主要缺点在于经常对训练数据过拟合。那么随机森林就是解决这个问题的一种方法。随机森林本质上是许多决策树的集合，其中每棵树都和其它的树略有不同。</p>
<p>它的思想是：<strong>每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对树的结果取平均值来降低过拟合</strong>。这样做，既能减少过拟合又能保持树的预测能力。</p>
<p>为了实现上面的思想，我们需要构造很多决策树，并且每棵树都与其它的树保持不同，即树的<strong>随机化</strong>。而树的随机化方法有两种：一种是通过选择用于构造树的数据点，另一种是通过选择每次划分测试的特征。接下来，我们来更深入地讲解这一块。</p>
<span id="more"></span>

<h2 id="构造原理"><a href="#构造原理" class="headerlink" title="构造原理"></a>构造原理</h2><p>想要构造一个随机森林模型，需要确定用于构造的树的个数，即RandomForestClassifier（分类算法）或RandomForestRegressor（回归算法）的<strong>n_estimators参数</strong>。比如我们想要构造10棵树，这些树在构造时彼此完全独立，算法对每棵树进行不同的随机选择。而想要构造一棵树，首先要对数据进行<strong>自助采样</strong>。也就是说，从n个数据点中有放回地重复抽取一个样本（同一样本可以被多次抽取），共抽取n次。</p>
<p>举例说明，比如我们想要创建列表[a,b,c,d]的自助采样。一种可能的结果是[b,d,d,c]，另一种可能的结果是[d,a,d,a]。</p>
<p>接下来，基于这个新创建的数据集来构造决策树。在每个结点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试（注意并不是对每个结点都寻找最佳测试）。选择的特征个数由<strong>max_features</strong>参数来控制。每个结点中特征子集的选择都是相互独立的，这样树的每个结点可以使用特征的不同子集来作出决策。</p>
<p><strong>总之，由于使用了自助采样，随机森林中构造每棵决策树的数据集都是略有不同的。由于每个结点的特征选择，每棵树中的每次划分都是基于特征的不同子集这两种方法共同保证随机森林中所有树都不同</strong>。</p>
<p>想要利用随机森林进行预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了“软投票”策略。也就是说，每个算法作出“软预测”，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。</p>
<h2 id="实操建模"><a href="#实操建模" class="headerlink" title="实操建模"></a>实操建模</h2><p>数据是一份<a target="_blank" rel="noopener" href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒质量分类</a>的数据集，通过各个维度来判断红酒质量，之前在<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中已经讲解使用过了，这里就不多在赘述了，我们直接建模，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> winreg</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier<span class="comment">#随机森林分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="comment">###################</span></span><br><span class="line">real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)</span><br><span class="line">file_address=winreg.QueryValueEx(real_address, <span class="string">&quot;Desktop&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">file_address+=<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">file_origin=file_address+<span class="string">&quot;\\源数据-分析\\winequality-red.csv&quot;</span><span class="comment">###https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</span></span><br><span class="line">red_wine=pd.read_csv(file_origin)</span><br><span class="line"><span class="comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line">train=red_wine.drop([<span class="string">&quot;quality&quot;</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(train,red_wine[<span class="string">&quot;quality&quot;</span>],random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span></span><br><span class="line">forest=RandomForestClassifier(n_estimators=<span class="number">50</span>,random_state=<span class="number">1</span>)<span class="comment">###n_estimators树的个数</span></span><br><span class="line">train_prediction=forest.fit(X_train,y_train).predict(X_train)</span><br><span class="line">test_prediction=forest.fit(X_train,y_train).predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机森林分类器训练模型评分：&quot;</span>+<span class="built_in">str</span>(accuracy_score(y_train,train_prediction)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机森林分类器待测模型评分：&quot;</span>+<span class="built_in">str</span>(accuracy_score(y_test,test_prediction)))</span><br></pre></td></tr></table></figure>

<p>结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428134640544.jpg" alt="在这里插入图片描述"><br>下面是之前的文章中单棵决策树建立的模型结果：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428134820111.jpg" alt="在这里插入图片描述"><br>二者相比可以看出，随机森林的模型精度要比单棵树的要好上不少，过拟合现象也比之前要减轻很多。<br>接下来我们了解一下随机森林的主要模型参数。</p>
<h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>在RandomForestClassifier中，我们主要会用到三个模型参数n_estimators（树的个数），max_depths（树的深度）,max_features（随机特征数），它们对模型的影响程度依次递减。至于其它的参数，一般情况下直接默认就好。</p>
<p><strong>n_estimators</strong>：这个参数总是越大越好，对更多的树取平均值可以降低过拟合，从而得到鲁棒性更好的集成。不过收益是递减的，而且树越多需要的内存也越多，训练的时间也越长。常用的经验法就是“<strong>在时间/内存允许的情况下尽量多</strong>”。</p>
<p>接下来我们来调节这个参数，提高模型精度，代码及结果如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result_1=pd.DataFrame(columns=[<span class="string">&quot;集成树的个数(n_estimators)&quot;</span>,<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">500</span>,<span class="number">10</span>):</span><br><span class="line">    forest=RandomForestClassifier(n_estimators=i,random_state=<span class="number">1</span>)</span><br><span class="line">    forest.fit(X_train,y_train)</span><br><span class="line">    result_1=result_1.append([&#123;<span class="string">&quot;集成树的个数(n_estimators)&quot;</span>:i,<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>:accuracy_score(y_test,forest.predict(X_test))&#125;])</span><br><span class="line">result_1[result_1[<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>]==result_1[<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>].<span class="built_in">max</span>()]</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428135903273.jpg" alt="在这里插入图片描述"><br>可以看到当我们设定参数n_estimators为141或者151的时候，模型精度可以达到73%左右，较之前的结果提高了一些。</p>
<p><strong>max_depths</strong>：通过调节这个参数可以像单棵决策树那样进行预剪枝，当然，这个参数默认情况下就是最大深度，一般不需要调节。</p>
<p><strong>max_features</strong>：这个参数决定每棵树的随机性大小，较小的话可以降低过拟合，一般来说，好的经验就是使用默认值。</p>
<p>不过为了演示，我们依然可以调节这个参数，代码及结果如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result_1=pd.DataFrame(columns=[<span class="string">&quot;最大特征数量(max_features)&quot;</span>,<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    forest=RandomForestClassifier(n_estimators=<span class="number">151</span>,max_features=i,random_state=<span class="number">1</span>)</span><br><span class="line">    forest.fit(X_train,y_train)</span><br><span class="line">    result_1=result_1.append([&#123;<span class="string">&quot;最大特征数量(max_features)&quot;</span>:i,<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>:accuracy_score(y_test,forest.predict(X_test))&#125;])</span><br><span class="line">result_1[result_1[<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>]==result_1[<span class="string">&quot;随机森林分类器待测模型评分&quot;</span>].<span class="built_in">max</span>()]</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428141704422.jpg" alt="在这里插入图片描述"><br>正如我之前提到的，这个参数主要是用来限制树的最大随机特征数量的。那么，如果我们设置max_features等于特征值的数量（在上面的红酒质量数据集中是11），接下来每次划分都要考虑数据集的所有特征，也就意味着在特征选择过程中没有添加随机性（不过自助采样依然存在随机性）。如果设置max_features等于1，那么在划分时将无法选择对哪个特征进行测试，只能对随机选择的某个特征搜索不同的阈值。因此，<strong>如果max_features较大，那么随机森林中的树将会十分相似，利用最独特的特征可以轻松地拟合数据。如果max_features较小，那么随机森林中的树将会差异很大，为了更好地拟合数据，每棵树的深度都要很大</strong>。当然，实际过程中，它的默认参数通常就已经可以给出很好的结果了。</p>
<p><strong>对于分类，默认值是max_features=sqrt（维度个数）；<br>对于回归，默认值是max_features=维度个数。</strong></p>
<h2 id="分析特征重要性"><a href="#分析特征重要性" class="headerlink" title="分析特征重要性"></a>分析特征重要性</h2><p>与决策树类似，随机森林也可以给出特征重要性，计算方法是将森林中所有树的特征重要性求和并取平均。一般来说，随机森林给出的特征重要性要比单棵树给出的更为可靠。代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">&quot;fivethirtyeight&quot;</span>)</span><br><span class="line">sns.set_style(&#123;<span class="string">&#x27;font.sans-serif&#x27;</span>:[<span class="string">&#x27;SimHei&#x27;</span>,<span class="string">&#x27;Arial&#x27;</span>]&#125;)</span><br><span class="line">%matplotlib inline</span><br><span class="line">tree=DecisionTreeClassifier(max_depth=<span class="number">3</span>,random_state=<span class="number">1</span>)<span class="comment">###单棵树分类器</span></span><br><span class="line">forest=RandomForestClassifier(max_depth=<span class="number">3</span>,random_state=<span class="number">1</span>)<span class="comment">###随机森林分类器</span></span><br><span class="line">tree_prediction=tree.fit(X_train,y_train)</span><br><span class="line">forest_prediction=forest.fit(X_train,y_train)</span><br><span class="line">fig= plt.subplots(figsize=(<span class="number">20</span>,<span class="number">15</span>))</span><br><span class="line">fig1 = plt.subplot(<span class="number">211</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;决策树分类器特征重要性&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.bar(train.columns,tree_prediction.feature_importances_,<span class="number">0.4</span>,color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">fig2=plt.subplot(<span class="number">212</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;随机森林分类器特征重要性&#x27;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.bar(train.columns,forest_prediction.feature_importances_,<span class="number">0.4</span>,color=<span class="string">&quot;green&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks(fontsize=<span class="number">13</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428143909169.jpg" alt="在这里插入图片描述"></p>
<p>如上图所示，在保证树的深度参数（max_depth）相同的情况下，与单棵树相比，随机森林中有更多特征的重要性不为零。由于构造随机森林过程中的随机性，算法需要考虑多种可能的解释，结果就是随机森林比单棵树更能从总体把握数据的特征性。</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>用于分类和回归的随机森林是目前应用最广泛的机器学习方法之一。这种方法十分强大，通常不需要反复调节参数就可以给出很好的结果，也不需要对数据进行缩放。</p>
<p>从本质上看，随机森林拥有决策树的所有优点，同时弥补了决策树的一些缺陷。而实际中，我们仍然需要使用决策树的一个根本原因就是需要决策过程的紧凑表示。实际过程中，基本上不可能对几十棵甚至上百棵树作出详细解释，同时随机森林中树的深度往往比决策树还要大（因为用到了特征子集）。因此，如果你需要以可视化的方式想非专家总结预测过程，那么选择单棵树可能更好。虽然在大型数据集上构建随机森林可能比较费时间，但在一台计算器上用多个内核并行计算也很容易。</p>
<p>随机森林本质上是随机的，设置不同的状态（或者不设置random_state）可以彻底改变构建的模型。森林中的树越多，它对随机状态选择的鲁棒性（<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115541911">朴素贝叶斯分类器之分类实操</a>这篇文章介绍了鲁棒性的概念）也就越好。如果你希望结果可以重现，固定random_state是很重要的。</p>
<p>如果是分析维度非常高的稀疏数据，比如文本数据，随机森林的表现往往不是很好。对于这种数据，使用线性模型（<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115541911">朴素贝叶斯分类器之分类实操</a>或者<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>）可能更合适。即使是非常大的数据集，随机森林的表现通常也很好。不过随机森林需要更大的内存，训练和预测的速度也比线性模型要慢。对于一个应用来说，如果时间和内存很重要的话，那么换用线性模型可能更为明智。</p>
<p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yyb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb705.github.io">Yyb的花园</a>！</span></div></div><div class="post-copyright valine" id="comments-container"><script src="//unpkg.com/valine@1.4.14/dist/Valine.min.js"></script><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2];
}
var flag = false;
var gitFun = function () {
    try {
        var valineObj = window.GLOBAL_CONFIG.valine;
        new Valine({
            el: "#comments-container",
            ...valineObj
        });
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><i class="fas fa-angle-left">&nbsp;</i><span>决策树算法-单棵树（上）</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/"><span>分类器不确定度估计，监督学习算法小结</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By Yyb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>