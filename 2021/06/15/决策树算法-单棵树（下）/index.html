<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="决策树算法-单棵树（下）"><meta name="keywords" content="python,单棵树"><meta name="author" content="Yyb,undefined"><meta name="copyright" content="Yyb"><title>决策树算法-单棵树（下）【Yyb的花园】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"VN5QYUXW8S","apiKey":"7bb2817029d8aaa580ce39e2aef50ce7","indexName":"search","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {"appId":"S3SM93qxXfn8olHkcUQnJuIp-gzGzoHsz","appKey":"dJ07LxI5XAsyjJ3DB7zmCUL3","placeholder":"走过路过不要错过,买不买的瞧一瞧啊!","pageSize":10},
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yyb的花园" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">决策树算法-单棵树（下）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F"><span class="toc-number">1.1.</span> <span class="toc-text">序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.2.</span> <span class="toc-text">决策树的分析与可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%91%E7%9A%84%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">1.3.</span> <span class="toc-text">树的特征重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.4.</span> <span class="toc-text">回归决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Yyb</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://mp.csdn.net/console/article?spm=1010.2135.3001.5416" target="_blank">CSDN<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="1994yybsr@sina.com" target="_blank">E-Mail<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="https://github.com/yb705" target="_blank">GitHub<i class="icon-dot bg-color4"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">19</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">20</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">13</span></a></div><div class="friend-link"><a class="friend-link-text" href="https://github.com/yb705" target="_blank">不怎么用的github</a><a class="friend-link-text" href="https://i.csdn.net/#/user-center/profile?spm=1010.2135.3001.5111" target="_blank">定期更新的CSDN</a><a class="friend-link-text" href="1994yybsr@sina.com" target="_blank">只用来接收消息的邮箱</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">Yyb的花园</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">决策树算法-单棵树（下）</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2021-06-15 | 更新于 2021-06-15</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/python/">python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E5%8D%95%E6%A3%B5%E6%A0%91/">单棵树</a></div></div></div><div class="main-content"><h1 id="决策树算法-单棵树（下）"><a href="#决策树算法-单棵树（下）" class="headerlink" title="决策树算法-单棵树（下）"></a>决策树算法-单棵树（下）</h1><h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在上篇的文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115696198?spm=1001.2014.3001.5501">决策树算法之讲解实操（上）</a>当中，我们主要了解了决策树的算法原理，实际应用，以及简单介绍了下决策树的复杂度参数。而这篇我们主要讲解决策树的分析可视化，特征值重要程度，以及讨论回归决策树。</p>
<span id="more"></span>

<h2 id="决策树的分析与可视化"><a href="#决策树的分析与可视化" class="headerlink" title="决策树的分析与可视化"></a>决策树的分析与可视化</h2><p>树的可视化有助于深入理解算法是如何进行预测的，也是易于向非专家解释的机器学习算法的优秀示例。我们可以利用tree模块的export_graphviz函数来将树可视化。这个函数会生成一个.dot格式的文件，这是一种用于保存图形的文本文件格式。我们设置为结点添加颜色的选项，颜色表示每个结点中的多数类别，同时传入类别名称和特征名称，这样可以对树进行标记，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line">classifier=DecisionTreeClassifier(random_state=<span class="number">1</span>,max_depth=<span class="number">3</span>)<span class="comment">###为了图形好看一点，这里就选择深度为3</span></span><br><span class="line">train_prediction=classifier.fit(X_train,y_train)<span class="comment">###这里要单独列出来模型，作为下面代码的输入参数，所以平时不要频繁地使用方法链，不然报错会很麻烦</span></span><br><span class="line">tree_dot=export_graphviz(train_prediction,out_file=<span class="literal">None</span>,feature_names=train.columns,impurity=<span class="literal">False</span>,filled=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>PS：数据是来自于上一篇文章<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/115696198?spm=1001.2014.3001.5501">决策树算法之讲解实操（上）</a>当中的<a target="_blank" rel="noopener" href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒数据</a>，不了解的朋友可以去看一看，具体的我就不在这里多讲了。</p>
<p>为了让图形更容易观察，我这里将树的深度调整为了3。当然，在实际调参过程中，为了保证模型的精度，树的深度肯定不只是3。</p>
<p>接下来我们利用graphviz模块读取这个文件并将其可视化（当然你也可以用其它可以读取.dot文件的程序）。</p>
<p>与其它的第三方库的安装不同，要想使用graphviz模块，还需要再单独下载安装，并且配置环境变量，具体安装流程可以参考这篇文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/hankleo/p/9733076.html">Graphviz安装</a>（ps：安装graphviz，并且配置完环境变量后，建议重启一下python程序，不然没法马上识别出来dot文件）。</p>
<p>那么在安装好了之后，运行下面的代码就能可视化决策树了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz<span class="comment">###https://www.cnblogs.com/hankleo/p/9733076.html</span></span><br><span class="line">graph=graphviz.Source(tree_dot)</span><br><span class="line">graph.render(view=<span class="literal">True</span>, <span class="built_in">format</span>=<span class="string">&quot;pdf&quot;</span>, filename=<span class="string">&quot;decisiontree_pdf&quot;</span>)<span class="comment">###以pdf文件展示</span></span><br><span class="line">graphviz.Source(tree_dot)</span><br></pre></td></tr></table></figure>

<p>结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421113152156.jpg" alt="在这里插入图片描述"><br>实际上，即使这棵树的深度只有3层，看起来也还是有点大的。而深度更大的树（深度为10并不罕见）更加难以理解。这个时候，就有一种观察法可以提升效率了，那就是<strong>找出大部分数据的实际路径</strong>。譬如在上面的图形中，<strong>寻找samples占比最高的数据路径</strong>。像是第三层的samples=514，samples=416这两个结点，然后顺着这两个结点继续向下观察samples=400，samples=306这两个结点。几乎大部分数据都是顺着流程进入这几个结点，而其它叶结点都只包含很少的样本。</p>
<h2 id="树的特征重要性"><a href="#树的特征重要性" class="headerlink" title="树的特征重要性"></a>树的特征重要性</h2><p>实际上就算将整棵树可视化进行观察，也是非常费劲的。所以除此之外，我们还可以利用一些有用的属性来总结树的工作原理。其中最常用的就是特征重要性，它为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于0到1之间的数字，其中0表示“根本没用到”，1表示”完美预测目标值“，而特征重要性的求和始终为1。代码如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421114704556.png" alt="在这里插入图片描述"><br>我们可以将特征重要性可视化：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421114806519.png" alt="在这里插入图片描述"><br>我们可以看到，在这些特征当中，酒精含量（alcohol）是影响红酒质量的最重要的因素。</p>
<p>但是如果某个特征的feature_importance_很小，并不能说明这个特征没有提供任何信息。只能说明该特征没有被树选中，可能是因为另一个特征也包含了同样的信息。</p>
<p>与线性模型的系数不同，特征重要性始终为正数，也不能说明该特征对应哪个类别。<strong>特征重要性告诉我们酒精含量（alcohol）特征很重要，但并没有告诉我们含量大表示红酒质量高或者是低。事实上，在特征和类别之间可能没有这样简单的关系</strong>。</p>
<h2 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h2><p>虽然我们主要讨论的是用于分类的决策树，但对用于回归的决策树来说，所有内容都是类似的。在DecisionTreeRegressor中实现，回归树的用法和分析与分类树非常类似。但在将基于树的模型用于回归时，我们想要指出它的一个特殊性质：<strong>那就是DecisionTreeRegressor（以及其他所有基于树的回归模型）不能外推，也不能在训练数据范围之外进行预测</strong>。</p>
<p>我们通过一个简单的对数求和例子来更详细地研究这点。代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">lab=pd.DataFrame()</span><br><span class="line">lab[<span class="string">&quot;数列&quot;</span>]=np.arange(<span class="number">1</span>,<span class="number">30</span>)</span><br><span class="line">lab[<span class="string">&quot;数列对数&quot;</span>]=np.log(lab[<span class="string">&quot;数列&quot;</span>])</span><br><span class="line">lab[<span class="string">&quot;求和&quot;</span>]=lab[<span class="string">&quot;数列&quot;</span>]+lab[<span class="string">&quot;数列对数&quot;</span>]</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421120210592.png" alt="在这里插入图片描述"><br>实际上，上述数据集的设定就是：一列是顺序数列，一列是顺序数列的对数，还有一列就是前两列的求和，它的回归线就是：<strong>y=x+log(x)<strong>，如下图所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421120642636.png" alt="在这里插入图片描述"><br>如果是利用线性模型来去做回归预测是非常容易的，并且模型精度会很高，这里就不再赘述了。那么接下来我们用决策树的回归算法进行建模，并分别对</strong>训练数据集内和训练数据集外</strong>的数据进行预测，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor<span class="comment">#决策树回归</span></span><br><span class="line">X_train=lab.iloc[:<span class="number">15</span>,<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">X_test=lab.iloc[<span class="number">15</span>:,<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">Y_train=lab.loc[:<span class="number">14</span>,<span class="string">&quot;求和&quot;</span>]</span><br><span class="line">tree_regressor=DecisionTreeRegressor().fit(X_train,Y_train)<span class="comment">###训练集模型</span></span><br><span class="line">tree_train_prediction=tree_regressor.predict(X_train)<span class="comment">###数据范围内预测</span></span><br><span class="line">tree_test_prediction=tree_regressor.predict(X_test)<span class="comment">###数据范围外预测</span></span><br></pre></td></tr></table></figure>

<p>将预测结果可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.style.use(<span class="string">&quot;fivethirtyeight&quot;</span>)</span><br><span class="line">sns.set_style(&#123;<span class="string">&#x27;font.sans-serif&#x27;</span>:[<span class="string">&#x27;SimHei&#x27;</span>,<span class="string">&#x27;Arial&#x27;</span>]&#125;)<span class="comment">#设定汉字字体，防止出现方框</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#在jupyter notebook上直接显示图表</span></span><br><span class="line">fig= plt.subplots(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(lab[<span class="string">&quot;数列&quot;</span>],lab[<span class="string">&quot;求和&quot;</span>],label=<span class="string">&quot;数据回归线&quot;</span>)</span><br><span class="line">plt.plot(X_train[<span class="string">&quot;数列&quot;</span>],tree_train_prediction,label=<span class="string">&quot;决策树训练集结果&quot;</span>)</span><br><span class="line">plt.plot(X_test[<span class="string">&quot;数列&quot;</span>],tree_test_prediction,label=<span class="string">&quot;外推测试集结果&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.xticks(fontsize=<span class="number">15</span>)<span class="comment">#设置坐标轴上的刻度字体大小</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;数列&quot;</span>,fontsize=<span class="number">15</span>)<span class="comment">#设置坐标轴上的标签内容和字体</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;求和&quot;</span>,fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421121055686.jpg" alt="在这里插入图片描述"><br>可以看出树模型完美预测了训练数据集（红线与蓝线重合的部分）。由于我们没有限制树的复杂度，因此它记住了整个数据集。但是，一旦输入超出训练集之外的数据，模型就只能持续预测最后一个已知数据点（黄线）。<strong>树不能在训练数据的范围之外生成”新的“响应，所有基于树的回归模型都有这个缺点。</strong></p>
<p>实际上，利用基于树的模型可以做出非常好的预测。上述例子的目的并不是说明决策树是一个不好的模型，而是为了说明树在预测方式上的特殊性质。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如上篇文章所述，控制决策树模型复杂度的参数是预剪枝参数，它在树完全展开之前停止树的构造。通常来说，选择一种预剪枝策略（设置max_depth,max_leaf_nodes或min_samples_leaf）足以防止过拟合。</p>
<p>与前面讨论过的许多算法相比，决策树有两个优点：一是得到的模型很容易可视化，非专家也很容易理解（至少对于较小的树来说）；二是算法完全不受数据缩放影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。特别是特征的尺度不一样时或者二元特征和连续特征同时存在时，决策树效果很好。</p>
<p>决策树的主要缺点在于，即使做了预剪枝，它也会经常过拟合，泛化能力较差。因此，在大多数应用中，往往绘制用集成方法来替代单颗决策树。集成方法我们会在以后进行介绍。</p>
<p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yyb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/">https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb705.github.io">Yyb的花园</a>！</span></div></div><div class="post-copyright valine" id="comments-container"><script src="//unpkg.com/valine@1.4.14/dist/Valine.min.js"></script><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2];
}
var flag = false;
var gitFun = function () {
    try {
        var valineObj = window.GLOBAL_CONFIG.valine;
        new Valine({
            el: "#comments-container",
            ...valineObj
        });
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/"><i class="fas fa-angle-left">&nbsp;</i><span>岭回归</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"><span>决策树算法-单棵树（上）</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By Yyb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>