<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="k临近算法-回归"><meta name="keywords" content="python,回归模型"><meta name="author" content="Yyb,undefined"><meta name="copyright" content="Yyb"><title>k临近算法-回归【Yyb的花园】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"VN5QYUXW8S","apiKey":"7bb2817029d8aaa580ce39e2aef50ce7","indexName":"search","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {},
  valine: {"appId":"S3SM93qxXfn8olHkcUQnJuIp-gzGzoHsz","appKey":"dJ07LxI5XAsyjJ3DB7zmCUL3","placeholder":"走过路过不要错过,买不买的瞧一瞧啊!","pageSize":10},
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yyb的花园" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#k%E7%9B%B8%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">k相邻近算法-回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90"><span class="toc-number">1.3.</span> <span class="toc-text">数据来源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98"><span class="toc-number">1.4.</span> <span class="toc-text">数据挖掘</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Yyb</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://mp.csdn.net/console/article?spm=1010.2135.3001.5416" target="_blank">CSDN<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="1994yybsr@sina.com" target="_blank">E-Mail<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="https://github.com/yb705" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">19</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">20</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">13</span></a></div><div class="friend-link"><a class="friend-link-text" href="https://github.com/yb705" target="_blank">不怎么用的github</a><a class="friend-link-text" href="https://i.csdn.net/#/user-center/profile?spm=1010.2135.3001.5111" target="_blank">定期更新的CSDN</a><a class="friend-link-text" href="1994yybsr@sina.com" target="_blank">只用来接收消息的邮箱</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">Yyb的花园</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">k临近算法-回归</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2021-06-15 | 更新于 2021-06-15</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">监督学习</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/k%E9%82%BB%E8%BF%91/">k邻近</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/python/">python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">回归模型</a></div></div></div><div class="main-content"><h1 id="k相邻近算法-回归"><a href="#k相邻近算法-回归" class="headerlink" title="k相邻近算法-回归"></a>k相邻近算法-回归</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>先简单介绍一下机器学习里面的两个概念</p>
<p><strong>1.分类与回归</strong></p>
<p>分类模型和回归模型本质一样，分类模型是将回归模型的输出离散化。</p>
<p>一般来说，回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。回归是对真实值的一种逼近预测。</p>
<p>分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。</p>
<p>简言之：</p>
<p>　　<strong>定量输出称为回归，或者说是连续变量预测，预测明天的气温是多少度，这是一个回归任务<br>定性输出称为分类，或者说是离散变量预测，预测明天是阴、晴还是雨，就是一个分类任务</strong></p>
<p><strong>2.拟合</strong><br><strong>泛化</strong>：如果一个模型能够对没见过的新数据作出准确预测，我们就能够说它能够从训练集<strong>泛化</strong>到测试集<br><strong>拟合</strong>：模型是否可以很好的描述某些样本，并且有较好的泛化能力<br><strong>欠拟合</strong>：测试样本的特性没有学到，或模型过于简单无法拟合<br><strong>过拟合</strong>：太过贴近于训练数据的特性，在训练集上优秀，但在测试集上不行，不具有泛化性</p>
<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>KNN回归</strong></p>
<p>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的某个（些）属性的平均值赋给该样本，就可以得到该样本对应属性的值。</p>
<p>knn分类实操可以参考这一篇文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法-分类实操</a></p>
<span id="more"></span>

<h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="#https://www.kaggle.com/neuromusic/avocado-prices">鳄梨单价预测（Kaggle）：#https://www.kaggle.com/neuromusic/avocado-prices</a></p>
<p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231144706634.png" alt="在这里插入图片描述"><br>该数据包含2017年到2019年的鳄梨单价，每次出售的重量，鳄梨种类，产地等信息。<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231144907215.png" alt="在这里插入图片描述"><br>Tips：4046，4225，4770是国外进口水果的plu码，plu四位码代表鳄梨的产地，种类，大小等水果信息，所以4046，4225，4770就是代表三种鳄梨。<br><em><del>有一说一，平常买水果还真没注意到这个。又知道了一个无用的小知识。</del></em> </p>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split<span class="comment">#导入划分数据集的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor<span class="comment">#导入knn回归算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br></pre></td></tr></table></figure>

<p>老规矩，上来先依次导入建模需要的各个模块，除了前四个库是数据挖掘必要的第三方库之外，重点说一下r2_score：</p>
<blockquote>
<p>sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)<br>y_true：观测值<br>y_pred：预测值<br>sample_weight：样本权重，默认None<br>multioutput：多维输入输出，可选‘raw_values’, ‘uniform_average’, ‘variance_weighted’或None。默认为’uniform_average’;<br>raw_values：分别返回各维度得分<br>uniform_average：各输出维度得分的平均<br>variance_weighted：对所有输出的分数进行平均，并根据每个输出的方差进行加权。</p>
</blockquote>
<p>r2_score评分是主要的回归模型评分方式，具体原理就不多做介绍了，感兴趣的朋友可以查看这篇文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/jpld/p/12022123.html">深度研究：回归模型评价指标R2_score</a></p>
<p><strong>2.读取文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> winreg</span><br><span class="line">real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)</span><br><span class="line">file_address=winreg.QueryValueEx(real_address, <span class="string">&quot;Desktop&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">file_address+=<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">file_origin=file_address+<span class="string">&quot;\\源数据-分析\\avocado.csv&quot;</span><span class="comment">#设立源数据文件的桌面绝对路径</span></span><br><span class="line">glass=pd.read_csv(file_origin)<span class="comment">#https://www.kaggle.com/neuromusic/avocado-prices</span></span><br></pre></td></tr></table></figure>

<p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。</p>
<p><strong>3.清洗数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">avocado.groupby(avocado[<span class="string">&quot;year&quot;</span>])[<span class="string">&quot;year&quot;</span>].count()</span><br><span class="line">avocado_2017=avocado[avocado[<span class="string">&quot;year&quot;</span>]==<span class="number">2017</span>].reset_index()</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231150641784.png" alt="在这里插入图片描述"><br>这里我们选择2017年的数据作为建模数据。<del><em>（有的时候数据量太多反而会造成一些噪音干扰）</em></del> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">avocado_2017=avocado_2017.replace(&#123;<span class="string">&quot;type&quot;</span>:&#123;<span class="string">&quot;conventional&quot;</span>:<span class="number">0</span>&#125;&#125;)</span><br><span class="line">avocado_2017=avocado_2017.replace(&#123;<span class="string">&quot;type&quot;</span>:&#123;<span class="string">&quot;organic&quot;</span>:<span class="number">1</span>&#125;&#125;)</span><br><span class="line">a=pd.DataFrame(avocado_2017.groupby(avocado_2017[<span class="string">&quot;region&quot;</span>])[<span class="string">&quot;region&quot;</span>].count())</span><br><span class="line">a[<span class="string">&quot;replace_num&quot;</span>]=<span class="built_in">range</span>(<span class="built_in">len</span>(a.index))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a.index)):</span><br><span class="line">    avocado_2017=avocado_2017.replace(&#123;<span class="string">&quot;region&quot;</span>:&#123;a.index[i]:a.loc[a.index[i],<span class="string">&quot;replace_num&quot;</span>]&#125;&#125;)</span><br><span class="line">    <span class="comment">###注意这里在利用.loc进行筛选时不能用数字索引进行筛选，因为当前a的行索引是一系列字符串</span></span><br></pre></td></tr></table></figure>

<p>因为type和region的文本数据有点复杂，所以利用replace函数替换成数字来代表不同的品种和产地。<br><del>其实这个替换并没有什么用，只是博主单纯地看不习惯文本数据而已。</del> </p>
<p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231151917738.png" alt="在这里插入图片描述"><br>因为数据中的特征值Total Bags=Small Bags+Large Bags，XLarge Bags=0，且region，type皆与4位plu码有属性重合的情况，所以为了避免出现过拟合，我这里只选取AveragePrice，Total Volume，4046，4225，4770，Small Bags，Large Bags进行预测建模。</p>
<p><strong>4.建模</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test,y_train,y_test=train_test_split(avocado_2017[[<span class="string">&quot;Total Volume&quot;</span>,<span class="string">&quot;4225&quot;</span>,<span class="string">&quot;4046&quot;</span>,<span class="string">&quot;4770&quot;</span>,<span class="string">&quot;Small Bags&quot;</span>,<span class="string">&quot;Large Bags&quot;</span>]],avocado_2017[<span class="string">&quot;AveragePrice&quot;</span>],random_state=<span class="number">24</span>)</span><br><span class="line"><span class="comment">#注意特征值标签要放在前面，预测值标签要放在后面</span></span><br><span class="line"><span class="comment">#考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span></span><br></pre></td></tr></table></figure>

<p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn=KNeighborsRegressor(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train,y_train)</span><br><span class="line">prediction=knn.predict(X_test)</span><br><span class="line">r2_score(y_test,prediction)</span><br></pre></td></tr></table></figure>

<p>引入knn算法，并将算法中的邻居值设为1，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231153653655.png" alt="在这里插入图片描述"><br>可以看到，该模型的精度为50%左右。</p>
<p><strong>5.简单的调参</strong></p>
<p>之前设立的邻居参数为1，接下来依次测试不同的参数，看看最优的邻居参数是多少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result=&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):<span class="comment">#一般n_neighbors的选取低于样本总数的平方根</span></span><br><span class="line">    knn=KNeighborsRegressor(n_neighbors=(i+<span class="number">1</span>))</span><br><span class="line">    knn.fit(X_train,y_train)</span><br><span class="line">    prediction=knn.predict(X_test)</span><br><span class="line">    score=r2_score(y_test,prediction)</span><br><span class="line">    result[i+<span class="number">1</span>]=score*<span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result.keys():</span><br><span class="line">    <span class="keyword">if</span> result[i]==<span class="built_in">max</span>(result.values()):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;最佳邻近数：&quot;</span>+<span class="built_in">str</span>(i))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型评分：&quot;</span>+<span class="built_in">str</span>(<span class="built_in">max</span>(result.values())))</span><br></pre></td></tr></table></figure>

<p>结果如下：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231154159122.jpg" alt="在这里插入图片描述"><br>可以看出在邻近数依次选择1～100的过程中，最佳邻近参数为4；模型的最佳精度评分是68分。（可以看到分数很低，博主认为有可能是算法本身的原因。毕竟是一个简单的算法，而不像森林或者树回归算法一样，可以调整权重等其它参数。）</p>
<p><strong>6.总结</strong></p>
<p>1.随着邻近参数的变化，模型精度也会跟随变化，并呈现一定的规律的规律：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231154542262.jpg" alt="在这里插入图片描述"></p>
<p>对于同一个数据集，随着邻近参数的逐渐增加，模型精度往往会到达一个临界点，之后便会逐渐降低。其他的knn回归模型也会呈现这种情况，有兴趣的朋友可以自行检验一下。</p>
<p>2.回归类算法是预测连续值的算法，如果打算通过对预测标签进行分箱（pd.cut），来对预测标签进行变化范围的预测是行不通的。ps:博主已经尝试过，会报错误。</p>
<p>3.对于不同类型的数据要选择不同类型的算法，每个算法都有各自的优缺点，并没有可以解决所有问题的算法，所以在以后的建模中不要钻牛角尖，要注意选择。</p>
<p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yyb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://yb705.github.io/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/">https://yb705.github.io/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb705.github.io">Yyb的花园</a>！</span></div></div><div class="post-copyright valine" id="comments-container"><script src="//unpkg.com/valine@1.4.14/dist/Valine.min.js"></script><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2];
}
var flag = false;
var gitFun = function () {
    try {
        var valineObj = window.GLOBAL_CONFIG.valine;
        new Valine({
            el: "#comments-container",
            ...valineObj
        });
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"><i class="fas fa-angle-left">&nbsp;</i><span>lasso回归算法</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/"><span>k邻近算法-分类</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By Yyb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>