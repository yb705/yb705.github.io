<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="岭回归实操"><meta name="keywords" content=""><meta name="author" content="Yyb,undefined"><meta name="copyright" content="Yyb"><title>岭回归实操【Yyb的花园】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"VN5QYUXW8S","apiKey":"7bb2817029d8aaa580ce39e2aef50ce7","indexName":"search","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {"owner":"yb705","repo":"yb705.github.io","client_id":"0ebe890365ab1fb8aa2b","client_secret":"af40ac901bc83782cc412a83baa80122c88a30d3"},
  valine: {},
}</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Yyb的花园" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#python-%E5%B2%AD%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%B9%8B%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D"><span class="toc-number">1.</span> <span class="toc-text">python 岭回归算法之回归实操</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90"><span class="toc-number">1.3.</span> <span class="toc-text">数据来源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98"><span class="toc-number">1.4.</span> <span class="toc-text">数据挖掘</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA"><span class="toc-number">1.5.</span> <span class="toc-text">讨论</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Yyb</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/fan-lv" target="_blank">GitHub<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="mailto:15757856604@163.com" target="_blank">E-Mail<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color4"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">3</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">Yyb的花园</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">岭回归实操</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2021-06-10 | 更新于 2021-06-10</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"></div><div class="button-hover tags"></div></div></div><div class="main-content"><h1 id="python-岭回归算法之回归实操"><a href="#python-岭回归算法之回归实操" class="headerlink" title="python 岭回归算法之回归实操"></a>python 岭回归算法之回归实操</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>正则化</strong></p>
<p><strong>正则化</strong>是指对模做显式约束，以避免过拟合。本文用到的岭回归就是L2正则化。（从数学的观点来看，岭回归惩罚了系数的L2范数或w的欧式长度）</p>
<p>正则化的具体原理就不在这里多叙述了，感兴趣的朋友可以看一下这篇文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a>。</p>
<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>岭回归</strong></p>
<p>岭回归也是一种用于回归的线性模型，因此它的模型公式与最小二乘法的相同，如下式所示：</p>
<blockquote>
<p>y=w[0]*x[0]+w[1]*x[1]+w[2]<em>x[2]+……+w[p]<em>x[p]+b</em></em></p>
</blockquote>
<p>但在岭回归中，对系数w的选择不仅要在训练数据上得到很好的预测结果，而且还要拟合附加约束。换句话说，w的所有元素都应接近于0。直观上来看，这意味着每个特征对输出的影响应尽可能小（即斜率很小），同时仍给出很好的预测结果，这个约束也就是<strong>正则化</strong>。</p>
<h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/altavish/boston-housing-dataset">波士顿房价：https://www.kaggle.com/altavish/boston-housing-dataset</a><br><del>也是非常经典的一个数据</del><br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121135007262.jpg" alt="在这里插入图片描述"></p>
<p>简单解释一下这个数据的几个主要指标：<br>ZN：25,000平方英尺以上的土地划为住宅用地的比例。<br>RM：每个住宅的平均房间数。<br>AGE：1940年之前建造的自有住房的比例<br>CHAS：有没有河流经过 (如果等于1，说明有，等于0就说明没有)<br>CRIM：犯罪率<br>MEDV：住房的价格<br><del>其它指标就不用说了，都是一些住房的其它指标，感兴趣的小伙伴可以自己查一下。</del> </p>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> winreg</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge<span class="comment">###导入岭回归算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br></pre></td></tr></table></figure>

<p>老规矩，上来先依次导入建模需要的各个模块<br><strong>2.读取文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> winreg</span><br><span class="line">real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)</span><br><span class="line">file_address=winreg.QueryValueEx(real_address, <span class="string">&quot;Desktop&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">file_address+=<span class="string">&#x27;\\&#x27;</span></span><br><span class="line">file_origin=file_address+<span class="string">&quot;\\源数据-分析\\HousingData.csv&quot;</span><span class="comment">#设立源数据文件的桌面绝对路径</span></span><br><span class="line">house_price=pd.read_csv(file_origin)<span class="comment">#https://www.kaggle.com/altavish/boston-housing-dataset</span></span><br></pre></td></tr></table></figure>

<p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p>
<p><strong>3.清洗数据</strong></p>
<p><em>1.查找缺失值</em><br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121135832795.png" alt="在这里插入图片描述"></p>
<p>可以看到这个数据并包括一些缺失值，并不是很多，所以直接删掉就好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">house_price1=house_price.dropna().reset_index()</span><br><span class="line"><span class="keyword">del</span> house_price1[<span class="string">&quot;index&quot;</span>]</span><br></pre></td></tr></table></figure>

<p><em>2.突变值查找</em></p>
<p><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121140155149.png" alt="在这里插入图片描述"><br>一般是看看特征值里面是否包含等于零的数据。其实说的直接一点就是看看数据里面是否包含不符合实际的数值，比如像是犯罪率，实际中不可能出现犯罪率等于0的片区。那么从上面的结果来看，这份数据并没有其它问题。<br><del>这份数据里面的ZN和CHAS都是利用0和1来当作一种指标，所以包含0是很正常的。</del><br><strong>4.建模</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train=house_price1.drop([<span class="string">&quot;MEDV&quot;</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(train,house_price1[<span class="string">&quot;MEDV&quot;</span>],random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#将MEDV划分为预测值，其它的属性划分为特征值，并将数据划分成训练集和测试集。</span></span><br><span class="line">ridge=Ridge(alpha=<span class="number">10</span>)<span class="comment">#确定约束参数</span></span><br><span class="line">ridge.fit(X_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;岭回归训练模型得分：&quot;</span>+<span class="built_in">str</span>(r2_score(y_train,ridge.predict(X_train))))<span class="comment">#训练集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;岭回归待测模型得分：&quot;</span>+<span class="built_in">str</span>(r2_score(y_test,ridge.predict(X_test))))<span class="comment">#待测集</span></span><br></pre></td></tr></table></figure>

<p>引入ridge算法，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121142032926.png" alt="在这里插入图片描述"></p>
<p>可以看到，该模型的训练精度为79%左右，对于新的数据来说，模型精度在63%左右。<br>至此，这个数据集的将建模就算是完成了。</p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p><strong>1.参数的讨论</strong></p>
<p>由于岭回归与线性回归（最小二乘法）的模型公式是一样的，所以这里我们与线性回归做一个比较。不了解线性回归的朋友可以看一下我的另一篇文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43580339/article/details/112271333">最小二乘算法之回归实操</a><br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121142126579.png" alt="在这里插入图片描述"></p>
<p>之前我们设立的约束参数是10，而上面模型参数设的是0，可以看出模型的训练精度有所提高，但泛化能力有所降低。同时与线性回归模型相比，二者的分数是完全一样的。所以，当岭回归的约束参数设为0时，失去约束的岭回归与普通最小二乘法就是同一个算法。</p>
<p><strong>2.与普通最小二乘法的比较</strong></p>
<p>我们通过变换约束参数的取值，来具体看一下岭回归与普通最小二乘法的优缺点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">result_b=pd.DataFrame(columns=[<span class="string">&quot;参数&quot;</span>,<span class="string">&quot;岭回归训练模型得分&quot;</span>,<span class="string">&quot;岭回归待测模型得分&quot;</span>,<span class="string">&quot;线性回归训练模型得分&quot;</span>,<span class="string">&quot;线性回归待测模型得分&quot;</span>])</span><br><span class="line">train=house_price1.drop([<span class="string">&quot;MEDV&quot;</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(train,house_price1[<span class="string">&quot;MEDV&quot;</span>],random_state=<span class="number">23</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">21</span>):</span><br><span class="line">    alpha=i/<span class="number">10</span><span class="comment">#约定参数可以选定为小数</span></span><br><span class="line">    ridge=Ridge(alpha=alpha)</span><br><span class="line">    ridge.fit(X_train,y_train)</span><br><span class="line">    linear=LinearRegression()</span><br><span class="line">    linear.fit(X_train,y_train)</span><br><span class="line">    result_b=result_b.append([&#123;<span class="string">&quot;参数&quot;</span>:alpha,<span class="string">&quot;岭回归训练模型得分&quot;</span>:r2_score(y_train,ridge.predict(X_train)),<span class="string">&quot;岭回归待测模型得分&quot;</span>:r2_score(y_test,ridge.predict(X_test)),<span class="string">&quot;线性回归训练模型得分&quot;</span>:r2_score(y_train,linear.predict(X_train)),<span class="string">&quot;线性回归待测模型得分&quot;</span>:r2_score(y_test,linear.predict(X_test))&#125;])</span><br></pre></td></tr></table></figure>

<p>结果如下所示：<br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121143002294.png" alt="在这里插入图片描述"><br>可以看出如果只是针对训练模型的精度，最小二乘法是要优于岭回归的，但是对新的数据作出预测时，也就是考虑模型的泛化能力上，可以看出岭回归的模型得分比最小二乘法要好一点。<br>我们通过一个折线图来更直观地表现上面的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">plt.style.use(<span class="string">&quot;fivethirtyeight&quot;</span>)</span><br><span class="line">sns.set_style(&#123;<span class="string">&#x27;font.sans-serif&#x27;</span>:[<span class="string">&#x27;SimHei&#x27;</span>,<span class="string">&#x27;Arial&#x27;</span>]&#125;)<span class="comment">#设定汉字字体，防止出现方框</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#在jupyter notebook上直接显示图表</span></span><br><span class="line">fig= plt.subplots(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(result_b[<span class="string">&quot;参数&quot;</span>],result_b[<span class="string">&quot;岭回归训练模型得分&quot;</span>],label=<span class="string">&quot;岭回归训练模型得分&quot;</span>)<span class="comment">#画折线图</span></span><br><span class="line">plt.plot(result_b[<span class="string">&quot;参数&quot;</span>],result_b[<span class="string">&quot;岭回归待测模型得分&quot;</span>],label=<span class="string">&quot;岭回归待测模型得分&quot;</span>)</span><br><span class="line">plt.plot(result_b[<span class="string">&quot;参数&quot;</span>],result_b[<span class="string">&quot;线性回归训练模型得分&quot;</span>],label=<span class="string">&quot;线性回归训练模型得分&quot;</span>)</span><br><span class="line">plt.plot(result_b[<span class="string">&quot;参数&quot;</span>],result_b[<span class="string">&quot;线性回归待测模型得分&quot;</span>],label=<span class="string">&quot;线性回归待测模型得分&quot;</span>)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks(fontsize=<span class="number">15</span>)<span class="comment">#设置坐标轴上的刻度字体大小</span></span><br><span class="line">plt.yticks(fontsize=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;参数&quot;</span>,fontsize=<span class="number">15</span>)<span class="comment">#设置坐标轴上的标签内容和字体</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;得分&quot;</span>,fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<p>结果如下所示：<br><img src="/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/20210121144729267.png" alt="在这里插入图片描述"><br>可以看出岭回归模型在模型的简单性（系数都接近于0）与训练集性能之间作出权衡。简单性和训练性能二者对于模型的重要程度可以由用户通过设置aplha参数来制定。增大alpha会使得系数更加趋向于0，从而降低训练集性能，但会提高泛化性能。</p>
<p>而且无论是岭回归还是线性回归，所有数据集大小对应的训练分数都要高于预测分数。由于岭回归是正则化的，所以它的训练分数要整体低于线性回归的训练分数。但岭回归的测试分数高，特别是对于较小的数据集。如果数据量小于一定程度的时候，线性回归将学不到任何内容，随着模型可用数据越来越多，两个模型的性能都在提升，最终线性回归的性能追上了岭回归。所以如果有足够多的训练内容，<strong>正则化</strong>变得不那么重要，并且岭回归和线性回归将具有相同的性能。</p>
<p>以上就是关于岭回归的实际操作与看法了，有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yyb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://yb705.github.io/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/">https://yb705.github.io/2021/06/10/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%93%8D/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yb705.github.io">Yyb的花园</a>！</span></div></div><div class="post-copyright" id="comments-container"><script src="/js/comments/gitment.js"></script></div><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2]
}
var flag = false;
var gitFun = function () {
    try {
        var gitmentObj = window.GLOBAL_CONFIG.gitment;
        var gitment = new Gitment({
            id: decodeURI(title), // 可选。默认为 location.href
            owner: gitmentObj.owner,
            repo: gitmentObj.repo,
            oauth: {
                client_id: gitmentObj.client_id,
                client_secret: gitmentObj.client_secret
            },
        });
        gitment.render('comments-container');
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></article><div id="pagination"><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2021/06/08/Hexo/"><span>Hexo</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By Yyb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>