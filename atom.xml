<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yyb的花园</title>
  
  
  <link href="https://yb705.github.io/atom.xml" rel="self"/>
  
  <link href="https://yb705.github.io/"/>
  <updated>2021-09-12T02:09:38.777Z</updated>
  <id>https://yb705.github.io/</id>
  
  <author>
    <name>Yyb</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>特征工程——独热编码</title>
    <link href="https://yb705.github.io/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/"/>
    <id>https://yb705.github.io/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/</id>
    <published>2021-09-12T02:06:07.000Z</published>
    <updated>2021-09-12T02:09:38.777Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的<strong>连续特征</strong>。对于许多应用而言，数据的收集方式并不是这样。一种特别常见的特征类型就是<strong>分类特征</strong>，也叫<strong>离散特征</strong>。这种特征通常并不是数值。分类特征与连续特征之间的区别类似于分类和回归之间的区别，只是前者在输入端而不是输出端。<strong>其实，无论你的数据包含哪种类型的特征，数据表示方式都会对机器学习模型的性能产生巨大影响</strong>。譬如说我们之前讲过的<a href="https://blog.csdn.net/weixin_43580339/article/details/117774014">数据缩放</a>。</p><p>对于某个特定的应用来说，如何找到最佳数据表示，这个问题被称为<strong>特征工程</strong>，它是数据科学家和机器学习从业者在尝试解决现实世界问题时的主要任务之一。<strong>用正确的方式表示数据，对监督模型性能的影响比所选择的精确参数还要大。</strong></p><h3 id="One-Hot编码（虚拟变量）"><a href="#One-Hot编码（虚拟变量）" class="headerlink" title="One-Hot编码（虚拟变量）"></a>One-Hot编码（虚拟变量）</h3><p>到目前为止，表示分类变量最常用的方法就是使用<strong>one-hot编码</strong>或<strong>N取一编码</strong>，也叫<strong>虚拟变量</strong>。虚拟变量背后的思想是将一个分类变量替换为一个或多个新特征，新特征取值为0或1.对于线性二分类（以及scikit-learn中其他所有模型）的公式而言，0和1这两个值是有意义的，我们可以像这样对每个类别引入一个新特征，从而表示任意数量的类别。</p><p>接下来，我用实际数据来讲解一下独热编码。</p><h4 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h4><p><a href="https://www.kaggle.com/prathamtripathi/drug-classification">药品分类：https://www.kaggle.com/prathamtripathi/drug-classification</a></p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWI3MDU=,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"></p><p>该数据集有200条特征记录，包含患者年龄，性别，血压水平 (BP)，胆固醇水平，钠钾比共5个特征值，而我们所需要做的就是通过这些特征值，对患者的用药（Drug)进行分类。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p><strong>1.读取数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\drug200.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/prathamtripathi/drug-classification</span><br>drug=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</p><p><strong>2.检查分类数据</strong></p><p>读取完数据集之后，最好先检查每一列是否包含有意义的分类数据。在处理人工（比如用户调查）输入的数据时，可能没有固定的类别，拼写和大小写也存在差异，因此可能需要预处理。比如，有人可能将性别填为”M“，有人可能填为”m“，虽然他们都代表男性，但是对于电脑来说，他们却是分属两个类别。</p><p>检查列的内容有一个好办法，就是使用pandas_Series的value_counts函数，以显示唯一值及其出现次数：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/df17f8e8b1ae40e3908faa579e47c25b.png" alt="在这里插入图片描述"></p><p>可以看到，在这个数据集中Drug刚好有5个值，这说明数据格式已经很好，可以用one-hot编码来表示。在实际应用中，你应该查看并检查所有列的值。为简洁起见，这里我们将跳过这一步。</p><h4 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h4><p>用pandas编码数据有一种非常简单的方法，就是使用get_dummies函数。get_dummies函数自动变换所有具有对象类型（比如字符串）的列或所有分类的列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">drug_dummies=pd.get_dummies(drug)<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWI3MDU=,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"></p><p>可以看到，连续特征Age和Na_to_K没有发生变化，而分类特征的每个可能取值都被扩展为一个新特征。实际上，就是利用多个新特征对一个分类特征进行编码（如Sex_F和Sex_M）。在机器学习中使用此数据时，我们将会删除原有特征，仅保证0-1特征（有点类似于统计学里面的离散变量)。</p><p>接下来，我们在变化后的数据集上训练一个Logistic回归分类器。</p><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>之前，我在<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">LogisticRegression算法</a>中已经讲解过算法原理了，这里就不再赘述了，感兴趣的朋友可以点击链接去查看下。</p><p>我们已经知道Logistic回归利用下列公式进行预测，预测值为y：</p><blockquote><p>y=w[0]*x[0]+w[1]x[1]+w[2]x[2]+…+w[p]x[p]+b</p></blockquote><p>其中w[i]和b是从训练集中学到的系数，X[i]是输入特征。显然，<strong>只有当x[i]是数字时这个公式才有意义</strong>。所以，独热编码的作用就体现出来了。</p><p>接下来，我们开始训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=pd.get_dummies(drug.drop([<span class="hljs-string">&quot;Drug&quot;</span>],axis=<span class="hljs-number">1</span>))<span class="hljs-comment">####去掉目标变量之后，进行独热编码</span><br>clusters=drug[<span class="hljs-string">&quot;Drug&quot;</span>]<span class="hljs-comment">###只是替换了训练特征，目标特征倒是不用独热编码，不然会替换原有特征，无法训练</span><br><span class="hljs-comment">###目标变量就和之前监督学习里面的训练一样，作为独立的一列变量存在就好，独热编码只是用来修改训练特征变量，就跟连续变量一样</span><br>X_train,X_test,y_train,y_test=train_test_split(train,clusters,random_state=<span class="hljs-number">1</span>)<br>logistic=LogisticRegression(C=<span class="hljs-number">20</span>,solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,max_iter=<span class="hljs-number">10000</span>)<br>logistic.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,logistic.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,logistic.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/ac78803f98c44528bfb8982f07c20041.png" alt="在这里插入图片描述"></p><p>在这个例子中，我们对同时包含训练数据和测试数据的数据框调用get_dummies。这一点很重要，可以确保训练集和测试集中分类变量的表示方式相同。</p><h3 id="数字可以编码分类变量"><a href="#数字可以编码分类变量" class="headerlink" title="数字可以编码分类变量"></a>数字可以编码分类变量</h3><p>在上面的例子中，分类变量被编码为字符串。一方面，可能会有拼写错误；但另一方面，它明确地将一个变量标记为分类变量。无论是为了便于存储还是因为数据的收集方式，分类变量通常被编码为整数。例如，在性别Sex这一特征中，用1指代男性，用2指代女性等等。现在该列包含数字1和2，而不是像“F”和“M”这样的字符串。如果有人观察表示数据集的表格，很难一眼看出这个变量应该被视为连续变量还是分类变量。但是，如果知道这些数字表示的是性别，那么很明显它们是不同的状态，不应该用单个连续变量来建模。</p><p>pandas的get_dummies函数将所有数字看作是连续的，不会为其创建虚拟变量。为了解决这个问题，你也可以使用scikit-learn的OneHotEncoder，指定哪些变量是连续的，哪些变量是离散的，你也可以将数据框转换为字符串。为了说明这一点，我们创建一个两列的DataFrame对象，其中一列包含字符串，另一列包含整数：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWI3MDU=,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"></p><p>如果使用get_dummies只会编码字符串特征，不会改变整数特征：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWI3MDU=,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"></p><p>如果你向为“整数特征”这一列创建虚拟变量，可以使用columns参数显式地给出想要编码的列。于是两个特征都会被当作分类特征处理：</p><p><img src="/2021/09/12/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/8842db22ca8744a2811a2bc623263a35.png" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h3&gt;&lt;p&gt;到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的&lt;strong&gt;连续特征&lt;/strong&gt;。对于许多应用而言，数据的收</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征工程" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    <category term="独热编码" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/"/>
    
    
    <category term="虚拟变量" scheme="https://yb705.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F/"/>
    
    <category term="独热编码" scheme="https://yb705.github.io/tags/%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81/"/>
    
    <category term="文本数据" scheme="https://yb705.github.io/tags/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/"/>
    
    <category term="分类变量" scheme="https://yb705.github.io/tags/%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——聚类评估</title>
    <link href="https://yb705.github.io/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/"/>
    <id>https://yb705.github.io/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/</id>
    <published>2021-09-12T02:03:58.000Z</published>
    <updated>2021-09-12T02:24:38.721Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>在用聚类算法时,其挑战之一就是很难评估一个算法的效果好坏,也很难比较不同算法的结果.在讨论完k均值,凝聚聚类和DBSCAN背后的算法之后,下面我们来说一下如何对聚类进行评估.</p><h3 id="用真实值评估聚类"><a href="#用真实值评估聚类" class="headerlink" title="用真实值评估聚类"></a>用真实值评估聚类</h3><p>有一些指标可用于评估聚类算法相对于真实聚类的结果，其中最重要的是<strong>调整rand指数</strong>和<strong>归一化互信息</strong>。二者都给出了定量的度量，其最佳值为1，0表示不相关的聚类（虽然ARI可以取负值）。</p><p>下面我们使用ARI来比较k均值，凝聚聚类和DBSCAN算法。</p><p>PS：数据集是make_moons，之前有在<a href="https://blog.csdn.net/weixin_43580339/article/details/119883924">DBSCAN</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">k均值聚类</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/119647712">凝聚聚类</a>中使用过，感兴趣的可以点开链接看一下，这里就不再赘述了。</p><p>首先，我们先导入数据集，并进行<a href="https://blog.csdn.net/weixin_43580339/article/details/117774014">标准化</a>（不了解标准化的可以点击链接浏览一下）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###引入数据集</span><br>standard=StandardScaler()<br>standard.fit(X)<br>X_scaled=standard.transform(X)<br></code></pre></td></tr></table></figure><p>之后，依次导入各类算法，并设立一个随机簇，作为参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN<span class="hljs-comment">###导入DBSCAN</span><br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<span class="hljs-comment">###导入k均值</span><br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering<span class="hljs-comment">###导入凝聚聚类</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>clusters=[KMeans(n_clusters=<span class="hljs-number">2</span>),AgglomerativeClustering(n_clusters=<span class="hljs-number">2</span>),DBSCAN()]<br><span class="hljs-comment">###创建一个随机的簇分配,作为参考</span><br>random_state=np.random.RandomState(seed=<span class="hljs-number">0</span>)<br>random_clusters=random_state.randint(low=<span class="hljs-number">0</span>,high=<span class="hljs-number">2</span>,size=<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure><p>最后，我们用”K均值聚类”,”凝聚聚类”,”DBSCAN”依次进行聚类操作，并把结果汇总成一个表格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics.cluster <span class="hljs-keyword">import</span> adjusted_rand_score<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>score=pd.DataFrame(index=[<span class="hljs-string">&quot;K均值聚类&quot;</span>,<span class="hljs-string">&quot;凝聚聚类&quot;</span>,<span class="hljs-string">&quot;DBSCAN&quot;</span>],columns=[<span class="hljs-string">&quot;随机分配评分&quot;</span>,<span class="hljs-string">&quot;ARI&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(clusters)):<br>    result=clusters[i].fit_predict(X_scaled)<br>    score.iloc[i,<span class="hljs-number">0</span>]=adjusted_rand_score(y,random_clusters)<br>    score.iloc[i,<span class="hljs-number">1</span>]=adjusted_rand_score(y,result)<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/f1267bb19cc54dff847c9e26f79cdc41.png" alt="在这里插入图片描述"></p><p>调整rand参数给出了符合直觉的结果，随机簇分配的分数约等于0，而DBSCAN（完美找到了期望中的聚类)的分数为1.</p><p>其实，用这种方法评估聚类时，一个常见的错误是使用accuracy_score（之前在监督学习中，用于分类器评分），而不是adjusted_rand_score或者其它聚类指标。使用精度的问题在于，它要求分配的簇标签与真实值完全匹配。但簇标签本身毫无意义——唯一重要的是哪些点位于同一个簇中。</p><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/20547fc5f00143d88ca1b78cf28c69f1.png" alt="在这里插入图片描述"></p><h3 id="在没有真实值的情况下评估聚类"><a href="#在没有真实值的情况下评估聚类" class="headerlink" title="在没有真实值的情况下评估聚类"></a>在没有真实值的情况下评估聚类</h3><p>我们刚刚展示了一种评估聚类算法的方法，但在实践中，使用诸如ARI之类的指标有一个很大的问题。在应用聚类算法时，通常没有真实值来比较结果。如果我们知道了数据的正确聚类，那么可以使用这一信息构建一个监督模型（比如分类器）。因此，使用类似ARI和NMI的指标通常仅有助于开发算法，但对评估应用是否成功没有帮助。</p><p>有一些聚类的评分指标不需要真实值，比如<strong>轮廓系数</strong>。但它们在实践中的效果并不好。轮廓分数计算一个簇的紧致度，其值越大越好，最高分数为1.虽然紧致的簇很好，但紧致度不允许复杂的形状。</p><p>接下来，我们李勇轮廓分数在make_moons数据集上比较k均值，凝聚聚类和DBSCAN：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics.cluster <span class="hljs-keyword">import</span> silhouette_score<br>score[<span class="hljs-string">&quot;轮廓系数&quot;</span>]=<span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">###新添加一列,存放轮廓系数</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>    result=clusters[i].fit_predict(X_scaled)<br>    score.iloc[i,<span class="hljs-number">2</span>]=silhouette_score(X_scaled,result)<br></code></pre></td></tr></table></figure><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/1e70fc4a8ded465bac22357d749e8a66.png" alt="在这里插入图片描述"></p><p>如你所见，k均值的轮廓分数最高，尽管我们可能更喜欢DBSCAN的结果：</p><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAeWI3MDU=,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"></p><p>对于评估聚类，稍好的策略是使用<strong>基于鲁棒性</strong>的聚类指标，这种指标先向数据中添加一些噪声，或者使用不同的参数设定，然后运用算法，并对结果进行比较。其思想是，如果许多算法参数和许多数据扰动返回相同的结果，那么它很可能是可信的。</p><p>即使我们得到一个鲁棒性很好的聚类或者非常高的轮廓分数，但仍然不知道聚类中是否有任何语义含义，或者聚类是否反应了数据中我们感兴趣的某个方面。要想知道聚类是否对应于我们感兴趣的内容，唯一的办法就是对簇进行人工分析。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>其实，<strong>聚类的应用与评估是一个非常定性的过程，通常在数据分析的探索阶段很有帮助。</strong></p><p>我们在前面分别学习了三种聚类算法：<a href="https://blog.csdn.net/weixin_43580339/article/details/119883924">DBSCAN</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">k均值聚类</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/119647712">凝聚聚类</a>。这三种算法都可以控制聚类的粒度。k均值和凝聚聚类允许你指定想要的簇的数量，而DBSCAN允许你用eps参数定义接近程度，从而间接影响簇的大小。三种方法都可以用于大型的现实世界数据集，都相对容易理解，也都可以聚类成多个簇。</p><p>当然，分解（<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">PCA</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/118416483">NMF</a>），<a href="https://blog.csdn.net/weixin_43580339/article/details/118574996">流形学习</a>与聚类一样，都是加深数据理解的重要工具，在没有监督信息的情况下，也是理解数据仅有的方法。即使在监督学习中，探索性工具对于更好地理解数据性质也很重要。通常来说，很难量化无监督算法的有用性，但这不应该妨碍你使用它们来深入理解数据。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h3&gt;&lt;p&gt;在用聚类算法时,其挑战之一就是很难评估一个算法的效果好坏,也很难比较不同算法的结果.在讨论完k均值,凝聚聚类和DBSCAN背后的算法之后,下面我们来</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="聚类评估" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/"/>
    
    
    <category term="DBSCAN" scheme="https://yb705.github.io/tags/DBSCAN/"/>
    
    <category term="K均值" scheme="https://yb705.github.io/tags/K%E5%9D%87%E5%80%BC/"/>
    
    <category term="凝聚聚类" scheme="https://yb705.github.io/tags/%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/"/>
    
    <category term="聚类评估" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——DBSCAN</title>
    <link href="https://yb705.github.io/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/"/>
    <id>https://yb705.github.io/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/</id>
    <published>2021-09-12T02:02:06.000Z</published>
    <updated>2021-09-12T02:19:07.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>与之前提到的<a href="https://blog.csdn.net/weixin_43580339/article/details/119647712">凝聚聚类</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">K均值聚类</a>类似，DBSCAN也是一个非常有用的聚类算法。它的主要优点是它不需要用户<strong>先验</strong>地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点。DBSCAN比凝聚聚类和k均值稍慢，但仍可以扩展到相对较大的数据集。</p><p>接下来，我“简单”地介绍一下算法的原理（PS：大概看一下就好）。</p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>DBSCAN的全称是具有噪声的基于密度的空间聚类应用。顾名思义，DBSCAN的原理是识别特征空间的“拥挤”区域中的点，在这些区域中许多数据点靠近在一起。这些区域被称为特征空间中的<strong>密集</strong>区域。DBSCAN背后的思想是，簇形成数据的密集区域，并由相对较空的区域分隔开。</p><p>在密集区域内的点被称为<strong>核心样本</strong>（或核心点），它们的定义如下。DBSCAN有两个参数：min_samples和eps。如果在距一个给定数据点eps的距离内至少有min_samples个数据点，那么这个数据点就是核心样本。DBSCAN将彼此距离小于eps的核心样本放到同一个簇中。</p><p>算法首先任意选取一个点，然后找到到这个点的距离小于等于eps的所有的点。如果距起始点的距离在eps之内的数据点个数小于min_samples，那么这个点被标记为<strong>噪声</strong>，也就是说它不属于任何簇。如果距离在eps之内的数据点个数大于min_samples，则这个点被标记为核心样本，并被分配一个新的簇标签。然后访问该点的所有邻居（在距离eps以内）。如果它们还没有被分配一个簇，那么就将刚刚创建的新的簇标签分配给它们。如果它们是核心样本，那么就依次访问其邻居，以此类推。簇逐渐增大，直到在簇的eps距离内没有更多的核心样本为止。然后选取另一个尚未被访问过的点，并重复相同的过程。</p><p>最后，一共有三种类型的点：核心点，与核心点的距离在eps之内的点（叫做<strong>边界点</strong>）和噪声。如果DBSCAN算法在特定数据集上多次运行，那么核心点的聚类始终相同，同样的点也始终被标记为噪声。但边界点可能与不止一个簇的核心样本相邻。因此，边界点所属的簇依赖于数据点的访问顺序。一般来说只有很少的边界点，这种对访问顺序的轻度依赖并不重要。</p><p>接下来，我们对一个数据集two_moons来应用DBSCAN算法。</p><h2 id="数据应用"><a href="#数据应用" class="headerlink" title="数据应用"></a>数据应用</h2><p>之前，我们对two_moons数据集应用过<a href="https://blog.csdn.net/weixin_43580339/article/details/119647712">凝聚聚类</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">K均值聚类</a>算法，但是聚类结果却都不理想：<br><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_Q1NETiBAeWI3MDU=,size_23,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"><br>既然如此，我们用DBSCAN算法来试试。</p><p>要注意的是，虽然DBSCAN不需要显式地设置簇的个数，但设置eps可以隐式地控制找到的簇的个数。使用StandardScaler或MinMaxScaler对数据进行缩放之后，有时会更容易找到eps的较好取值，因为使用这些缩放技术将确保所有特征具有相似的范围。<br>（PS：关于数据缩放方面的内容可以看一看这篇文章：<a href="https://blog.csdn.net/weixin_43580339/article/details/117774014">数据预处理与缩放</a>，具体的我就不再赘述了。）</p><p>所以我们先对原有数据集进行标准化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>standard=StandardScaler()<br>standard.fit(X)<br>X_scaled=standard.transform(X)<br></code></pre></td></tr></table></figure><p>与凝聚聚类类似，DBSCAN也不允许对新的测试数据进行预测，所以我们将使用fit_predict方法来执行聚类并返回簇标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN<br><span class="hljs-keyword">import</span> mglearn<br>dbscan=DBSCAN()<span class="hljs-comment">###更像是拿到一份一无所知的数据，然后直接去进行分类的感觉</span><br>clusters=dbscan.fit_predict(X_scaled)<span class="hljs-comment">###与凝聚聚类类似，DBSCAN也不允许对新的测试数据进行预测，所以使用fit_predict方法来执行聚类并返回簇标签</span><br>plt.scatter(X_scaled[:,<span class="hljs-number">0</span>],X_scaled[:,<span class="hljs-number">1</span>],c=clusters,cmap=mglearn.cm2,s=<span class="hljs-number">60</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_Q1NETiBAeWI3MDU=,size_24,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述"><br>可以看到，利用DBSCAN的默认设置，算法找到了两个半圆形并将其分开。</p><p>由于算法找到了我们想要的簇的个数（2个），因此参数设置的效果很好。如果将eps减小到0.2（默认值为0.5），我们将会得到8个簇，这显然太多了。将eps增大到0.7则会导致只有一个簇。</p><h2 id="实例应用"><a href="#实例应用" class="headerlink" title="实例应用"></a>实例应用</h2><p>现在，我们用另一个数据集来讲解DBSCAN的参数——eps和min_samples。</p><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类：https://www.kaggle.com/andrewmvd/fetal-health-classification</a></p><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/20210326150227435-16314131175368.png" alt="20210326150227435"></p><p>该数据包含胎儿心电图，胎动，子宫收缩等特征值，而我们所需要做的就是通过这些特征值来对胎儿的健康状况(fetal_health)进行分类。</p><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/20210326150323351-16314131322859.png" alt="20210326150323351"></p><p>数据集包含从心电图检查中提取的2126条特征记录，然后由三名产科专家将其分为3类，并用数字来代表：1-普通的，2-疑似病理，3-确定病理。</p><p><strong>2.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>3.对数据集进行缩放以及聚类</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>standard=StandardScaler()<br>standard.fit(train)<br>X_scaled=standard.transform(train)<br>dbscan=DBSCAN()<span class="hljs-comment">###更像是拿到一份一无所知的数据，然后直接去进行分类的感觉</span><br>clusters=dbscan.fit_predict(X_scaled)<br>health[<span class="hljs-string">&quot;cluster&quot;</span>]=clusters<br><span class="hljs-built_in">set</span>(health[<span class="hljs-string">&quot;cluster&quot;</span>])<span class="hljs-comment">###剔除某列的重复值，查看这一列都有哪些具体元素</span><br></code></pre></td></tr></table></figure><p><img src="/2021/09/12/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94DBSCAN/390117b1f46d4de587d8196c6c4c55fe-16314130482275.png" alt="在这里插入图片描述"></p><p>其中set()方法是用于去重，并查看这一列都有哪些具体元素。</p><p>可以看到，在默认设置的情况下，该数据集被DBSCAN分成了6类（其中-1是指噪声）。</p><p>接下来，我们尝试不同的参数组合，来具体讲解一下：</p><p><img src="https://img-blog.csdnimg.cn/3827acded67c414396ba10c0b49e694d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_Q1NETiBAeWI3MDU=,size_28,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>其实参数eps在某种程度上更重要，因为它决定了点与点之间“接近”的含义。将eps设置得非常小，意味着没有点是核心样本，可能会导致所有点都被标记为噪声。将eps设置得非常大，可能会导致所有点形成单个簇。而设置min_samples主要是为了判断稀疏区域内的点被标记为异常值还是形成自己的簇。如果增大min_samples,任何一个包含少于min_samples个样本的簇现在将被标记为噪声。因此，min_samples决定簇的最小尺寸。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在使用DBSCAN时，你需要谨慎处理返回的簇分配。如果使用簇标签对另一个数据进行索引，那么使用-1表示噪声可能会产生意料之外的结果。</p><p><a href="https://www.yyb705.com/">个人博客</a>：<a href="https://www.yyb705.com/">https://www.yyb705.com/</a><br>欢迎大家来我的个人博客逛一逛，里面不仅有技术文，也有系列书籍的内化笔记。<br>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;与之前提到的&lt;a href=&quot;https://blog.csdn.net/weixin_43580339/article/details/11964</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="DBSCAN" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/DBSCAN/"/>
    
    
    <category term="聚类" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="DBSCAN" scheme="https://yb705.github.io/tags/DBSCAN/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(附录)-外展推理</title>
    <link href="https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E5%A4%96%E5%B1%95%E6%8E%A8%E7%90%86/"/>
    <id>https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E5%A4%96%E5%B1%95%E6%8E%A8%E7%90%86/</id>
    <published>2021-09-12T01:57:17.000Z</published>
    <updated>2021-09-12T02:01:05.468Z</updated>
    
    <content type="html"><![CDATA[<h2 id="在无结构情况下解决问题的方法"><a href="#在无结构情况下解决问题的方法" class="headerlink" title="在无结构情况下解决问题的方法"></a>在无结构情况下解决问题的方法</h2><p>前面提到的”界定问题”把解决问题描绘成一个不断进行逻辑分析的过程,目的是发现并展示导致非期望结果的内在结构.<strong>如果问题是我们不喜欢该结构造成的结果,解决方案就是调整结构.</strong></p><p>当然,有的时候,我们无法解释问题,发生这种情况的原因有3个:</p><blockquote><p>造成非期望结果的结构根本不存在,比如你要发明一件新东西时.</p><p>结构无形,如空气或者是wifi等,只能分析其结果.</p><p>结构不能解释结果,比如无论怎么保养,工具都会生锈.</p></blockquote><p>遇到上述的情况时,我们可以采取另一种方法——外展推理.</p><h3 id="分析性外展推理"><a href="#分析性外展推理" class="headerlink" title="分析性外展推理"></a>分析性外展推理</h3><p>前面我们已经讲过演绎推理和归纳推理,那么接下来我通过对比三个方面(规则,情况,结果),来说明它们与外展推理之间的区别:</p><p>演绎推理:</p><table><thead><tr><th>规则</th><th>如果价格定得太高,销量将下降</th><th>如果A,则B</th></tr></thead><tbody><tr><td>情况</td><td>价格定的太高</td><td>A</td></tr><tr><td>结果</td><td>所以销量下降</td><td>则B</td></tr></tbody></table><p>归纳推理:</p><table><thead><tr><th>规则</th><th>提高价格</th><th>A</th></tr></thead><tbody><tr><td>情况</td><td>销量下降</td><td>B</td></tr><tr><td>结果</td><td>销量下降的原因可能是价格太高</td><td>如果A,则很可能B</td></tr></tbody></table><p>外展推理:</p><table><thead><tr><th>结果</th><th>销量下降</th><th>B</th></tr></thead><tbody><tr><td>规则</td><td>销量下降通常由于价格太高</td><td>如果A,那么B</td></tr><tr><td>情况</td><td>检查价格是否确实太高</td><td>可能A</td></tr></tbody></table><p>分析性的解决问题的方法包括关注现状(R1,非期望结果),了解背景,寻找非期望结果产生的原因(规则),并检验是否已经找到(情况).这和上面描述的外展推理有异曲同工之妙.</p><h3 id="科学性外展推理"><a href="#科学性外展推理" class="headerlink" title="科学性外展推理"></a>科学性外展推理</h3><p>上面讨论了分析性的解决问题的方法,这里主要讨论的是创造性或科学性的方法,二者的区别主要在于,我们知道导致结果的结构,而科学家在运用后者时还不知道,即前者已经有了两个基本要素,由此便能推理出第三个要素,而后者,科学家们必须创造出第二个要素,才能推理出第三个要素.</p><p>通常,他们会采用传统的科学方法:</p><ul><li>设想一个能解释结果的结构.</li><li>设计一项能证实或排除假设的实验.</li><li>进行实验,得出明确的是或否的答案.</li><li>重复这一过程,进行分支假设或后续假设,以界定其它可能性,如此循环往复.</li></ul><p>这种科学方法的特点就是构造假设和设计实验.</p><h4 id="提出假设"><a href="#提出假设" class="headerlink" title="提出假设"></a>提出假设</h4><p>科学的假设不是凭空得来的,而是仔细研究问题产生背景的构成要素后自然而然引出的.举个例子,如果你的问题是想找到一种让人们不用喊叫也能长距离沟通的方法,你自然会去寻找改变声音或增强听力的各种方法,由此形成假设.</p><p>遗憾的是,如何提出假设并没有什么诀窍.通常需要一定的天赋,你才能发现你所了解的问题和所了解的世界之间的相似之处.</p><h4 id="设计实验"><a href="#设计实验" class="headerlink" title="设计实验"></a>设计实验</h4><p>提出假设后,下一步就是设计能够肯定或否定假设的实验.这个过程需要的思维就是:”如果假设是有效的,接下来必然会发生什么呢?我要设计实验来证明它确实会发生.”用外展推理的方法表述就是:</p><blockquote><p><strong>结果</strong>:我观察到的非期望结果A</p><p><strong>规则</strong>:A的发生可能是因为B</p><p><strong>情况</strong>:如果B,那么C必然发生.检查接下来是否发生C.</p></blockquote><p>设计实验的意义在于,通过实验得到的最终答案一定是明确的”是”或”非”.改变背景中的某一个条件还不足以使你”看清楚发生了什么”,而实验结果却能够让你明确地肯定或者否定最初所做的假设.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;在无结构情况下解决问题的方法&quot;&gt;&lt;a href=&quot;#在无结构情况下解决问题的方法&quot; class=&quot;headerlink&quot; title=&quot;在无结构情况下解决问题的方法&quot;&gt;&lt;/a&gt;在无结构情况下解决问题的方法&lt;/h2&gt;&lt;p&gt;前面提到的”界定问题”把解决问题描绘成一个不</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="推理逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%8E%A8%E7%90%86%E9%80%BB%E8%BE%91/"/>
    
    <category term="附录" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%8E%A8%E7%90%86%E9%80%BB%E8%BE%91/%E9%99%84%E5%BD%95/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="外展推理" scheme="https://yb705.github.io/tags/%E5%A4%96%E5%B1%95%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(九)-结构化分析问题</title>
    <link href="https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/"/>
    <id>https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/</id>
    <published>2021-09-12T01:54:22.000Z</published>
    <updated>2021-09-12T01:56:25.861Z</updated>
    
    <content type="html"><![CDATA[<h3 id="结构化分析问题"><a href="#结构化分析问题" class="headerlink" title="结构化分析问题"></a>结构化分析问题</h3><p>一般分析问题的流程如下所示：</p><ol><li>信息资料</li><li>描述发现</li><li>得出结论</li><li>提出解决方案</li></ol><p>从上述步骤来看，分析问题的起点要从收集信息资料开始。因此，最省脑筋的做法就是尽可能多的，尽可能详细的收集资料。可以预见的是，直接分析大量的资料会很麻烦（主要是显得并不聪明）。实际上，我们可以用一些辅助手段来节省精力，譬如诊断图，树状图等。中规中矩地顺着流程走是没有问题的，但是没有谁会拒绝省力吧。不过，为了显示方法的简便，我还是会先介绍一下直接分析的方法。</p><h3 id="从信息资料入手"><a href="#从信息资料入手" class="headerlink" title="从信息资料入手"></a>从信息资料入手</h3><p>最早应用问题分析的行业就是咨询公司。当时，咨询公司刚刚兴起，底蕴欠缺，缺少各个行业的知识，顾问们没有合适稳定的数据进行分析。所以他们只能囫囵吞枣式地收集大量相关数据。而这也就造成了初期数据收集的困难，牵扯了从业人员大量的精力。</p><p>就算最后得到了完整的数据，要想对其进行整理，并从中得到有效的发现也是非常困难的。有一家高级咨询公司在受到采访时说过，他们初期收集到的数据只有40%是有用的。后续分析的困难可想而知。</p><p>因此，咨询公司后来开始采用另一种方法来分析问题——结构化分析问题。实际上，这就是一种重复性的分析方法：</p><ol><li>对问题提出多种假设</li><li>重复性进行实验，依次验证各个假设，剔除不符合实验结果的假设。</li><li>详细描述实验结果，得出结论</li><li>提出解决方案</li></ol><p>上面说的内容是整体分析流程，接下来我说一下辅助我们进行分析的方法。</p><h3 id="诊断框架"><a href="#诊断框架" class="headerlink" title="诊断框架"></a>诊断框架</h3><p>我们都知道，医生不是患者，他没有得过相关病症，但是他却可以为患者诊断病情，并为患者开药，给出康复方案。之所以如此，就是因为他们有一套自己的诊断框架。举个栗子，有个患者头痛，那么就会有下面的分析框架：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210903135911789.png" alt="image-20210903135911789"></p><p>首先判断是生理问题，还是心理问题。如果是生理问题，那么是外部问题还是内部问题。如果是外部问题，那么具体又是什么原因。就这样自左到右依次判断，直到分析出具体的原因。</p><p>问题诊断框架有三种，呈现有形结构，寻找因果关系，构建分类结构。接下来，我依次讲解这三种结构。</p><h4 id="呈现有形结构"><a href="#呈现有形结构" class="headerlink" title="呈现有形结构"></a>呈现有形结构</h4><p>任何企业行动或者行业交易流程都可以用一个形状结构表示出来。通过这个结构图，我们可以很清晰地分析出问题出现在哪里，举个栗子：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210903143134332.png" alt="image-20210903143134332"></p><p>上面就是一个简单的销售结构，清晰地展现出了整个交易流程。如果销量出现下滑，我们可以顺着箭头依次分析是哪个部分出现问题，并给出具体的解决方案。如果要提高销售效率，我们也可以调整各个部分的成本占比，或者缩短某部分的运转周期。</p><h4 id="寻找因果关系"><a href="#寻找因果关系" class="headerlink" title="寻找因果关系"></a>寻找因果关系</h4><p>诊断问题的第二种方法是寻找具有因果关系的要素,行为或任务,经过分析得到最终结果.</p><h5 id="1-财务结构"><a href="#1-财务结构" class="headerlink" title="1.财务结构"></a>1.财务结构</h5><p>为了找出投资回报率低的原因,可以画出企业的财务结构示意图:</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210906143451.png" alt="微信图片_20210906143451"></p><p>将各项数据填入表中,很快就能判断出,问题的产生是由于销售收入比去年少,还是由于成本太高,还是其它.随后,对每个要素进行细分,找出它们主要受哪些因素影响.实际上,完成了结构图,接下来只需要确定问题的是出在哪个方面就可以了.</p><h5 id="2-任务结构"><a href="#2-任务结构" class="headerlink" title="2.任务结构"></a>2.任务结构</h5><p>在展示企业必须完成的重要任务时,采用树状图能呈现得更深入,更清晰,如下图所示:</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_202109061434511.png" alt="微信图片_202109061434511"></p><p>通过结构图,我们可以知道哪些是公司的关键任务,并能通过分析树状图中的有关数据(趋势,敏感性,行业比较等),决定应该优先完成哪些任务,以增加收益.</p><h5 id="3-行动结构"><a href="#3-行动结构" class="headerlink" title="3.行动结构"></a>3.行动结构</h5><p>有时,我们还可以用树状图分析哪些行动会导致非期望结果,比如,导致高成本或超长安装时间的行动:</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210906144223826.png" alt="image-20210906144223826"></p><p>在得到完整的图表之后，我们就可以进一步分析：什么原因可能使每人每班的工作时间更长？工人效率低？还是工作本身的耗时？亦或者是其它意外？就这样持续分析，直到找到最终原因。</p><h4 id="分类结构"><a href="#分类结构" class="headerlink" title="分类结构"></a>分类结构</h4><p>诊断问题的第三种方法是把所有可能的原因按相似性分类，这样做有一个前提，即预先分类有助于综合分析各种事实：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210906145304388.png" alt="image-20210906145304388"></p><p>除了相似性分类，还有另一种方法即选择结构。这类树状图与用来寻找导致非期望结果的原因的行动结构相关，但每次只能进行简单的是非选择，二选一，一直进行到能准确了解可能的原因为止：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210906145822471.png" alt="image-20210906145822471"></p><h3 id="使用诊断框架"><a href="#使用诊断框架" class="headerlink" title="使用诊断框架"></a>使用诊断框架</h3><p>有的人可能会问：“我怎么知道应该在什么时候建立哪一种框架？”。其实答案取决于你对所要分析的目标领域了解多少。好的解决方案不是平空抽象得来的，它要求你对所在领域——制造，营销，信息系统等有全面的了解。大量深入的关于目标领域的知识必不可少。这也就是为什么很多公司在招聘数据分析师的时候要求对相关业务有一定的了解。</p><h4 id="分析客户问题"><a href="#分析客户问题" class="headerlink" title="分析客户问题"></a>分析客户问题</h4><p>某家公司为了加快销售周期，特制定了相关计划，并开始实行。但实施之后，发现销售效率相较之前反而下降了不少。现在，公司负责人要求咨询顾问给出解决方案。</p><p>好的，现在客户已经提出来了问题。其实，销售往往来源于公司基层，所以现在的问题就是基层的销售效率低。那么我们需要制定的第一个诊断框架就是关于基层工作流程的。自然而然的，咨询顾问要开始收集有关资料，但要有目的，有针对性的收集，而不是采用以往面面据道的方式。</p><p>如果咨询顾问沿用过去标准的资料收集方法，就所有问题去采访公司员工，他可能会得到大量需要整理，综合和分析的资料，不仅不能完全理解和吸收，而且很难客观地说出哪些有用，哪些无用。</p><h4 id="分析方法"><a href="#分析方法" class="headerlink" title="分析方法"></a>分析方法</h4><p>我们通过之前提到的一个例子来讲一下具体的分析流程：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210903143134332.png" alt="image-20210903143134332"></p><p>1.观察公司交易流程，并描绘结构图。</p><p>2.根据上面的结构图，结合现实中出现的问题，我们列出所有可能导致问题发生的原因。譬如：营销周期长，物流仓储紧张，定价不合理等。</p><p>3.我们根据有可能出现问题的部分搜集相关资料，当然是要有针对性，有目的性的收集。有的时候，“面面俱到”不是一个褒义词，而是一个贬义词。</p><p>4.寻找解决方案。还是那句话，要想提出创造性的解决方案，咨询顾问必须要全面掌握相关行业的业务知识。而对于缺乏敏锐洞察力的人来说，逻辑树往往能够帮助他找到最终结果。下面，我来说一下逻辑树。</p><h3 id="逻辑树"><a href="#逻辑树" class="headerlink" title="逻辑树"></a>逻辑树</h3><p>其实分析问题的过程就是寻找5个问题答案的过程：</p><blockquote><ol><li>问题是否存在</li><li>问题是什么</li><li>问题发生原因是什么</li><li>我们应该做什么</li><li>我们是否应该这样做</li></ol></blockquote><p>其中问题1~3的解答方法我们已经找到了，也就是诊断框架。而问题4和问题5就要着落在接下来要讲的内容——逻辑树上了。</p><h4 id="寻找解决方法"><a href="#寻找解决方法" class="headerlink" title="寻找解决方法"></a>寻找解决方法</h4><p>要想利用逻辑树寻找解决方法，首先要做的就是针对发现的问题，遵循<strong>“相互独立，完全穷尽”</strong>的原则，列出逻辑细化的所有可能性。譬如：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210908131845475.png" alt="image-20210908131845475"></p><p>然后再由专业的分析师或者统计师计算评估各个方法所能产生的收益，风险等，最后作出正确的选择。</p><p>除了这种寻找解决方法之外，逻辑树还可以用于分析战略性机会：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210908132657792.png" alt="image-20210908132657792"></p><h4 id="检查文章逻辑"><a href="#检查文章逻辑" class="headerlink" title="检查文章逻辑"></a>检查文章逻辑</h4><p>逻辑树还有另一个功能，就是<strong>检查</strong>文章的逻辑。接下来，我通过一个例子来更进一步地讲解如何通过逻辑树复查：</p><blockquote><ol><li>通过在每个分厂改进操作方法和实施简单的低成本工程项目，我们能降低多少能耗？</li><li>假如通过改进操作流程能大量降低能耗，那么与竞争对手相比，我们的成本优势或劣势有多大？是否可持续？</li><li>一个不很充裕的资金充裕计划，对降低能源成本可以带来多大的领先优势？</li><li>为显著提高我们的竞争优势，理想的能源发展计划是什么？</li><li>为了在短期和长期都能控制成本，保证供应，最佳的燃料组合和采购安排是什么？</li><li>我们的资金计划评价和批准流程能否使最佳能源计划得到迅速制定和实施，使所有工厂获得最大收益？</li><li>为了有效地管理必要的能源相关业务——如组织，职责分配，技能，资源等，需要什么样的人力资源？</li><li>公司的能源战略和相应的经营计划是什么？</li></ol></blockquote><p>整理成逻辑树后结果如下：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/image-20210909161233521.png" alt="image-20210909161233521"></p><p>你会发现问题7与主题无关，问题1，2，6与修理现有设备以减少使用BTU有关，问题3，4与采用新设备以减少使用BTU有关，问题5与维持现有设备，使用低成本燃料有关，问题6与增加新设备，使用低成本燃料有骨干，问题8则与总体缩减能源开支有关。</p><p>记住，各组思想都来自于大脑的分析活动。在试图解决某个问题的时候，你的分组思路可能来源于指导你进行分析的某种思维结构。把你的思路和这样的思维结构相对照，可以帮助你证明它们在逻辑上是否有效。（PS：简而言之，就是“理想”与“现实”的距离）</p><h3 id="是非问题"><a href="#是非问题" class="headerlink" title="是非问题"></a>是非问题</h3><p>“是非问题”是判断解决方案是否有效的一个最简单清晰的标准。举个栗子：“我们要投资多少才不会超过标准线？”。当拿到这个问题的时候，我们可能会想方设法地去计算具体数值，或者是评估风险。虽然这个问题很明确地阐述了成功的标准，但是要想求出明确的数字，我们可能会经历复杂的计算，我们甚至没有办法去验证计算结果的准确性或者说合理性。而“我们的投资是否超过了标准线？”这种是非问题就可以很好地替换成为“关键问题”。</p><h4 id="对是非问题的误解"><a href="#对是非问题的误解" class="headerlink" title="对是非问题的误解"></a>对是非问题的误解</h4><p>也许一些咨询公司在解决问题时找到了有效使用是非问题分析的方法，不过有一些人采用的方法有些混乱，举个例子：</p><blockquote><ol><li>从客户的问题入手。</li><li>提出主要是非问题和次要是非问题。</li><li>提出假设（即是非问题的可能答案）。</li><li>确定要回答这些问题所需要的资料。</li><li>分派任务等。</li><li>得出结论，提出建议。</li><li>检查结论和建议的有效性。</li></ol></blockquote><p>首先是第一步，“是非问题”不应来自“客户问题”，而应该来自导致R1（非期望结果）的背景。而客户的问题通常是对R2（期望结果）的反应。</p><p>其次，从“客户问题”到“主要是非问题和次要是非问题”有一个思维跳跃。我不知道主要是非问题和次要是非问题来自何处，也不知道如何判断我列出的是非问题是否完全穷尽。</p><p>另外，是非问题和假设问题之间也存在混乱。是非问题来自假设，因为你已经假设问题存在于你建立的分析框架中。但是，这种区别也没有任何意义。从某种程度上来说，两者之间可以相互替换。</p><p>最后，一些人把用来形成可供客户选择的解决方案和描绘行动可能结果的逻辑树也称为是非问题分析。我们知道，利用逻辑树来形成备选方案是很常用的方法，但是把它称为是非问题分析很可能会引起不必要的混乱，因为是非问题分析所用的逻辑树（“二选一”）和诊断框架的逻辑树完全不同。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;结构化分析问题&quot;&gt;&lt;a href=&quot;#结构化分析问题&quot; class=&quot;headerlink&quot; title=&quot;结构化分析问题&quot;&gt;&lt;/a&gt;结构化分析问题&lt;/h3&gt;&lt;p&gt;一般分析问题的流程如下所示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;信息资料&lt;/li&gt;
&lt;li&gt;描述发现&lt;/li</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="解决问题的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第九章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B9%9D%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="结构化分析问题" scheme="https://yb705.github.io/tags/%E7%BB%93%E6%9E%84%E5%8C%96%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98/"/>
    
    <category term="逻辑树" scheme="https://yb705.github.io/tags/%E9%80%BB%E8%BE%91%E6%A0%91/"/>
    
    <category term="诊断框架" scheme="https://yb705.github.io/tags/%E8%AF%8A%E6%96%AD%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(八)-界定问题</title>
    <link href="https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/"/>
    <id>https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/</id>
    <published>2021-09-12T01:52:19.000Z</published>
    <updated>2021-09-12T01:53:41.783Z</updated>
    
    <content type="html"><![CDATA[<h2 id="界定问题"><a href="#界定问题" class="headerlink" title="界定问题"></a>界定问题</h2><p>大多数商务文章的目的都是为了解决问题，那么如何判断问题是否存在呢？要想知道是否存在问题，就是要清楚通过努力得到的结果（正常发展）或者说是现状，与想要完成的目标或者说是期望之间是否存在差距。而“问题”就是导致这个差距存在的“罪魁祸首”。</p><p>本章主要介绍界定问题方面的内容。</p><h3 id="界定问题的框架"><a href="#界定问题的框架" class="headerlink" title="界定问题的框架"></a>界定问题的框架</h3><p>刚才提到了判断问题需要从两个因素入手，一个是现状（R1），一个是期望（R2）。实际上，问题往往是有悖于正常流程的，是在特定背景下由某个或某些特定条件造成的。但无论是在什么复杂的条件下，我们都可以从构造框架或者是遵循固定模式来去分析。</p><h4 id="说明框架的要素"><a href="#说明框架的要素" class="headerlink" title="说明框架的要素"></a>说明框架的要素</h4><p>为了说明如何搭建框架，我们先讲一个例子：</p><p>有一家钢材公司，销售人员在确定潜在客户的名单之后，通过向客户发送包含广告，报价等信息的电子邮件，来获取订单，促成交易。在这套销售流程下，公司每月的销售额以10%的速率稳步增长，但到了11月份，销售额环比下降了20%。现在，公司要求你找出问题。</p><p>话不多说，先上结论：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/image-20210827114453797.png" alt="image-20210827114453797"></p><p>其实在对比现状和期望（目标）的时候，我们就能够表述出问题的具体内容了。并且我们可以拆分交易流程的各个部分，然后针对每个部分提出解决问题的方法。当然，将整体流程拼凑起来，并通过其它维度去分析解决方案也是可以的（内部原因，外部原因，隐含原因）。</p><p>其实写到这里，相信大家已经感受到说明框架的优点了。它是思维的先导，也是金字塔搭建第一步，同时可以辅助你精准地找到问题和解决方案，便于总结。</p><h4 id="框架转换成序言"><a href="#框架转换成序言" class="headerlink" title="框架转换成序言"></a>框架转换成序言</h4><p>其实，从框架转换成序言是非常简单的，只需要在原有的框架上遵循从左到右，从上到下的顺序去叙述就可以了。接下来，我用一个例子来说明如何通过问题界定框架将复杂的情况转换成序言：</p><blockquote><p>一，假设你是一家大型包装食品的生产商，这时超市尚处于新兴阶段。尽管已经做了大量的新产品实验，但你还是希望，产品在全面上市之前，能在超市的货架上进行为期一周左右的试销。你向超市说明了想法，但他们不同意，害怕你的产品会打破现有秩序。当你同意支付一定的试销费用后，他们又接受了你的要求。</p><p>二，一段时间后，超市已经成功发展成为连锁超市，试销费用也上涨到每周2万元。这样的数额让人难以接受，于是你组织研究小组讨论这个问题，但除了拒绝付款之外，没有想出其它办法。</p><p>三，超市同样拒绝了为你的产品进行为期一周的试销售。</p></blockquote><p>首先，我们将其以多层级的结构展示出来：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/image-20210830140155875.png" alt="image-20210830140155875"></p><p>虽然故事很复杂，但只要将其展开并逐条分析，就可以很容易地在序言中用几句话描述清楚，就像是上面提到的一样从左到右再向下，把读者了解的最终事实作为冲突。</p><blockquote><p>背景：为让超市同意将我司的新产品放在其货架上进行试销，我司一直在想超市支付试销费用。这项费用每年都在增加，目前已经加到了2万元。对于公司来说，已经有点超标，所以我们决定拒绝支付该费用。</p><p>冲突：超市也拒绝让我们试销新产品。</p><p>疑问：我们要如何解决这个问题？</p></blockquote><h3 id="结构框架的构成因素"><a href="#结构框架的构成因素" class="headerlink" title="结构框架的构成因素"></a>结构框架的构成因素</h3><p>前面我们提到了问题框架有四个因素：</p><ol><li>切入点/序幕</li><li>困惑/疑问</li><li>非期望问题，现状（R1）</li><li>期望问题，目标（R2）</li></ol><p>接下来，我们依次介绍这些组成成分。</p><h4 id="切入点-流程-序幕"><a href="#切入点-流程-序幕" class="headerlink" title="切入点/流程/序幕"></a>切入点/流程/序幕</h4><p>大家应该已经注意到，小标题后面加了“序幕”这个词。这是因为问题框架就像是一场舞台剧一样。设想一下，我们坐在一个昏暗的剧场里，伴随着主持人的开场白，舞台大幕缓缓拉开，并交代了在特定的时间，特定的地点，某个特定的人物发生了特定的事件。这是不是与框架里面的流程相同？</p><p>需要注意的是，在这里，我们并不需要花费大量的口舌在背景方面，尽量精简一点。譬如说“一家快递驿站要同时向三个小区配送快件”，只需这一句话，就能让读者在大脑中产生相应的景象：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/image-20210830143810905.png" alt="image-20210830143810905"></p><h4 id="困惑-问题"><a href="#困惑-问题" class="headerlink" title="困惑/问题"></a>困惑/问题</h4><p>问题会使原有的流程产生偏差，进而导致非期望的结果（R2）。或者说为了达到期望结果或者说目标（R1），我们需要跨越的障碍，克服的困难，也可以称为问题。</p><p>而造成问题的原因分为三种：</p><p>内部原因：公司制度，人员变动，业务调整等</p><p>外部原因：政府政策，市场涨跌，竞争公司等</p><p>近期发生的未知原因：近期发生的，超出意料之外的原因，或者是不包括在过往经验中。</p><p>需要提一下，有的时候，我们并不能精准的把握到读者心中的疑问，或者是客户遇到的具体问题，毕竟我们不是当事人。那么，这个时候运用春秋笔法，或者是直接跳过这一段也是可以的。</p><h4 id="现状，非期望结果（R1）"><a href="#现状，非期望结果（R1）" class="headerlink" title="现状，非期望结果（R1）"></a>现状，非期望结果（R1）</h4><p>R1指的是某种原因导致事情脱离了原有的运行轨迹所导致的结果。要注意的是，R1并不是特指已经发生的事，也可以是将要发生的事。比如说市场动荡可能会导致公司的营收下降，这里的“营收下降”就并没有发生，只是有可能会发生。并且，一个问题可能会引起多个非期望结果，所以要用一些简短的语句来描述这个因素。</p><h4 id="目标，期望结果（R2）"><a href="#目标，期望结果（R2）" class="headerlink" title="目标，期望结果（R2）"></a>目标，期望结果（R2）</h4><p>R2就是我们想要的结果，或者说是想要达到的目标。也就是说，要想知道问题是否被解决，可以通过有没有达到目标来判断。所以目标明确是非常重要的，比如降低销售周期30%，营收环比增加50%，负债率降低70%等。当然，目标并不一定要用数字来衡量，也可以用完成状态来定义，譬如说经过制度改革，公司员工的素质得到了提升。</p><h3 id="发掘读者的疑问"><a href="#发掘读者的疑问" class="headerlink" title="发掘读者的疑问"></a>发掘读者的疑问</h3><p>问题的基本内容展开后，下一步就是寻找读者的疑问。读者在你阐述问题的不同阶段会有不同的疑问。通常是以下7中情况：</p><p><strong>最常见的状况</strong></p><ol><li>不知道如何将R1转化为R2</li><li>知道如何将R1转化为R2，但不确定是否正确</li><li>知道将R1转化为R2的正确方案，但不知道如何实施</li></ol><p><strong>最常见情况的变形</strong></p><ol><li>知道将R1转化为R2的方案，并且已经实施，但在实施过程中发现方案行不通</li><li>制订了好几个解决方案，但不知道选哪一个</li></ol><p><strong>可能但不常见的情况</strong></p><ol><li>知道R1，但不能具体描述R2，所以无法找到解决方案</li><li>知道R2，但不清楚现在是否处于R1（这是典型的标杆比对），无法确定是否存在问题</li></ol><p><strong>注：其实上述模式只是提供了一个方向，虽然说“万事开头难”，思路是解决问题所需要的最重要的部分。但是上面的7种模式不可能囊括所有场景，盲目跟进可能会使你误入歧途。我们也是可以根据具体的事情去自主分析（倒不如说我很建议这样去做），毕竟固定的模式也是来源于自主分析的重复性。需知任何现象都是来自于详细的状态，任何观点也都是有明确的原因，任何解决方案也都是有具体的数据做支撑的。所以只要分析过程中的每一步都保证明确，有具体的来源就可以。</strong></p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>最后，我们讲一个例子来复盘一下如何界定问题，以及如何将其转化为序言。</p><blockquote><p>有一家物流仓储公司，可以存储遍布天津北辰，西青，津南三个地区共490个快递配送点的货物，并提供中转运输服务。目前，这个仓储点勉强为438个配送点提供服务。不过，考虑到市场份额占有率的与日俱增，总公司决定明年要将天津地区的快递配送点增加到600个。但是，公司希望将可以选择一种策略，既能保证投资和运营成本最低，又不必改变公司现有的处理速度和全局战略。</p></blockquote><p>首先，我们将之前讲的各个构成元素填补好，搭建问题界定框架。</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/image-20210902144017923.png" alt="image-20210902144017923"></p><p>然后，遵循自左到右再到下的顺序，依次填写序言的各个部分：</p><blockquote><p>背景：一个仓储点理应可以供应490家配送点，但实际只能勉强供应438家，并且明年预计会增加到600家配送点，仓储点的存储能力会出现不足。公司希望及时采取措施保证供货能力。方法很多，可以扩建一到两个仓储点，或者是多个方法的组合。</p><p>冲突：不同的解决方案对投资回报率的影响不同。公司希望方案在保证投资和运营成本最低的同时，维持现有的处理速度和全面布局。</p><p>疑问：应该采用何种解决方案？</p></blockquote><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/image-20210902145420127.png" alt="image-20210902145420127"></p><p>到这里，序言到金子塔的部分就完成了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;界定问题&quot;&gt;&lt;a href=&quot;#界定问题&quot; class=&quot;headerlink&quot; title=&quot;界定问题&quot;&gt;&lt;/a&gt;界定问题&lt;/h2&gt;&lt;p&gt;大多数商务文章的目的都是为了解决问题，那么如何判断问题是否存在呢？要想知道是否存在问题，就是要清楚通过努力得到的结果（正常发</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="解决问题的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第八章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E5%85%AB%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="界定问题" scheme="https://yb705.github.io/tags/%E7%95%8C%E5%AE%9A%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(七)-概括各组思想</title>
    <link href="https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E6%A6%82%E6%8B%AC%E5%90%84%E7%BB%84%E6%80%9D%E6%83%B3/"/>
    <id>https://yb705.github.io/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E6%A6%82%E6%8B%AC%E5%90%84%E7%BB%84%E6%80%9D%E6%83%B3/</id>
    <published>2021-09-12T01:49:16.000Z</published>
    <updated>2021-09-12T01:51:47.395Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>这篇文章主要讲一下金字塔原理的第一原则:金字塔结构中的每一层思想都是下一层思想的精炼,概括,总结.而之所以这么说,便是因为每一层思想都是来源于下一层,同时每一层思想也都为上一层思想提供支持.</p><p>不过,无论是写文章,还是生活中的其它事务,概括总结都并不是一件容易的事情,所以人们也很少愿意在这方面去费脑筋.因此,我们在阅读文章的时候,常常能看到作者的”敷衍了事”:</p><blockquote><p>公司遇到的三个问题.</p><p>整个流程的四个步骤.</p><p>政策带来的五个影响.</p></blockquote><p>在上面的内容中,每句话都可以说是概括总结,但是它们却都缺少”灵魂”.之所以会这样,是因为这几句话并不是某种思想的提炼,精华,而只是将有相同关系的内容做了个分类整理.换言之,就是这部分概括总结”缺少思想”.</p><h3 id="总结句要避免”缺少思想”"><a href="#总结句要避免”缺少思想”" class="headerlink" title="总结句要避免”缺少思想”"></a>总结句要避免”缺少思想”</h3><p>对于这部分内容,我们先从一个例子开始说:</p><blockquote><p>为什么陈独秀和李大钊在&lt;&lt;新青年&gt;&gt;杂志上发表文章?</p><ol><li>陈独秀和李大钊都是早期的马克思主义者.</li><li>陈独秀和李大钊都提倡新文化运动.</li><li>陈独秀和李大钊都希望可以从思想上武装中华民族.</li></ol><p>所以,陈独秀和李大钊都要在&lt;&lt;新青年&gt;&gt;杂志上发表文章.</p></blockquote><p>在阅读完上面的内容之后,我的第一感受就是明明说了什么,但是仔细一想好像什么都没说,有些言之无物.其实,这就是缺少思想的后果.想像一下,如果一篇文章里充斥着这种内容,那对于读者来说真的是莫大的”考验”.那这部分内容要如何描述呢?</p><blockquote><p>陈独秀和李大钊的经历,思想以及目标的共同性,使得他们都在&lt;&lt;新青年&gt;&gt;杂志上发表文章.</p></blockquote><p>怎么样?用这句概括总结去代替之前的内容是不是感觉清爽了不少?</p><p>而避免”缺少思想”还有另外一个原因,那就是它不会让你错过梳理思想,整理逻辑和追本溯源的机会.举个例子:有个同事发现了公司存在的两个问题,于是他把这两个问题的内容,原因以及解决办法写了下来.但是在做最后总结的时候,他却发现这两个问题之间并没有什么联系.于是,他又去寻找这两个来源,发现这两个问题实际上是属于两个领域的.同时,公司里有4个部门都或多或少地涉及到了这种情况.于是,他便将原有的”<strong>公司遇到的两个问题</strong>“,改成了”<strong>公司某些部门遇到的两类问题</strong>“.实际上,这也是概括总结的另一个作用.</p><h3 id="如何总结概括"><a href="#如何总结概括" class="headerlink" title="如何总结概括"></a>如何总结概括</h3><p>与之前文章提到内容相似,总结概括的架构内容也要保证相斥性,独立性以及有穷性.但是表达逻辑却并不等于概括逻辑.通常来说,概括内容有两种表达方式:</p><ol><li>概述行动方式,这种是以步骤,流程,顺序等名词的方式出现,主要围绕着过程展开.</li><li>概述描述方式,这种是以内容,叙述,平铺直陈等名词出现,主要阐述某件事的存在意义.</li></ol><h3 id="分清行动的因果关系"><a href="#分清行动的因果关系" class="headerlink" title="分清行动的因果关系"></a>分清行动的因果关系</h3><p>在商务文章里，阐述行动类的方式是用的最多的。无论是工作流程，计划，解决方案，进度汇报等，都离不开行动内容。这时，我们不可避免地需要叙述每组行动性思想的原因，结果，建议，目标，完成度等等要素。平心而论，我觉得这是非常困难的一件事。它的困难不在于详细的描述各个要素，而在于分清每组思想之间的关系，接下来我来详细说明这一点。</p><p>我们往往是为了某个原因来去开展一系列行动的，而在各种行动的共同作用下，就会产生一个结果（也许并不是原定的结果）。这样一来，原因，行动，结果这三者也就构成了一个封闭的系统。但是，在这个系统里面，各组行动性思想除了起于共同原因，终于共同结果之外，本身是没有任何联系的。例如：</p><blockquote><p>为了提高公司业绩，我们需要开展两个行动：</p><ol><li>培训销售人员的素质，提高业务水平。</li><li>开展A/B测试，细化客户分类，提供画像。</li></ol></blockquote><p>可以看到，上述的1和2除了共同作用于结果之外，本身并没有任何关系。并且因结果才采取行动，但行动又共同作用于结果，这种行动与结果相互依赖的关系也让人很难理清思路。</p><p>不过别着急，这里有三个小技巧可以帮助我们理清其中的关系：</p><ol><li>在正式落笔之前，明确每一句话的中心思想</li><li>确定每句话，每组思想的因果关系，并尽量将每一组中的行动，步骤控制在5个以内。（神奇数字“7”）</li><li>检查每句话的标签或者概述，并将其依次填入金字塔模型中，搭建结构。</li></ol><h3 id="总结句要明确概括"><a href="#总结句要明确概括" class="headerlink" title="总结句要明确概括"></a>总结句要明确概括</h3><p>我一直都觉得文章在展示给读者之前，一定要自己跟自己“抬杠”一遍，尽可能多的给自己挑错，哪怕错误非常离谱都是可以接受的。虽然文章本身并不能决定以后的阅读对象是谁，但是如果作者自己都能挑出错误，那又凭什么要求读者不对文章提出异音呢？所以在尽可能丰富文章内容的前提下，文章精简，总结明确就可以保证作者与读者站在同一个位置上。最起码，双方不会因为用句不明确而产生分歧。</p><p>比如说“我们要在第三季度努力提高公司产出业绩。”，那这个提高是提高多少？要知道5%和50%都算是提高，但是具体目标却没有交代，这就会给读者造成困惑。如果我们把这句话改成“我们要在第三季度将公司产出业绩提高到25%。”，那么读起来就清晰多了，并且读者就会知道具体目标是多少。实际上，量化在招聘简历里面也是很常见的。在简历里面，我们往往会介绍自己的项目，或者说做的工作给公司带来了好处。那么这个好处是什么，给公司带来了多少收益，效率提高了多少等等，这些都是需要在简历里面给展示给HR看的。假如你是面试官，你的面前有两份简历，一份上面写着“我的工作节约了公司的运营成本”，另一份上面写着“我的工作将公司的运营成本降低了70%。”，那么你会选择谁，就不用我多说了吧。</p><p>当然，数字化并不是明确概括的唯一选择。还有另一种方法，就是<strong>大脑中想像一个真实的人去完成某件事情，并且完成过程中的细节越多越好，然后我们将其中的步骤转化为结果性语言，并阐述出来</strong>。</p><p>在上大学的时候，我策划过一个院级活动，主题是关于校园演唱的。那个时候博主没有任何处理经验，不知道要如何下手。后来距离提交活动策划的时间越来越近，没办法，我就只能走到举办场地，然后开始设想整个活动流程。从场景布置，同学入座，领导入座，开场等一直想到活动结束，想清楚每一个环节的开展需要的人是谁，需要什么道具，流程怎么走等等，甚至是包括领导从哪个方向入场，入座的姿势都想的一清二楚，就好像活动已经开展了，而我以一个上帝视角去观看一样。然后，我把中间所需要做的事情按照顺序概括起来，最终形成了一篇活动策划，提交给了学院。当然，最后活动自然是圆满成功了。</p><p>其实，<strong>唯一能够避免出现流程模糊的方法，就是强迫自己确定明确的结果或结点，以便能够据此判断某个步骤是否已经完成，然后根据这一阶段性的结果修改各项行动步骤的措辞。</strong></p><p>举个栗子：</p><table><thead><tr><th>原表达方式</th><th>修改后的表达方式</th></tr></thead><tbody><tr><td>1.修改公司战略</td><td>1.制定应对长期问题的战略</td></tr><tr><td>2.提高销售人员素质</td><td>2.培训销售人员的专业素养</td></tr><tr><td>3.处理回款问题</td><td>3.视情况不同，督促款项追回</td></tr></tbody></table><p>上面的原表达方式就是很典型的表达模糊，而经过修改变成右边的表达方式之后，读起来就清晰了许多。实际上，修改前后最大的不同，就是修改后的表达方式使读者产生了“画景”，就像是小说让人的头脑中产生了画面感一样。而这个“画景”不仅能给读者一种身临其境的感觉，还能够引导读者进行更深一步的思考。</p><p>前面说了那么多明确概括的方法和好处，其实总结句的明确还有一个好处，就是自我反省，查漏补缺。譬如说，为了节省公司的后台运营开支，我制定了两个步骤：</p><ol><li>检查并剔除不必要开支，如重复性支出，超出预算的支出，无效支出等。</li><li>制定相应的指标来杜绝“浪费”。</li></ol><p>但是，等我做完这些步骤的计划书之后，发现上面的步骤不仅可以用于后台运营，也可以用于其它的诸如前台销售，后台客服等部门。这个时候，我就需要将“节省公司的后台运营开支”的主题修改成“节省公司各部门开支”，并且在写作过程中发现除了之前提到的步骤外，其它部门还有一些问题需要解决，所以我又在上面的基础上又提出了4个解决问题的步骤。就这样修修改改后，一篇解决公司各部门开支过多的计划书就诞生了。而这些修改的来源就在于一开始的目的明确，即“节省公司各部门开支”，而不是“节省公司的后台运营开支”。</p><h3 id="辨别，分层总结句"><a href="#辨别，分层总结句" class="headerlink" title="辨别，分层总结句"></a>辨别，分层总结句</h3><p>之前提过，我们在概述行动性思想时，要把具有共同特性的行动分类到一起。但是在这个过程中，我们要注意的是，要把组内的行动进行分层，不然会让读者难以分清原因和结果。举个栗子：</p><blockquote><p>为了解决公司现金流紧张的问题，我们需要执行下面的措施：</p><ol><li>复查公司所有支出项目，提出无效支出。</li><li>减少公司运营成本。</li><li>对公司的产业体量作“减法”。</li><li>培训销售人员的专业素质，来提高销售质量。</li><li>增加现金收入。</li><li>拍卖利润低的项目，并抓紧追收回款。</li></ol></blockquote><p>例子中提到的解决方法都是比较合理的，甚至可以说，要想解决公司的现金问题，上面提到的措施一个都不能少。但是分层之后的内容就会让读者阅读起来更加的清晰：</p><blockquote><p>要想解决公司现金流紧张的问题，我们需要从两方面着手：</p><ol><li><p>减少成本：</p><ol><li>复查公司所有支出项目，提出无效支出。</li><li>对公司的产业体量作“减法”。</li></ol></li><li><p>增加现金收入：</p><ol><li>培训销售人员的专业素质，来提高销售质量。</li><li>拍卖利润低的项目，并抓紧追收回款。</li></ol></li></ol></blockquote><p>实际上分层本身并不是很困难的操作。如果一个行动开展的同时可以开展另一个行动，或者说两个行动遵守时间的先后顺序，那么这两个行动就属于同一个层次；如果一个行动是开展另一个行动的前提条件，或者说一个行动引出了另一个行动，那么这两个行动就是分属上下两个层次的。</p><p>不过物极必反，分层可以是文章看起来更加清晰，但要注意避免过度分类。譬如，有个咨询公司为了帮助客户提高销售人员的专业素养，提出了一些解决方法，并在文章中将其分类：</p><table><thead><tr><th>任务</th><th>目标</th><th>结果</th></tr></thead><tbody><tr><td>1.向销售人员培训必要的专业知识</td><td>1.调动销售人员的积极性</td><td>1.销售人员的专业素养收到了良好的维护</td></tr><tr><td>2.通过活动的形式调动销售人员的积极性</td><td>2.提高销售人员的重视程度</td><td>2.销售人员得到了培训</td></tr><tr><td>3.可以把专业素养的考核纳入绩效体系</td><td>3.传播销售知识</td><td>3.销售人员越来越重视自身的专业素养</td></tr><tr><td>4.制定周期长效计划来避免素养出现下降的情况</td><td>4.在提高专业素养的同时，也要防止下降</td><td>4.销售人员的学习态度愈加良好</td></tr></tbody></table><p>可以看出，整个文章内容分成了任务-目标-结果三个部分，看起来非常的清晰，详细。但是经过仔细观察后，就会发现任务1-目标3-结果2说的是一回事，其它内容亦是如此，而这就是过度分类的问题了。有的时候，太过于细致反而会造成内容重复，食之无味。</p><h3 id="直接概括行动的结果"><a href="#直接概括行动的结果" class="headerlink" title="直接概括行动的结果"></a>直接概括行动的结果</h3><p>其实直接概括结果是很难的一件事，但是本身却没有任何捷径可以走。所以这里就直接说一下如何概括：</p><ol><li>该组行动，步骤之间必须相互独立不重叠，完全穷尽无遗漏。</li><li>总结概括性语句必须说明各项行动，步骤导致的直接结果，且措辞必须明确，具体。</li><li>先用明确的语句表达各项行动，步骤，流程等，然后区分不同的抽象层次，再从各行动，步骤，流程等总结概括出行动的结果。</li></ol><h3 id="对结论进行总结"><a href="#对结论进行总结" class="headerlink" title="对结论进行总结"></a>对结论进行总结</h3><p>之前在“逻辑顺序”这一章中，我们提到过可以按照某种共性对问题进行分类。但是对于一篇完整的商务文章来说，总结部分共包含三步，而分类只是第一步。这三步分别是：</p><blockquote><ol><li>探索结论之间的共性，并进行分类</li><li>寻找结论之间更深层次的联系</li><li>总结跃进，对文章的主题进行高度概括</li></ol></blockquote><p>接下来，我依次介绍这三个步骤。</p><h4 id="找到结构上的共性"><a href="#找到结构上的共性" class="headerlink" title="找到结构上的共性"></a>找到结构上的共性</h4><p>在前面“演绎推理与归纳推理”的章节中，我们提到过，一句话是由主语和谓语组成的。因此，要想寻找某些思想的共性，往往需要从下面这三方面入手：</p><blockquote><ol><li>具有同一类主语</li><li>具有同一类谓语</li><li>具有同一类隐含思想</li></ol></blockquote><p>要注意的是，上面提到的是同一类，而不是同一个。譬如美国，英国，法国都属于国家，但是他们却并不相等。</p><p>相信大家在阅读文章的时候都碰到过“7个问题”，“4个特点”，“6个规律”这样的内容，譬如：</p><blockquote><p>接下来所制定的计划需要有四个特点：</p><ol><li>需要周期性地提供计划，供给分析师分析</li><li>计划要精准地反映出市场波动幅度</li><li>内容必须包含每一条措施的原因及预测结果</li><li>该计划要囊括公司所有部门</li></ol></blockquote><p>那么接下来，我们来剥掉一层“语言”的外衣：</p><blockquote><ol><li>周期性提供</li><li>精准反映</li><li>内容具体</li><li>囊括范围广</li></ol></blockquote><p>可以看出，这四个中心思想组合起来之后并不能支撑起文章主题。而之所以会出现这种情况，就是因为最上面那个“缺乏思想”的句子阻碍了我们对其进行深入思考。</p><p><strong>深入思考的目的首先就是要得出推论。</strong>但是上面的例子所列出的特点却无法让我们得到概括性结论，所以我们对这段内容进行修改：</p><blockquote><p>为了应对近期的市场波动，我们要制定相关计划：</p><ol><li>周期性的向分析师提供计划，分析市场波动</li><li>内容要覆盖公司所有部门的行动措施，原因及预测结果</li></ol></blockquote><p><strong>要记住，如果你无法在一组被列为“问题”，“原因”或结论的思想中找出明确的逻辑关系，就像上面这个例子一样，那么这一组思想必定会存在问题，你必须重新思考。</strong></p><h4 id="探索更深层次的联系"><a href="#探索更深层次的联系" class="headerlink" title="探索更深层次的联系"></a>探索更深层次的联系</h4><p>下面，我通过一个例子来说明如何寻找某些结论更深层次的联系。</p><blockquote><p>一个咨询顾问对客户公司的项目承接能力的判断：</p><ol><li>您要求信息系统项目经理提供有关截止日期的信息，以使战略性业务项目如期开展</li><li>有些项目经理缺乏经验</li><li>信息系统允许错过预订日期，而不是用创造性的方法满足预订日期的要求</li><li>现有系统的开发方法，工具和技术不统一</li><li>项目经理们没有组建过如此大型，复杂的系统</li><li>项目经理接受的脱产或在职项目管理培训较少，实践经验有限</li></ol></blockquote><p>接下来，我们找出每一句话最核心的内容：</p><blockquote><ol><li>需要了解截止日期</li><li>项目经理经验不足</li><li>有错过日期的风险</li><li>工具的使用不统一</li><li>未做过如此大型的项目</li><li>经验有限</li></ol></blockquote><p>实际上，无论是否了解这一主题，你都可以将上述思想组织得更清楚，更有意义。</p><blockquote><p>我们对贵公司的信息系统部门的评估显示，贵公司的项目经理们有可能无法按预定日期完成工作。（3）</p><p>他们在此类项目上的经验有限。（2，6）</p><p>他们从未组建过如此大型，复杂的系统。（5）</p><p>他们缺乏对所需方法，工具和技术的应用能力。（4）</p></blockquote><h4 id="进行归纳跃进"><a href="#进行归纳跃进" class="headerlink" title="进行归纳跃进"></a>进行归纳跃进</h4><p>以下是在一次说明会上，一名咨询顾问就是否应当进入汽车配件市场，提交给客户的说明材料中的要点。</p><blockquote><p>我们的结论：</p><ol><li>市场巨大，且正以较快的速度增长。</li><li>配件市场有利可图。</li><li>市场的主要特征显示，进入市场有较大的障碍。</li><li>总体趋势可喜，但由于一些不确定性，部分细分市场的前景尚不明确。</li><li>总体上看，该市场有吸引力，但是高度分散。</li></ol></blockquote><p>这些思想实际上可以分成两组：</p><blockquote><p>只有部分细分市场具有吸引力。（1，2，4，5）</p><p>这些细分市场难以进入。（3，4，5）</p></blockquote><p>很明显，“具有吸引力”和“难以进入”是对反义词，两者之间没有共性。因此，两个句子之间如果存在逻辑关系，就只能是演绎性关系，而不可能是归纳性关系：</p><p><img src="/2021/09/12/%E9%87%91%E5%AD%97%E5%A1%94-%E6%A6%82%E6%8B%AC%E5%90%84%E7%BB%84%E6%80%9D%E6%83%B3/image-20210826155218876.png" alt="image-20210826155218876"></p><p>因此什么？因此算了？因此花大本钱也要进入？所以这个例子再次说明，人们难以得出明确结论的时候，很容易写一句“缺乏思想”的句子应付了事（如本例），而不是努力完成思考。</p><p>需要注意的是，在通篇文章中，没有必要每次都非常严格地遵守这样的方式。并不是因为这样做没有用，而是因为读者每次都倾向于将所获信息纳入已有只是体系，进而理解和预测。如果你确定推理过程是有效的，那么即使概括性语句稍欠精确，也没有太大影响。</p><p>从本章的讨论中，你可以了解到，不能简单地把一系列思想堆放在一起，并假定读者能够看出其中的意义。每一组思想都隐含着一个总结性思想，能够呈现该组思想之间关系的本质。你应当首先明确各组思想之间的关系，然后为读者指明这种本质。</p><p>每当对一系列思想进行分组时，都要问自己一个问题为什么我只列出这些思想，而没有列出其它思想？答案应当是:</p><blockquote><ul><li><p>只有这些思想具备某种共性，并通过这种共性相互关联。</p><p>-在这种情况下，概括性思想应当是该组思想的共性所隐含的含义。</p></li><li><p>这些思想都是为实现某一结果而必须同时采取的行动。</p><p>-在这种情况下，概括性思想应当揭示采取以上行动后的直接结果。</p></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h3&gt;&lt;p&gt;这篇文章主要讲一下金字塔原理的第一原则:金字塔结构中的每一层思想都是下一层思想的精炼,概括,总结.而之所以这么说,便是因为每一层思想都是来源于下一层</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="思考的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第七章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B8%83%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="概括总结" scheme="https://yb705.github.io/tags/%E6%A6%82%E6%8B%AC%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——凝聚聚类</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/</id>
    <published>2021-08-15T04:35:04.000Z</published>
    <updated>2021-09-12T02:10:47.372Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p><strong>凝聚聚类</strong>指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止规则为止。scikit-learn中实现的停止规则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。还有一些<strong>链接准则</strong>，规定如何度量”最相似的簇“。这种度量总是定义在两个现有的簇之间。</p><p>scikit-learn中实现了以下三种选项：<br><strong>ward</strong>：默认选项。ward挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。<br><strong>average</strong>:average链接将簇中所有点之间的平均距离最小的两个簇合并。<br><strong>complete</strong>：complete链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。</p><p>ward适用于大多数数据集，在我们的例子中将使用它。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么average或complete可能效果更好。</p><p>接下来，我通过一个实际例子来说明这个算法。</p><h2 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h2><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/shub99/student-marks">Student Marks：https://www.kaggle.com/shub99/student-marks</a></p><p><img src="https://img-blog.csdnimg.cn/5f55d77402304711b55160282d76ebc1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这是一个简单的二维数据集，通过学生的学期成绩来判断这名学生最后可不可以升级（毕业）。该数据集包含100条数据记录，共有三个特征维度，分别是期中成绩（MID-SEM-MARKS），期末成绩（END-SEM-MARKS）以及升级结果（“0”:失败；“1”:成功)。</p><p><strong>2.数据处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\marks.txt&quot;</span><span class="hljs-comment">###https://www.kaggle.com/shub99/student-marks</span><br>marks=pd.read_csv(file_origin,sep=<span class="hljs-string">&#x27;\t&#x27;</span>,header=<span class="hljs-literal">None</span>)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p>需要注意的是，这份数据集的文件格式是txt，所以用<strong>pd.read_csv</strong>读取出来的数据是这个样子的：</p><p><img src="https://img-blog.csdnimg.cn/09c62b10516a4593b9393f690eb50c1f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>所以接下来还需要用方法<strong>split</strong>来依据特定字符“**,**”进行划分，示例代码如下所示：</p><p><img src="https://img-blog.csdnimg.cn/f9f9217012d246b7a4c72452489c1bb7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>同时，每条数据记录都是以字符串（string）的格式存储的，因此我们还需要用格式转换方法astype(“float”)来将原本的字符串格式（string）强制转换成浮点型（float），最终代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">marks[<span class="hljs-string">&quot;期中成绩&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">0</span>].astype(<span class="hljs-string">&quot;float&quot;</span>)<br>marks[<span class="hljs-string">&quot;期末成绩&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">1</span>].astype(<span class="hljs-string">&quot;float&quot;</span>)<br>marks[<span class="hljs-string">&quot;是否通过考试&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">2</span>]<br><span class="hljs-keyword">del</span> marks[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4f1e20f133084221b1c824d63e62b679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>3.使用凝聚聚类</strong></p><p>我们看一下凝聚聚类对这个数据集的效果如何。要注意，由于算法的工作原理，凝聚算法不能对新数据点作出预测。因此AgglomerativeClustering没有predict方法。为了构造模型并得到训练集上簇的成员关系，可以改用fit_predict方法，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering<br>agg=AgglomerativeClustering(n_clusters=<span class="hljs-number">2</span>)<br>X=marks.iloc[:,:-<span class="hljs-number">1</span>].values<span class="hljs-comment">####values方法很有用，专门用于将dataframe格式的某几列转化成相应维度的矩阵数组</span><br>assignment=agg.fit_predict(X)<br></code></pre></td></tr></table></figure><p>PS：推荐大家记一下values方法，这个方法可以把dataframe格式的数据完美转化成相应维度的矩阵数据（array），非常好用。</p><p>接下来，我们将聚类结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],assignment)<br>plt.xlabel(<span class="hljs-string">&quot;期中成绩&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;期末成绩&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/bb665ea4318847ea8f0e1d10a37d057e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到，算法完成了数据集的聚类。但是上面的图片并没有体现出分类的准确性，所以接下来我们通过对比源数据集中的特征分类来评估算法的精度：</p><p><img src="https://img-blog.csdnimg.cn/7d9c591ba8ec4d1bb16e508169935046.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这里主要是通过统计已知结果和聚类结果的差值，来评估算法精度。所以最后，我们可以知道算法的精度为83%。</p><p><strong>4.层次聚类与树状图</strong></p><p>凝聚聚类生成了所谓的<strong>层次聚类</strong>。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。有时候，同时查看所有可能的聚类是有帮助的。所以接下来，我们考虑用一种工具来将层次聚类可视化。</p><p>不幸的是，目前scikit-learn没有绘制这种图像的功能。但可以利用SciPy轻松生成树状图。SciPy的聚类算法接口与scikit-learn的聚类算法稍有不同。SciPy提供了一个函数，接受数据数组X并计算出一个<strong>链接数组</strong>，它对层次聚类的相似度进行编码，然后我们可以将这个链接数组提供给SciPy的dendrogram函数来绘制树状图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram,ward<br>linkage_array=ward(X)<span class="hljs-comment">###将ward聚类应用于数据数组X，返回一个数组，制定执行凝聚聚类是跨离的距离</span><br>dendrogram(linkage_array)<span class="hljs-comment">###现在为包含簇之间距离的linkage_array绘制树状图</span><br><br><span class="hljs-comment">###开始画图</span><br>ax=plt.gca()<br><br>bounds=ax.get_xbound()<br>ax.plot(bounds)<br>ax.plot(bounds)<br>plt.xlabel(<span class="hljs-string">&quot;样本&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;簇之间的距离&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3bbb8e547c164212bf36248c3692b767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>树状图在底部显示数据点。然后以这些点（表示单点簇)作为叶节点绘制一棵树，每合并两个簇就添加一个新的父节点。</p><p>从下往上看，每层的数据点两两合并，依次类推，直到在顶层生成两个分支。这对应于算法中两个最大的簇。</p><p>树状图的y轴不仅说明凝聚算法中两个簇何时合并，每个分支的长度还表示被合并的簇之间的距离。同时我们还可以看到，最大的三个簇（绿色，红色，兰色）在合并成两个簇的过程中跨越了相对较远的距离。</p><p>不幸的是，凝聚聚类依然无法分析图形复杂的数据集，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/bc96c7c74a2346c281cf69914a1ed8b8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>但是，DBSCAN可以解决这个问题。碍于篇幅限制，下一篇文章我再来说一说这个算法。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;凝聚聚类&lt;/strong&gt;指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="凝聚聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="聚类可视化" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="层次聚类" scheme="https://yb705.github.io/tags/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——K均值聚类（下）</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2021-08-15T04:33:30.000Z</published>
    <updated>2021-08-15T04:34:52.470Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比较。希望大家在阅读下面的内容之前，已经了解了K均值，PCA与NMF算法的基础知识。</p><p>如果不清楚的话，可以点击下面的链接，来简单阅读下：<br>K均值：<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">无监督学习——K均值聚类（上）</a><br>PCA：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>；<a href="https://blog.csdn.net/weixin_43580339/article/details/118223829">主成分分析（PCA）应用（下）</a><br>NMF：<a href="https://blog.csdn.net/weixin_43580339/article/details/118416483">非负矩阵分解（NMF）</a></p><h2 id="图像重建与矢量量化"><a href="#图像重建与矢量量化" class="headerlink" title="图像重建与矢量量化"></a>图像重建与矢量量化</h2><p>虽然k均值是一种聚类算法，但在k均值和分解方法（比如之前讨论的PCA和NMF）之间存在一些有趣的相似之处。大家可能还记得，PCA试图找到数据中方差最大的方向，而NMF试图找到累加的分量，这通常对应于数据的“极值”或“部分”。两种方法都试图将数据点表示为一些分量之和。与之相反，k均值则尝试利用簇中心来表示每个数据点。你可以将其看作仅用一个分量来表示每个数据点，该分量有簇中心给出。这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点被称为<strong>矢量量化</strong>。<strong>实际上，对于k均值来说，重建就是在训练集中找到的最近的簇中心。</strong></p><p>接下来，我们通过一个实际例子来比较三者之间的关系。</p><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><p><strong>2.数据处理</strong></p><p>我之前写了一篇文章来讲图像数据处理方面的知识，需要了解的朋友可以自行阅读这篇文章，这里就不再赘述了。（传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>）</p><p>处理代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>3.划分数据集并进行建模</strong></p><p>下面我们划分数据集，并利用NMF，PCA和K均值来依次进行建模，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>nmf.fit(X_train)<br>pca=PCA(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>pca.fit(X_train)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans.fit(X_train)<span class="hljs-comment">###注意这是训练数据</span><br></code></pre></td></tr></table></figure><p>然后，我们依次利用训练好的模型，依次生成重建数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_reconstructed_pca=pca.inverse_transform(pca.transform(X_test))<span class="hljs-comment">###注意这是测试数据模型</span><br>X_reconstructed_kmeans=kmeans.cluster_centers_[kmeans.predict(X_test)]<br>X_reconstructed_nmf=np.dot(nmf.transform(X_test),nmf.components_)<br></code></pre></td></tr></table></figure><p><strong>4.恢复图像</strong></p><p>为了方便接下来的图像展示，我们首先将上面的重建数据整合在一起，构成一个二维列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>=[X_test,X_reconstructed_nmf,X_reconstructed_kmeans,X_reconstructed_pca]<br>X_reconstructed=[[],[],[],[]]<br>shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>)):<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>[i]:<br>        vector=np.matrix(m)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>        arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br>        X_reconstructed[i].append(arr2)<br></code></pre></td></tr></table></figure><p>接下来，为了比较方便，我们只选取前5张图像，并利用多图表结构将其展现出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>)) <br>plt.suptitle(<span class="hljs-string">&#x27;K均值，PCA与NMF的图像还原比较&#x27;</span>, fontsize=<span class="hljs-number">80</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_reconstructed)):<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        axes[l,n].imshow(X_reconstructed[l][n],cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>axes[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;原始图片&quot;</span>,fontsize=<span class="hljs-number">50</span>) <br>axes[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;NMF&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;K均值&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">3</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;PCA&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>plt.show()<span class="hljs-comment">###由于之前已经划分了数据集，这是利用训练出来的模型对测试数据集进行的图像恢复，所以只有38个图像，而不是原来的150个图像</span><br></code></pre></td></tr></table></figure><p>最终结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/cc0d37c9ecc046ed86a453402a9a870e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图就是利用了100个分量（簇中心)的k均值，PCA和NMF的图像重建对比，其中k均值的每张图像中仅使用了一个簇中心。可以看出，相对来说还是k均值的表现要好一点。当然，PCA和NMF亦可以通过调整参数来提高精度，感兴趣的朋友可以自行探究。</p><p><strong>5.矢量量化</strong></p><p>利用K均值做矢量量化的一个有趣之处在于，可以用比输入维度更多的簇来对数据进行编码。让我们回到two_moons数据。利用PCA或NMF，我们对这个数据无能为力，因为它只有两个维度。如果使用PCA和NMF将其降维到一维，将会完全破坏数据结构。但通过使用更多的簇中心，我们可以用K均值找到一种更具表现力的表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">10</span>,random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=<span class="hljs-string">&quot;Paired&quot;</span>,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],s=<span class="hljs-number">60</span>,marker=<span class="hljs-string">&quot;*&quot;</span>,c=<span class="hljs-built_in">range</span>(kmeans.n_clusters),linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/90543c394f3c49ad9401fd807e8228e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>我们使用了10个簇中心，也就是说，现在每10个点都被分配了0到9之间的一个数字。我们可以将其看作10个分量表示的数据（有10个新特征)，只有表示该点对应的簇中心的那个特征不为0，其它特征均为0。利用这个10维表示，我们就可以用线性模型来划分两个半月形，而利用原始的两个特征是不可能做到这一点的。</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>K均值是非常流行的聚类算法，因为它不仅相对容易理解和实现，而且运行速度也相对较快。同时K均值可以轻松扩展到大型数据集。</p><p>而K均值的缺点之一在于，它依赖随机初始化，也就是说，算法的输出依赖于随机种子。默认情况下scikit-learn用10种不同的随机初始化将算法运行10次，并返回最佳结果。当然，K均值还有一个缺点，就是簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（在现实世界中可能并不知道这个数字）。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。序</p><p>之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比较。希望大家在阅读下面的内容之前，已经了解了K均值，PCA与NMF算法的基础知识。</p><p>如果不清楚的话，可以点击下面的链接，来简单阅读下：<br>K均值：<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">无监督学习——K均值聚类（上）</a><br>PCA：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>；<a href="https://blog.csdn.net/weixin_43580339/article/details/118223829">主成分分析（PCA）应用（下）</a><br>NMF：<a href="https://blog.csdn.net/weixin_43580339/article/details/118416483">非负矩阵分解（NMF）</a></p><h2 id="图像重建与矢量量化-1"><a href="#图像重建与矢量量化-1" class="headerlink" title="图像重建与矢量量化"></a>图像重建与矢量量化</h2><p>虽然k均值是一种聚类算法，但在k均值和分解方法（比如之前讨论的PCA和NMF）之间存在一些有趣的相似之处。大家可能还记得，PCA试图找到数据中方差最大的方向，而NMF试图找到累加的分量，这通常对应于数据的“极值”或“部分”。两种方法都试图将数据点表示为一些分量之和。与之相反，k均值则尝试利用簇中心来表示每个数据点。你可以将其看作仅用一个分量来表示每个数据点，该分量有簇中心给出。这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点被称为<strong>矢量量化</strong>。<strong>实际上，对于k均值来说，重建就是在训练集中找到的最近的簇中心。</strong></p><p>接下来，我们通过一个实际例子来比较三者之间的关系。</p><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><p><strong>2.数据处理</strong></p><p>我之前写了一篇文章来讲图像数据处理方面的知识，需要了解的朋友可以自行阅读这篇文章，这里就不再赘述了。（传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>）</p><p>处理代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>3.划分数据集并进行建模</strong></p><p>下面我们划分数据集，并利用NMF，PCA和K均值来依次进行建模，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>nmf.fit(X_train)<br>pca=PCA(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>pca.fit(X_train)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans.fit(X_train)<span class="hljs-comment">###注意这是训练数据</span><br></code></pre></td></tr></table></figure><p>然后，我们依次利用训练好的模型，依次生成重建数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_reconstructed_pca=pca.inverse_transform(pca.transform(X_test))<span class="hljs-comment">###注意这是测试数据模型</span><br>X_reconstructed_kmeans=kmeans.cluster_centers_[kmeans.predict(X_test)]<br>X_reconstructed_nmf=np.dot(nmf.transform(X_test),nmf.components_)<br></code></pre></td></tr></table></figure><p><strong>4.恢复图像</strong></p><p>为了方便接下来的图像展示，我们首先将上面的重建数据整合在一起，构成一个二维列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>=[X_test,X_reconstructed_nmf,X_reconstructed_kmeans,X_reconstructed_pca]<br>X_reconstructed=[[],[],[],[]]<br>shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>)):<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>[i]:<br>        vector=np.matrix(m)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>        arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br>        X_reconstructed[i].append(arr2)<br></code></pre></td></tr></table></figure><p>接下来，为了比较方便，我们只选取前5张图像，并利用多图表结构将其展现出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>)) <br>plt.suptitle(<span class="hljs-string">&#x27;K均值，PCA与NMF的图像还原比较&#x27;</span>, fontsize=<span class="hljs-number">80</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_reconstructed)):<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        axes[l,n].imshow(X_reconstructed[l][n],cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>axes[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;原始图片&quot;</span>,fontsize=<span class="hljs-number">50</span>) <br>axes[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;NMF&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;K均值&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">3</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;PCA&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>plt.show()<span class="hljs-comment">###由于之前已经划分了数据集，这是利用训练出来的模型对测试数据集进行的图像恢复，所以只有38个图像，而不是原来的150个图像</span><br></code></pre></td></tr></table></figure><p>最终结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/cc0d37c9ecc046ed86a453402a9a870e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图就是利用了100个分量（簇中心)的k均值，PCA和NMF的图像重建对比，其中k均值的每张图像中仅使用了一个簇中心。可以看出，相对来说还是k均值的表现要好一点。当然，PCA和NMF亦可以通过调整参数来提高精度，感兴趣的朋友可以自行探究。</p><p><strong>5.矢量量化</strong></p><p>利用K均值做矢量量化的一个有趣之处在于，可以用比输入维度更多的簇来对数据进行编码。让我们回到two_moons数据。利用PCA或NMF，我们对这个数据无能为力，因为它只有两个维度。如果使用PCA和NMF将其降维到一维，将会完全破坏数据结构。但通过使用更多的簇中心，我们可以用K均值找到一种更具表现力的表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">10</span>,random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=<span class="hljs-string">&quot;Paired&quot;</span>,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],s=<span class="hljs-number">60</span>,marker=<span class="hljs-string">&quot;*&quot;</span>,c=<span class="hljs-built_in">range</span>(kmeans.n_clusters),linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/90543c394f3c49ad9401fd807e8228e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>我们使用了10个簇中心，也就是说，现在每10个点都被分配了0到9之间的一个数字。我们可以将其看作10个分量表示的数据（有10个新特征)，只有表示该点对应的簇中心的那个特征不为0，其它特征均为0。利用这个10维表示，我们就可以用线性模型来划分两个半月形，而利用原始的两个特征是不可能做到这一点的。</strong></p><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>K均值是非常流行的聚类算法，因为它不仅相对容易理解和实现，而且运行速度也相对较快。同时K均值可以轻松扩展到大型数据集。</p><p>而K均值的缺点之一在于，它依赖随机初始化，也就是说，算法的输出依赖于随机种子。默认情况下scikit-learn用10种不同的随机初始化将算法运行10次，并返回最佳结果。当然，K均值还有一个缺点，就是簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（在现实世界中可能并不知道这个数字）。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="K均值聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="人脸识别" scheme="https://yb705.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
    <category term="图像聚类" scheme="https://yb705.github.io/tags/%E5%9B%BE%E5%83%8F%E8%81%9A%E7%B1%BB/"/>
    
    <category term="PCA与NMF" scheme="https://yb705.github.io/tags/PCA%E4%B8%8ENMF/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——K均值聚类（上）</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2021-08-15T04:31:18.000Z</published>
    <updated>2021-08-15T04:33:20.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>近几年在机器学习领域里面，<strong>聚类</strong>是比较热门的一个词汇。它是将数据集划分成组的任务，这些组叫做<strong>簇</strong>。其目标是划分数据，使得一个簇内的数据点非常相似且簇内的数据点非常不同。与分类算法相似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。但是，与分类算法不同的是，<strong>聚类</strong>属于无监督学习，也就是说事先并不知道数据集的标签或者说特征值分类，而分类算法是监督学习，意味着已经提前知道了数据点的所属类别。接下来，我重点介绍下聚类里面比较常用的算法——<strong>k均值聚类</strong>。</p><h2 id="K均值聚类的原理讲解"><a href="#K均值聚类的原理讲解" class="headerlink" title="K均值聚类的原理讲解"></a>K均值聚类的原理讲解</h2><p><strong>1.算法介绍</strong></p><p>k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的<strong>簇中心</strong>。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束。</p><p><strong>2.代码讲解</strong></p><p>用scikit-learn应用K均值相当简单。下面，我们将其应用于模拟数据make_blobs，将Kmeans实例化，并设置我们要找的簇的个数。然后对数据调用fit方法。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br>X,y=make_blobs(random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###生成模拟的二维数据</span><br>kmeans=KMeans(n_clusters=<span class="hljs-number">3</span>)<span class="hljs-comment">###如果不指定n_clusters，它的默认值是8</span><br>kmeans.fit(X)<br></code></pre></td></tr></table></figure><p>PS：其中make_blobs的函数功能是生成各向同性的高斯斑点以进行聚类，感兴趣的同学可以自行百度研究下，这里就不在进行赘述了。</p><p><strong>(1) kmeans.labels_</strong></p><p>在算法运行期间，Kmeans为X中的每个训练数据点分配一个簇标签。我们可以在kmeans.labels_中找到这些标签，实际上<strong>kmeans.labels_就是算法结果（与kmeans.predict(X)的意义一样）</strong>：</p><p><img src="https://img-blog.csdnimg.cn/cdbf5685acc84df89d440048f1dcc040.png" alt="在这里插入图片描述"></p><p>如上图所示，聚类算法与分类算法有些相似，每个元素都有一个分配的标签。我们与数据集的原分类对比一下：</p><p><img src="https://img-blog.csdnimg.cn/10337246a5744094ac334c1ba5469dbc.png" alt="在这里插入图片描述"></p><p>两相对比可以看出，标签并不是真实的，因此标签本身并没有什么意义。算法给予你的唯一信息就是所有标签相同的数据点都是相似的。所以，<strong>我们不应该为其中一组的标签是0，另一组的标签是1赋予任何意义。</strong></p><p><strong>(2) kmeans.cluster_centers_</strong></p><p>之前提到的簇中心数据被保存在kmeans.cluster_centers_属性中：</p><p><img src="https://img-blog.csdnimg.cn/42edcd9c93314bb9a07738515cfd4596.png" alt="在这里插入图片描述"></p><p><strong>(3) 可视化</strong></p><p>接下来我们将数据分类结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,markers=<span class="hljs-string">&quot;o&quot;</span>)<br>mglearn.discrete_scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],markers=<span class="hljs-string">&quot;*&quot;</span>,markeredgewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/a59c850558184abca5faa7c85a0d8f3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>从上面的图像可以看出，数据被3个簇中心很好的分成了三个部分。</p><p>实际上，我们也可以使用更多或更少的簇中心：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>fig,axes=plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,ax=axes[<span class="hljs-number">0</span>])<br>kmeans=KMeans(n_clusters=<span class="hljs-number">5</span>)<span class="hljs-comment">###使用五个簇</span><br>kmeans.fit(X)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3dbf8441317240cda24a78382fa0b28b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="K均值聚类的实际应用"><a href="#K均值聚类的实际应用" class="headerlink" title="K均值聚类的实际应用"></a>K均值聚类的实际应用</h2><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/adityakadiwal/water-potability">水质测试数据：https://www.kaggle.com/adityakadiwal/water-potability</a></p><p><img src="https://img-blog.csdnimg.cn/0c19bb2d6a5246cd899e4f5bbe674429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>该数据集共包含3277个数据点，并通过水质硬度，酸碱度等9个特征值，将水分成了两类。（0和1指代水的类别）</p><p>接下来，为了演示无监督学习，我们将其当作不知分类结果的数据集，并用K均值算法进行聚类。</p><p><strong>2.数据处理</strong></p><p>老规矩，还是先读取数据，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\water_potability.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/adityakadiwal/water-potability</span><br>water=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/e4694529b78047c0a672a6e92261d360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到这份数据中有很多空值，为了保证数据的完整性，我们将包含空值的行全部删掉（也可以用平均数来代替)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">water=water.dropna()<span class="hljs-comment">####去掉包含空值的行</span><br>water_test=water.drop(<span class="hljs-string">&quot;Potability&quot;</span>,axis=<span class="hljs-number">1</span>)<span class="hljs-comment">#####去掉分类结果</span><br>y=water[<span class="hljs-string">&quot;Potability&quot;</span>]<br></code></pre></td></tr></table></figure><p><strong>3.应用K均值聚类并与原结果进行比较</strong></p><p>我们对上面已经处理好的数据集应用K均值聚类算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###对应源数据中的两个分类结果</span><br>kmeans.fit(water_test)<br>y_pred=kmeans.predict(water_test)<br></code></pre></td></tr></table></figure><p>处理好之后，我们将聚类结果与源数据的分类进行差值计算。如果分类结果相同，则差值等于0，因此，通过统计差值中有多少个数值为0，来对聚类结果进行测评：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>y_original=np.array(y)<br>a=y_pred-y_original<span class="hljs-comment">###将结果进行比较，计算两者之间的差值</span><br><span class="hljs-built_in">len</span>(a[a==<span class="hljs-number">0</span>])/<span class="hljs-built_in">len</span>(a)<span class="hljs-comment">###如果分类结果相同，则等于0，所以统计差值中有多少个数值为0</span><br></code></pre></td></tr></table></figure><p>结果如下所示，无监督学习的K均值聚类精度为52.8%</p><p><img src="https://img-blog.csdnimg.cn/244906cb82864d078380d4005b6331b3.png" alt="在这里插入图片描述"></p><h2 id="K均值的精度"><a href="#K均值的精度" class="headerlink" title="K均值的精度"></a>K均值的精度</h2><p>我们从上面的结果可以看出来，实际应用中K均值的精度并没有很高。一是因为与监督学习中的分类器相比，K均值聚类属于无监督学习，也就是说事先并不知道分类结果，没有办法进行模型训练。二是因为即使你知道给定数据中簇的“正确”个数，k均值可能也不是总能找到它们。每个簇仅由其中心定义，这意味着每个簇都是凸形。因此，K均值只能找到相对简单的形状。K均值还假设所有簇在某种程度上具有相同的“直径”，它总是将簇之间的边界刚好画在簇中心的中间位置。</p><p>举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=mglearn.cm2,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],marker=<span class="hljs-string">&quot;*&quot;</span>,c=[mglearn.cm2(<span class="hljs-number">0</span>),mglearn.cm2(<span class="hljs-number">1</span>)],s=<span class="hljs-number">100</span>,linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/414b500b5abd402d9c4e41988d468704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>对于上面的数据集，很明显，我们是希望聚类算法能够发现两个半月形，但是在这里利用K均值算法是不可能做到这一点的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇主要讲了关于K均值聚类的一些基础的东西，包括代码原理，实际应用以及应用中的一些局限性。限于篇幅原因，下一篇我再说一些有关K均值聚类稍微复杂一点的内容。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;近几年在机器学习领域里面，&lt;strong&gt;聚类&lt;/strong&gt;是比较热门的一个词汇。它是将数据集划分成组的任务，这些组叫做&lt;strong&gt;簇&lt;/s</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="K均值聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="聚类" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="簇" scheme="https://yb705.github.io/tags/%E7%B0%87/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(六)-应用逻辑顺序</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/</id>
    <published>2021-08-15T04:21:55.000Z</published>
    <updated>2021-09-12T02:00:47.850Z</updated>
    
    <content type="html"><![CDATA[<h2 id="应用逻辑顺序"><a href="#应用逻辑顺序" class="headerlink" title="应用逻辑顺序"></a>应用逻辑顺序</h2><p>当我们在写文章的时候，往往会把存在某种联系的思想放在同一组，进行讲解。之所以要把某些思想放在一起，便是因为它们之间存在着某种逻辑关系，既方便作者进行分析，也有助于读者的理解。而这种逻辑关系有三种，分别是因果关系，结构关系，重要性关系。接下来，我们就简单介绍下这三种关系。</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210806161601588.png" alt="image-20210806161601588"></p><h4 id="1-时间（因果）顺序"><a href="#1-时间（因果）顺序" class="headerlink" title="1.时间（因果）顺序"></a>1.时间（因果）顺序</h4><p>因果关系，顾名思义就是某些原因导致了某些结果。举个例子：</p><blockquote><p>受到市场波动，政策调控，疫情叠加的影响，7月份二三线城市中有七成左右的城市的土地成交面积环比下降超过10%。那么我们来依次分析这三个原因都带来了哪些影响。。。。。。</p></blockquote><p>上面的内容就是一个典型的因果关系的思想分组，即多个原因共同导致了一个结果。当然，不只有“先因后果”，还有“先果后因”。实际上，在生活或者工作中，我们往往会先设立一个目标，或者说是要取得某个结果，然后作出计划或者是列出步骤，来依次执行。举个简单的例子：</p><blockquote><p>公司想要在今年第二季度达到100w的业绩。那么为了达成这个目标，我们第一步要先做。。。；第二步再去做。。。；最后再完成。。。</p></blockquote><p>就像上面的例子中所说的，为了完成某个结果，而产生的过程或者系统，这段内容所依据的逻辑顺序就是时间顺序（如第一天，第二天；第一步，第二步等等）。</p><h4 id="2-空间（结构）顺序"><a href="#2-空间（结构）顺序" class="headerlink" title="2.空间（结构）顺序"></a>2.空间（结构）顺序</h4><p>结构逻辑往往是针对于某个可以分割的整体对象。譬如：</p><blockquote><p>一个销售公司往往包括后台职能部门，销售部门，以及客服部门。这些部门往往各行其职，其中后台职能部主要负责运营，人事，财务等，销售部门负责产品的具体销售，而客服则负责售后服务。</p></blockquote><p>当然，这个对象可以是具体的，比如电脑及其各个组成零件，非洲大陆及其组成国家等；也可以是概念的，比如消费者市场的构成，客户交易过程的组成部分（可监控分析异常值，优化流程，提高效率，降低成本）等。</p><h4 id="3-程度（聚类）顺序"><a href="#3-程度（聚类）顺序" class="headerlink" title="3.程度（聚类）顺序"></a>3.程度（聚类）顺序</h4><p>通常来说，我们往往会把有某种共性的思想分到一起，譬如说它们是由于成本降低所导致的三个后果，或者这两个优化举措都是董事会提出的。类似于这种有某些差异性，但是却又因为它们之间的共同性分到了一起的逻辑顺序，就是聚类结构。当然，我们也可以依照重要性的不同程度来划分组别。譬如说最重要的分到一类，次重要的分到一类，不重要的放到一类。</p><p>在日常写作的过程中，我们能用到的逻辑顺序可以是上面提到的单独的一种，也可以是三种混合使用，但无论如何，必须要包含至少一种。如果没有的话，建议回过头来检查一下文章的结构，看看有没有问题。</p><p>接下来详细说一下这三个常用的逻辑顺序。</p><h3 id="因果顺序"><a href="#因果顺序" class="headerlink" title="因果顺序"></a>因果顺序</h3><p>因果顺序是这三种顺序里面最简单，最容易理解的。因为它非常符合人类的正常思考逻辑，就像是数数一样。如果要求你从1数到5，那么你会不假思索地数出1，2，3，4，5。那么对比数数，我们在工作过程中也会经常提到诸如第一步，第二步，第三步；第一天，第二天，第三天；首先，然后，最后等词语。所以，这也就是为什么我私下里更喜欢把因果顺序称为自然顺序。不过，虽然因果顺序比较简单，但是在使用的过程中也要有几点需要注意，接下来，我来一一讲解。</p><h4 id="分清因果"><a href="#分清因果" class="headerlink" title="分清因果"></a>分清因果</h4><p>我前几天读了《精益数据分析》这本书，发现书里所提到的双边市场的概念用于因果顺序非常合适。那么套用在双边市场里，因果关系就被拆分成两个对象，即原因—结果。在具体的应用过程中，我们只需要对这两者负责，不用去考虑第三方，并且这两者又是相互作用的。所以将这两者分清，继而去单独分析就显得非常重要。举个例子：</p><blockquote><ol><li>“三条红线”限制了房企的现金流，导致公司的资金紧张。</li><li>为了参与第一季度的土地拍买，许多房企不得不抽调全国的布局资金来去抓住重点城市。</li><li>部分城市提高了申请银行房贷的门槛。</li><li>部分城市公告了要限制售价的重点地段。</li><li>大部分城市的商用住宅成交面积的环比，同比均有不同程度的下降。</li><li>很多城市的房价有所下降。</li><li>房产销售市场迎来了久违的降温。</li></ol></blockquote><p>上面描述的内容就给人一种似是而非的感觉，没有办法分清谁是原因，谁是结果。不着急，我们慢慢解决。我们依次查看每句话里面所包含的原因与结果：</p><blockquote><ol><li>“三条红线”和第一季度的土地拍卖使得房企公司的资金紧张。</li><li>房贷申请变得困难，使得成交量和成交面积较之前有所下降。</li><li>部分地段的房屋售价受到限制，使得最后的成交房价有所下降。</li><li>多种政策调控使得房产市场降温。</li></ol></blockquote><p>其实到这里，上面的内容已经整理的差不多了。整个梳理过程中，主要的困难就是原因和结果的区分，或者说是什么原因导致了什么结果，甚至还涉及到了子原因和子结果。譬如前三句话中，每一句话都涉及到了子结果（房企的资金紧张，成交面积下降，房价下降），而这三个子结果又共同导致了第四句话（市场降温）。</p><p>PS：当我们拿到一段内容，难以梳理其中关系的时候，可以尝试列出表格，或者构建一个金字塔模型（不要忘了我们学的是什么）。然后将每句话的原因和结果填写到表格或模型当中，进行梳理。这种有依据的梳理会使逻辑非常清晰，并且减少遗漏。</p><h4 id="揭示隐含逻辑"><a href="#揭示隐含逻辑" class="headerlink" title="揭示隐含逻辑"></a>揭示隐含逻辑</h4><p>我们在写文章时并不是追求越详细越好，毕竟有的时候，事无巨细也就意味着啰哩啰唆。适当的隐含往往能引起思考，抓住读者的注意力。当然，这其中的度要把握好，过于隐含可能会给读者带来疑惑，或者不符合读者的预期，而过于详细则会使文章冗余烦杂。举个栗子：</p><blockquote><p>在非洲，矿产的过度开采，给环境造成了非常严重的影响。</p><p>制度的改革给公司节省了大量的人工成本。</p></blockquote><p>像是上面的两句话就比较恰到好处。譬如矿产的过度开采给环境造成了影响，但是却不必罗列出来具体造成了什么样的影响，为什么会造成这样的影响。如果真的把每一条都详细地说明一遍，那对读者可真是莫大的考验啊。</p><h4 id="检查思想分组"><a href="#检查思想分组" class="headerlink" title="检查思想分组"></a>检查思想分组</h4><p>无论是写作，还是生活，工作当中的其它活动，检查都是一个非常重要的行为，尤其是从事数据方面的工作，一个小小的数字出现问题，就会造成一系列的分析错误，引起连锁反应。（PS：别问博主为什么很有经验）</p><p>在写作过程中，如果我们想要知道逻辑顺序有没有问题，主要方法就是顺着逻辑结构去检查。举个栗子：</p><blockquote><ol><li>市场调查</li><li>制定计划</li><li>实施计划</li><li>公司进入快速增长期，要加快投入。</li><li>公司进入缓慢增长期，将优化投入。</li><li>公司进入衰退期，要发现问题，及时修正。</li><li>公司进入亏损期，减少投入，必要时可放弃投入。</li></ol></blockquote><p>不知道大家有没有发现，上面这段分组内容的问题就是将原因与结果写到了一起，其中前三句的结果就是4-7句。那么该如何修改呢？</p><p>其实后四句总结一下就是公司对不同阶段的适应，最后就可以改成下面这样：</p><blockquote><ol><li>市场调查</li><li>制定计划</li><li>实施计划</li><li>公司要采取不同的措施来应对不同的阶段。</li></ol></blockquote><h3 id="结构顺序"><a href="#结构顺序" class="headerlink" title="结构顺序"></a>结构顺序</h3><p>什么是结构顺序？结构顺序就是使用示意图或者地图，查看交易流程，分析组织架构，阅读说明书等行为的顺序。它既可以针对具体的事物，如某种电器及其组成零件，也可以针对抽象的概念，如市场的构成。因此，结构顺序并没有特定的使用范围。下面，我就来讲解有关于结构顺序的内容。</p><h4 id="构建结构顺序"><a href="#构建结构顺序" class="headerlink" title="构建结构顺序"></a>构建结构顺序</h4><p>在搭建结构顺序的过程中，要注意“一个整体”和“两个原则”。“一个整体”指的是要针对一个可以分割成部分的整体；而“两个原则”针对的是各个部分，具体来讲就是：</p><ol><li>被分组的各个部分不仅要有各自的<strong>独立性</strong>，也要有相互之间的<strong>排斥性</strong>。</li><li>组成整体的各个部分要具有<strong>有穷性</strong>，不能有遗漏。</li></ol><p>举个简单地例子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812111642107.png" alt="image-20210812111642107"></p><p>某个轮胎公司<strong>只</strong>包含三个部门（职能包括财务，人事等），且这三个部门都是独立存在的，相互之间不存在重叠的情况。</p><h4 id="描述结构顺序"><a href="#描述结构顺序" class="headerlink" title="描述结构顺序"></a>描述结构顺序</h4><p>有的朋友可能有疑问：对于结构逻辑，在描述具体内容的时候应该按照什么顺序去描写？很简单，在我们搭建金字塔的时候，只要按照填写架构内容的顺序去描述就可以了。拿上面的例子来说，我们就可以按照销售，研发，职能的顺序来去具体地讨论各个部分。</p><p>实际上，在描述具体内容的时候，我们也可以搭配时间顺序，空间顺序，重要性顺序来去描述。比如，我现在想要讨论全球气候的变化问题，那么就可以按照空间顺序，从北到南，依次讨论北极，俄罗斯，亚洲，南极等各个地位的气候变化。</p><h4 id="修改结构顺序"><a href="#修改结构顺序" class="headerlink" title="修改结构顺序"></a>修改结构顺序</h4><p>在工作当中，改善往往比创新更常见，毕竟创新要更困难一些。所以有的时候我们并不需要从无到有地搭建一个结构，而只需要对原有的结构进行整理修改。接下来我就通过一个例子来去说明下如何修改原有结构：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812132110446.png" alt="image-20210812132110446"></p><p>上面是某个综合投资的集团公司领导组织，看起来比较混乱，并且有职能重叠的部分。现在老板要求你对公司的高层架构做一些调整，来提高工作效率，节省人力成本。这次先说修改后的结果：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812133217111.png" alt="image-20210812133217111"></p><p>其实，调整结构顺序主要还是依照前面的“一个整体”和“两个原则”去做的。总的来说就是先将各个思想总结成一个名词，或者是打上标签，然后依照标签区分大分类与小分类的包含关系，最后确定要调整的主体，并围绕着两个原则确定各个部分的分组。</p><h4 id="检查结构顺序"><a href="#检查结构顺序" class="headerlink" title="检查结构顺序"></a>检查结构顺序</h4><p>与前面提到的因果顺序相同，在完成一篇文章之后，我们也要去顺着文章的顺序结构去检查。</p><p>最后再强调一下，我们在写文章的时候一定要先想好文章的主题，并列出结构，然后顺着文章结构去写，来避免遗漏。毕竟如果不按照提前构思好的结构去写，那么就算有遗漏，我们也很难有所察觉。</p><h3 id="重要性顺序"><a href="#重要性顺序" class="headerlink" title="重要性顺序"></a>重要性顺序</h3><p>最后，我们来说一下重要性顺序。重要性顺序指的是对一类具有共同特质的事物进行陈述，分析的顺序。接下来介绍一下它的相关内容。</p><h4 id="构建分组"><a href="#构建分组" class="headerlink" title="构建分组"></a>构建分组</h4><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210813125240155.png" alt="image-20210813125240155"></p><p>构建分组就是根据问题的共同特征来进行分类。如上图所示，我之所以要把5个问题分到一组，便是因为这5个问题可能都来自于财务部门，或者它们都是由市场震荡引起的。需要注意的是每组的问题数量并没有要求，也就是说只要符合逻辑，不管每组所包含的问题数量的差距有多么大，都是可以接受的。</p><p>完成了分组之后，接下来我们就要考虑组内事物的排列顺序了。一般情况下，我们是按照每个事物所包含的共同特质的高低来进行排序的。换句话说，假如我们把每个问题所包含的共同特质的高低，或者说每个问题的重要性，定义为1，2，3，4，5。其中，1为最低，5为最高。那么我们就可以按照54321，从高到低，由重要到不重要的顺序，去排列组内的问题。当然，按照12345的顺序去排列也是可以的，因为“从弱到强”往往更能带动读者的情绪。实际上，无论是12345还是54321，都只是作者的写作习惯的不同而已。只不过，有的时候，我们的写作目的是阐述分析，而不是为了触动读者感情。</p><h4 id="检查，辨别分组"><a href="#检查，辨别分组" class="headerlink" title="检查，辨别分组"></a>检查，辨别分组</h4><p>现实生活中，大部分事情都不是一蹴而就的。因此，对于一个初创或者假定的分组，我们需要反复地检查辨别其中的分组逻辑。这样做可以使我们避免错漏，并且更好地理解分组基础。接下来，我通过一个例子来说明如何进行分组。</p><blockquote><ol><li>库存数据的时间戳的格式有些错乱。</li><li>提交库存数据的周期并不符合分析师的要求。</li><li>库存数据的分析结果总是落后于市场的变化。</li><li>分析师们很难从前后矛盾的库存数据中得到有用的信息。</li><li>库存数据的特征维度错乱，无法对应。</li><li>表格中每条数据的标注字体各不相同。</li></ol></blockquote><p>不知道大家在阅读上面内容的时候是什么感受，反正我是没有摸清作者想要表达的东西。那么如何将这些内容分辨清楚呢？</p><p>首先，我们根据每句话所表达的意思来定义一些名词，或者是分类的类别，譬如说：库存数据格式有问题，提交时间不合适等。</p><p>然后，我们依照这些类别来对具体内容进行划分，如下所示：</p><table><thead><tr><th>内容</th><th>类别</th></tr></thead><tbody><tr><td>1.库存数据的时间戳的格式有些错乱；6.表格中每条数据的标注字体各不相同。</td><td>库存数据的格式有问题</td></tr><tr><td>2.提交库存数据的周期并不符合分析师的要求；3.库存数据的分析结果总是落后于市场的变化</td><td>库存数据的时间周期需要调整</td></tr><tr><td>4.分析师们很难从前后矛盾的库存数据中得到有用的信息；5.库存数据的特征维度错乱，无法对应。</td><td>库存数据的内容混乱</td></tr></tbody></table><p>最后，我们再根据文章的写作目的，架构以及题材，来确定组与组之间要遵循什么顺序去排列。譬如说，如果我想阐述服务器故障对库存数据造成了影响，那么文章中应该重点描述数据的格式与内容混乱的问题；如果我想说分析师们从数据中得出错误结论，那么文章前半部分就应该先说明库存数据的时间周期需要调整。</p><p>把上面的步骤总结一下就是;</p><ol><li>找出相关内容的分类类别。</li><li>将具体内容进行分类。</li><li>确定各组别的排列顺序。</li></ol><p>在本篇文章的最后，我想再强调一点：检查逻辑顺序是判断分组是否恰当的重要手段。当你遇到一组归纳性思想，需要找出其真实意义时，一定要先快速浏览一下该组中的所有思想。从中能否发现某种逻辑顺序（时间顺序，结构顺序，重要性顺序）？如果不能，那能否发现这样分组的基础（过程或流程，结构，类别），并采用某种逻辑顺序进行梳理？如果某一组罗列的思想过多，你能否发现它们的共同特性，并根据这些共同特性将思想进行细分，归纳，然后用一种逻辑顺序组织起来？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;应用逻辑顺序&quot;&gt;&lt;a href=&quot;#应用逻辑顺序&quot; class=&quot;headerlink&quot; title=&quot;应用逻辑顺序&quot;&gt;&lt;/a&gt;应用逻辑顺序&lt;/h2&gt;&lt;p&gt;当我们在写文章的时候，往往会把存在某种联系的思想放在同一组，进行讲解。之所以要把某些思想放在一起，便是因为它</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="思考的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第六章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E5%85%AD%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="应用逻辑顺序" scheme="https://yb705.github.io/tags/%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(五)-归纳推理与演绎推理</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/</id>
    <published>2021-08-15T04:16:02.000Z</published>
    <updated>2021-08-15T04:25:08.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="演绎推理和归纳推理"><a href="#演绎推理和归纳推理" class="headerlink" title="演绎推理和归纳推理"></a>演绎推理和归纳推理</h2><p>之前我们讲过金字塔结构的文章有两个表达方向，分别是纵向和横向，其中”纵向“的内容，指的是自上而下表达和自下而上分析。那么本篇文章，我们主要讨论下</p><p>同一层次思想的横向表达逻辑，即<strong>演绎推理</strong>和<strong>归纳推理</strong>。</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804114439350.png" alt="image-20210804114439350"></p><h3 id="演绎推理"><a href="#演绎推理" class="headerlink" title="演绎推理"></a>演绎推理</h3><h4 id="演绎推理的步骤"><a href="#演绎推理的步骤" class="headerlink" title="演绎推理的步骤"></a>演绎推理的步骤</h4><p>从上面的推理图形我们可以看出，演绎推理就是个单线程的线性推理。那么我们将其化繁为简，总结成三步：</p><ol><li>提出大家广泛认可的一件事情/现象</li><li>从普遍中提出某个个体</li><li>依照前两句话顺理成章地推论出个体必然会发生的事情</li></ol><p>当然，除了上面的这种总结之外，我们还可以将其概括成另一种模式：</p><ol><li>第一个思想是提出大众认可的事情/现象</li><li>第二个思想则是针对第一个思想中提到的主语或谓语来做特殊个体的阐述</li><li>通过前两个思想合理地推导出第三个结论思想</li></ol><h4 id="演绎推理的缺点"><a href="#演绎推理的缺点" class="headerlink" title="演绎推理的缺点"></a>演绎推理的缺点</h4><p>相较于归纳推理来说，演绎推理是一个很容易理解的内容。但在实际写作过程中，还是希望大家尽量不要用演绎推理，因为它有个很难让人忽视的缺点。下面我用一个栗子来解释这个缺点。</p><blockquote><ol><li>非洲是一片矿藏非常丰富的大陆。</li><li>非洲人民自己开采矿物，并将矿物出口给其它国家。</li><li>非洲人民通过矿物买卖所赚的钱来购买自己所需要的生活用品。</li><li>较多的矿物被出口给其它国家，导致非洲各国自身的矿产需求得不到满足。</li><li>由于非洲国家严重匮乏矿产使用，所以大部分的基础设施均没有搭建。</li><li>基础设施的缺失对社会/国家经济的影响非常大，因此，大部分非洲人民都处于贫穷的生活状态之中。</li></ol></blockquote><p>PS：不知道大家在看完上面这段话是什么感受，反正我是长出了一口气。</p><p>上面的这段话就是一段典型的用演绎推理来表达的内容。从第一句到第六句呈线性推演逻辑，且逻辑本身非常清晰，不存在分歧。但是我认为“这段话很啰嗦”的这个观点应该也是不存在分歧吧。可以看出，这段话自身的思考逻辑并没有问题，但是它的表达逻辑却有问题，最终导致段落冗余复杂，“劝退”读者。</p><p>优化改进往往比推倒重来所需要的成本低，那么要如何改善演绎推理的缺点呢？</p><h4 id="演绎推理的实际使用"><a href="#演绎推理的实际使用" class="headerlink" title="演绎推理的实际使用"></a>演绎推理的实际使用</h4><p>在阐述具体的改善方法之前，我们先来看一个例子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804141552455.png" alt="image-20210804141552455"></p><p>这也是一个典型的演绎推理的思路流程，同样有着之前我们提到的问题——冗余复杂。当读者在阅读到“出现问题的原因”这部分的时候，他就需要去回想之前”制度出现的问题“这部分，并将原因一与问题一对应起来，同理原因二也需要与问题二联系起来。那当读者阅读到“解决问题的措施”这部分内容的时候呢？这时，他就需要寻找前面的内容，并将措施一，原因一，问题一这三者一一对应。说实话，别说读者了，就算作者重新阅读自己写的这部分内容恐怕都会感到烦躁吧。</p><p>上文提过，表达逻辑有两种，一个是演绎逻辑，一个是归纳逻辑。那么<strong>当其中一种方法单靠自己已经无法修正自身的缺点时，我们就可以尝试搭配另一种方法来去改善</strong>，所以接下来我们在上面说到的例子中加入归纳推理：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804145807426.png" alt="image-20210804145807426"></p><p>结果自不必多说，相较于之前的单演绎推理来说，演绎+归纳的组合使得思考逻辑和表达逻辑，都要清晰很多，且简单易懂，没有冗余。</p><h4 id="内容要在表达之前"><a href="#内容要在表达之前" class="headerlink" title="内容要在表达之前"></a>内容要在表达之前</h4><p>通过上面的讨论，可以看出在文章写作过程中，我们还要先明确表达的内容，再确定表达的方式。</p><p>譬如，在上面的例子中，当我们想要阐述“制度需要改革”这一中心思想时，要先明确它的支持内容都有哪些，先不需要考虑表达问题，只是简单地罗列出来，如下所示：</p><table><thead><tr><th>问题</th><th>原因</th><th>措施</th></tr></thead><tbody><tr><td>问题一</td><td>原因一</td><td>措施一</td></tr><tr><td>问题二</td><td>原因二</td><td>措施二</td></tr><tr><td>问题三</td><td>原因三</td><td>措施三</td></tr></tbody></table><p>然后，我们再考虑如何以一种<strong>合理且清晰</strong>的逻辑推理，将这些内容表达出来。（演绎，归纳，演绎+归纳）</p><h4 id="演绎推理的优点及使用原则"><a href="#演绎推理的优点及使用原则" class="headerlink" title="演绎推理的优点及使用原则"></a>演绎推理的优点及使用原则</h4><p>前面“贬低”了演绎推理这么久，那么是不是说演绎推理本身就是一无是处的呢？当然不是。我们将之前所提到的问题总结成一句话，就是正确的方法用在了错误的地方。实际上，演绎推理是比归纳推理更加符合人们的思考逻辑，且更加简单易懂。但是当读者需要向前翻阅十几页文章内容，来去寻找当前内容的对应部分的时候，再简单易懂的内容，也会变成烦躁难耐。</p><p>那演绎推理的使用范围在哪里呢？</p><p>相较于文章框架/大纲，演绎推理更加适用于底层的思维，即用两三句就可以表达的内容。这样既可以避免文章整体的冗余复杂，又可以保证关键句/底层语句之间的逻辑清晰，便于读者理解。</p><p>所以，在使用演绎推理的时候，要注意两点原则：</p><ol><li>演绎推理的过程不要超过4步</li><li>推导出来的结论不要超过两个</li></ol><h3 id="归纳推理"><a href="#归纳推理" class="headerlink" title="归纳推理"></a>归纳推理</h3><p>相较于演绎推理的易于思考，难于表达的特点来说，归纳推理正好相反。其原因便在于，当我们拿到一些有某种关系的内容时，大脑需要思考，并提供创造性思维，来将其中的共性总结出来。换言之，就是归纳推理要比演绎推理多需要一些主观能动性。 </p><p>那么接下来，我来说一下归纳推理的具体步骤。</p><h4 id="归纳推理的步骤"><a href="#归纳推理的步骤" class="headerlink" title="归纳推理的步骤"></a>归纳推理的步骤</h4><p>归纳推理的第一步，也是最重要的一步，就是寻找内容之间的共性，并将其总结出来。简单来说，就是用一个名词，或者是一句简单的话，来包括所有思维的相同点。举个栗子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210805115709310.png" alt="image-20210805115709310"></p><p>可以看到底层的每个观点都可以找到顶层思想的痕迹。也就是说，虽然底层的同类思想之间有着各种差异性，但是顶层思想却包含了它们的共同性，这就是归纳。</p><p>当然，在我们找完共性之后，还要记着要分别从自上而下和自下而上两个方向，再来检查两遍，毕竟“好事多磨”嘛。</p><p>那么总结一下，我们就知道归纳推理的建立是需要两个技能的：</p><blockquote><ol><li>正确定义一组思想</li><li>总结共性，并剔除同类思想之间的不相称性。</li></ol></blockquote><h3 id="归纳推理与演绎推理的区别"><a href="#归纳推理与演绎推理的区别" class="headerlink" title="归纳推理与演绎推理的区别"></a>归纳推理与演绎推理的区别</h3><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>在阐述归纳推理与演绎推理的区别之前，我想先分享两个例子，来帮助大家更好地理解演绎推理和归纳推理。</p><p>首先，先说演绎推理，大家先看下面的例子：</p><blockquote><ol><li>工党支持医疗社会化。</li><li>政府中有一些人支持医疗社会化。</li></ol><p>结论：政府中有一些人是工党。</p></blockquote><p>如果大家没有感受什么异样的话，可以再看看下面这个例子：</p><blockquote><ol><li>兔子跑得很快。</li><li>马跑得很快。</li></ol><p>结论：马是兔子。</p></blockquote><p>这样，大家应该感觉到有问题了吧。这说明，有一些演绎推论只是看起来是演绎流程，但是它们的内在逻辑是有问题的。那就是第二个思想并没有针对第一个思想中的主语或谓语提出讨论，而只是把两个看起来很像的句子放到一起。所以，这种情况不属于演绎推理，而是属于归纳推理。</p><p>那么既然提到了归纳推理，那我们正好也说一个例子：</p><blockquote><p>美国本土发现新增新冠病例。</p><p>日本本土发现新增新冠病例。</p><p>意大利本土发现新增新冠病例。</p><p>英国本土发现新增新冠病例。</p><p>法国本土发现新增新冠病例。</p><p>德国本土发现新增新冠病例。</p><p>新加坡本土发现新增新冠病例。</p><p>泰国本土发现新增新冠病例。</p><p>俄罗斯本土发现新增新冠病例。</p><p>结论：世界上大部分国家均出现了新冠病毒的新增患者。</p></blockquote><p>上面这段话是一段典型的归纳推理，先不提内容怎么样，我想大家看到的第一反应就是啰嗦吧。实际上，这就是一段新闻，但需要注意的是新闻本身是不包含任何主观思想的。也就是说，在任何阐述作者主题思想的文章中，新闻都是没有立足之地的。如果一定要有，那么新闻所扮演的也是支持文章主题思想的一个角色，<strong>不宜太长</strong>，只要篇幅恰到好处就可以了。（明明两三句就足以支撑文章内容，为什么一定要这么冗余呢？）</p><p>PS：生活中也是这样，须知有些事情过犹不及，恰到好处往往才是最难的。</p><h4 id="两者之间的区别"><a href="#两者之间的区别" class="headerlink" title="两者之间的区别"></a>两者之间的区别</h4><p>前面说了这么多，总算是要讲到正题了。</p><p>演绎关系的建立，要求推理过程中的第二步对第一步作出评述，并推导出一个结论。而归纳关系则基于句子结构，必须找到各句主语或谓语之间的相同点，并根据这一相同点的出结论。</p><p>还要注意的是，大脑对归纳论述和演绎论述的完整性有一种预期，这使作者将思维“投射”到前方，预测作者的下一个句子。如果读者预期的结果与作者实际表达的不同，读者就会感到疑惑，不解，烦躁。因此，在呈现归纳或演绎过程之前，应当先告诉读者文章的主题思想，使读者能够更容易地跟上你的思路。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;演绎推理和归纳推理&quot;&gt;&lt;a href=&quot;#演绎推理和归纳推理&quot; class=&quot;headerlink&quot; title=&quot;演绎推理和归纳推理&quot;&gt;&lt;/a&gt;演绎推理和归纳推理&lt;/h2&gt;&lt;p&gt;之前我们讲过金字塔结构的文章有两个表达方向，分别是纵向和横向，其中”纵向“的内容，指</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第五章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%BA%94%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="归纳推理" scheme="https://yb705.github.io/tags/%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86/"/>
    
    <category term="演绎推理" scheme="https://yb705.github.io/tags/%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(四)-序言的具体写法</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/</id>
    <published>2021-08-15T04:11:08.000Z</published>
    <updated>2021-08-15T04:23:45.867Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h3><p>虽然在之前的章节中已经提到过序言的组成部分了，但为了引出接下来要说的内容，我还是想要再重复一遍序言的构成—“背景-冲突-疑问-解答”。具体结构如下所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210728133414453.png" alt="image-20210728133414453"></p><h3 id="序言，是一篇“故事”"><a href="#序言，是一篇“故事”" class="headerlink" title="序言，是一篇“故事”"></a>序言，是一篇“故事”</h3><p>当读者在拿到一篇文章准备阅读之前，他的大脑中往往会充斥很多与文章主题无关的想法。这时读者是很难去阅读文章内容的，因为那些想法在牵扯他的注意力。那么如何才能够抓住读者呢？—讲“故事”。</p><blockquote><p>深夜，两个衣衫褴褛的醉鬼行走在大街上。。。。。。</p></blockquote><p>或许在看到这句话之前，你可能在想“中午吃什么？”，“什么时候才能下班啊？”，“外面下雨了吗？”……</p><p>但是在看到这句话之后，你的想法就有可能变成了“晚上会发生什么可怕的事情吗？”，“他们为什么还不回家？”，“接下来会发生什么？”……</p><p>而当你产生上面的疑问的时候，就说明无关的想法已经被剔除了，当下你的注意力已经被这句话牢牢地抓住了，甚至还会有点期待接下来要说的内容。</p><p>所以当我们在写序言部分的时候，推荐采取<strong>平铺陈述</strong>的方法去描写，就像是在写一篇故事一样。或许，<strong>一段好的序言，就是一篇好的“故事”。</strong></p><h3 id="何时引入背景"><a href="#何时引入背景" class="headerlink" title="何时引入背景"></a>何时引入背景</h3><p>“背景”有两个特征，一个是符合文章主题，另一个就是读者已知或者是读者认可的与自身相关连的内容，并且可以引起读者的好奇，探索心。</p><p>那么什么时候可以引入“背景”呢？</p><p>一般说来，在“背景”内容前后的某句话有一个特征，那就是锁住特定的时间和特定的地点，举个栗子：</p><blockquote><p>7月28日至29日，受台风“烟花”减弱后的热带低压影响，河南仍有强降雨。</p><p>在24日举行的东京奥运会女子10米气步枪决赛中，中国选手杨倩夺得冠军，为中国代表团揽入本届奥运会第一枚金牌。</p></blockquote><p>可以看出，在上面的两句话中都有特定的时间和特定的地点，而接在这两句话之后的内容就可以顺理成章地阐述文章的”背景了“。</p><h3 id="什么是冲突"><a href="#什么是冲突" class="headerlink" title="什么是冲突"></a>什么是冲突</h3><p>这里的”冲突“并不是指两者之间的矛盾，在大多数情况下，它更像是某种转折。</p><p><strong>在序言中，”冲突“是推动故事情节发展的因素。</strong>举个栗子：</p><blockquote><p>产品经理是指在公司中针对某一项或是某一类的产品进行规划和管理的人员，可以说他们充斥着产品的一生。<strong>但是</strong>，最近有一些公司的总裁提出，产品经理在逐渐变成一个可有可无的职位。</p></blockquote><p>可以看到，在上面的例子中，”但是“后面的内容就是序言的”冲突“部分。它不仅”乘上“推动了”背景“的叙事发展，还”启下“引出了读者的疑问—”为什么产品经理在逐渐变得可有可无？“</p><h3 id="序言的搭建顺序"><a href="#序言的搭建顺序" class="headerlink" title="序言的搭建顺序"></a>序言的搭建顺序</h3><p>平常，我们在构建序言的时候，可以按照正常顺序依次搭建，即”背景-冲突-疑问-回答”。但并不是说所有的序言都要按照这个顺序来去阐述。有的时候，也可以考虑“冲突-背景-疑问”，“疑问-背景-冲突”等顺序。</p><p>实际上，有一些文学作品，像是《盗墓笔记》，《诡秘之主》等人气小说中有很多内容便是利用倒序，插叙，伏笔等非顺序的叙事手法来描写的。而这种叙事顺序不仅没有让读者的思路混乱，反而营造了一种引人入胜的感觉，非常吸引读者。</p><p>所以，我们在构思阐述序言的时候，不必按照某种固定的顺序，只要符合正常的表达逻辑就可以。</p><h3 id="关键句要点"><a href="#关键句要点" class="headerlink" title="关键句要点"></a>关键句要点</h3><p>在文章中，关键句要点不仅要符合文章主题，提醒读者，还要充当文章的整体框架。</p><p>实际上，每个关键要点都是文章中的部分思想总结。换言之，就是下层内容支持关键要点，而关键要点汇总成文章的主题。结构如下图所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210729132213211.png" alt="image-20210729132213211"></p><p>而列出关键要点的作用就是提前跟读者说好接下来文章的内容，将文章交给读者，让读者去选择去阅读接下来的哪个部分。同时，提前知道关键思想也可以帮助读者更好地去理解文章的内容。（毕竟，怀揣着结论去了解过程要比根据流程去探索结论更加容易些。）</p><h3 id="序言有多长"><a href="#序言有多长" class="headerlink" title="序言有多长"></a>序言有多长</h3><p>序言的长度要符合文章主题与读者的需要。如果过长的话，作者就需要花费多余的精力去将过多的熟悉的内容“提示”给读者，这样做的话，不仅会让文章变得冗余繁杂，也会消耗读者的耐心；如果过短的话，序言部分就会很难引起读者的初始疑问，继而让读者难以进入文章的引导。</p><p>所以说序言就是“一双腿”，既不用太长，也不用太短。只要能够保证读者与作者刚好站在同一高度上就是最合适的。</p><h3 id="关键要点也可以用引言"><a href="#关键要点也可以用引言" class="headerlink" title="关键要点也可以用引言"></a>关键要点也可以用引言</h3><p>之前提到了，序言有四个要素“背景-冲突-疑问-解答”。那么一个包含了关键要点的段落中，是不是也可以用“背景-冲突-疑问-解答”的引言形式作为开头呢？</p><p>答案当然是可以的。只不过，文章开头的序言部分是与文章主题相关的，用于引起初始疑问，而段落的引言部分是与关键要点相关，用于引起第N个疑问。</p><p><strong>同时，也要注意每个段落的小标题。不要使用“简介”，“序言”，“总结”等空泛的名词，因为它们很难引起读者的兴趣。这类名词虽然完整地概括了段落，但是却并没有将任何实际内容提示给读者，颇有一种“废话”的意味。当然，如果不知道如何给段落取一个好的小标题的话，那么直接参考本篇文章的小标题就可以了。</strong></p><p>最后再说一下在写引言的过程中需要注意的三个点：</p><blockquote><ol><li>引言不是“告诉”读者需要知道的内容，而是“提示”读者。</li><li>与序言相同，引言也包含四个要素，即“背景-冲突-疑问-解答”。</li><li>引言的长度要适中，要同时满足文章的要求和读者的需要。</li></ol></blockquote><h3 id="序言常见模式"><a href="#序言常见模式" class="headerlink" title="序言常见模式"></a>序言常见模式</h3><p>总结是将看似不同的事情放在一起，提炼它们之间的相同点，整理出一套常见模式的过程。对于有着某种共性的事情，我们可以通过套用这种模式来吸取经验教训，提高完成质量。所以总结是工作学习中非常重要的一项技能，也是一个从“经事”到“长智”的过程 。</p><p>因此，我们把商务文章中常见的序言整理成4套模式，来加深我们对序言的理解，提高写作效率。</p><h4 id="发出指示式"><a href="#发出指示式" class="headerlink" title="发出指示式"></a>发出指示式</h4><p>在日常工作过程中，我们经常会遇到公司老板要求员工区完成某项工作的情况。（这里的动词是“要求”或“指示”，而不是“请求”，“提示”。）</p><p>在这种情形下，我们可以将序言写成下面的模式：</p><blockquote><p>背景：公司要完成X</p><p>冲突：在完成X之前，我们要先完成Y</p><p>疑问：如何完成Y</p></blockquote><h4 id="请求批准式"><a href="#请求批准式" class="headerlink" title="请求批准式"></a>请求批准式</h4><p>在日常工作中，当现有的环境条件已经无法满足工作的要求时，员工一般会对公司提出优化请求。</p><p>举个栗子：车间要求工人们一天共要完成50个零件，但是即使算上加班的工作量，工人们一天也只能完成40个零件。所以为了满足公司的工作要求，设备部门就要向公司提交申请，更换一批可以提高生产效率的零件加工机器。</p><p>把上面的情形总结下来就是：</p><blockquote><p>背景：我们遇到了一个问题</p><p>冲突：我们制定了解决方案，该方案需要XXX支持</p><p>疑问：我应该批准吗</p></blockquote><h4 id="解决问题式"><a href="#解决问题式" class="headerlink" title="解决问题式"></a>解决问题式</h4><p>很多时候，尤其是在提供咨询的时候，我们写作的目的，就是告诉某人如何解决他所遇到的问题，即向读者解释解决问题的方法。这类文章的关键就在于步骤：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210730163043105.png" alt="image-20210730163043105"></p><p>那么这类序言总结一下就是：</p><blockquote><p>背景：必须要做X</p><p>冲突：还未做好完成X的准备</p><p>疑问：如何做？</p></blockquote><p><strong>这里还需要强调一下，开始写作之前，在纸上列出两个流程进行对比非常重要。也许你认为，你已经在这个领域工作很长时间了，完全了解问题出在哪里，但是，如果你不将这两个流程列出来进行比较，那么遗漏某个重要元素的可能性就很大。</strong></p><h4 id="优化比较式"><a href="#优化比较式" class="headerlink" title="优化比较式"></a>优化比较式</h4><p>上面提到的解决问题模式主要是给出解决问题的<strong>某个</strong>方法，但有的时候，上级或许会说“多拿几个方案出来”。需要注意的是，上级的这句话也许并不是在追求方案的数量，而是在他的心里可能已经有了某种解决方法，但他还是希望员工可以提供一些其它的思路，并通过对比不同方案的优劣性，来选择相对更好的那一个。</p><p>PS：所以说揣摩上级的心理还是很重要的</p><p>那么，在这种环境下，序言就可以总结成下面的模式：</p><blockquote><p>背景：遇到了什么问题</p><p>冲突：对问题提出了多个解决方案</p><p>疑问：选择哪种方案比较合适</p></blockquote><p>同时还要留意的是，在回答阶段不仅要解释各个方案的内容，还要比较方案之间的优缺点，以便作出选择，如下图所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210803121204279.png" alt="image-20210803121204279"></p><h3 id="序言的实际使用"><a href="#序言的实际使用" class="headerlink" title="序言的实际使用"></a>序言的实际使用</h3><p>我们在前面讲了很多有关于写序言的注意事项。但“纸上得来终觉浅,绝知此事要躬行”，所以接下来我们讲一下商务工作中常见的两类文章实例，一个是<strong>项目建议书</strong>，一个是<strong>项目阶段总结</strong>。</p><h4 id="项目建议书"><a href="#项目建议书" class="headerlink" title="项目建议书"></a>项目建议书</h4><p>项目建议书，顾名思义就是对某个项目作出建议，并罗列实施步骤。这类商务文章多用于咨询公司中咨询顾问对客户某个问题的回答，或者是帮助，可以说是<strong>咨询公司的“生命线”</strong>。那么该如何阐述项目建议书的序言部分呢？</p><blockquote><p>背景：客户有什么问题？</p><p>冲突：需要我做什么/需要提供什么帮助？</p><p>疑问：我对事件的处理有什么建议？</p><p>回答：依次描述具体措施/建议，并解释为什么这么做。</p></blockquote><h4 id="项目阶段总结"><a href="#项目阶段总结" class="headerlink" title="项目阶段总结"></a>项目阶段总结</h4><p>项目阶段总结主要用于某个项目或者是工程中阶段性的总结（PS：好一句废话…）。它主要有两个用途，一是对接上级领导，进行周期性/阶段性的工作汇报；二是对接客户，让客户了解流程进度，并及时地进行沟通交流。所以序言可以按照下面的内容来去构思：</p><blockquote><p>背景：介绍项目整体内容，并说明目前所处的阶段</p><p>冲突：在已经完成的内容中遇到了什么问题？/在接下来的工作中可能会遇到什么问题？</p><p>疑问：如何解决将要面对的问题？是否需要什么支持？</p><p>回答：提出需要的支持，依次阐述解决问题的步骤。</p></blockquote><h3 id="序言的作用"><a href="#序言的作用" class="headerlink" title="序言的作用"></a>序言的作用</h3><p>本文前面讲了这么多关于序言的内容，相信大家对于序言应该有了一个清晰的认识。那么最后，我再简单说下序言的作用。</p><p>不知道大家对于律师的当庭辩护有没有了解（感兴趣的朋友可以去听一听罗翔老师的讲课），我觉得序言就像是律师的开场白一样，起到一个框架的作用，限制接下来所讲到的内容不超出框架的范围。也就是说读者可以不同意作者的观点，但他的思考却不会超过序言所限定的范围。简单来说就是，作者认为“1+1=2”，但是读者却觉得有的时候“1+1=3”，这时读者便与作者产生了分歧，可读者却是知道作者为什么会得出“1+1=2”的结论。</p><p>同时读者可以更加顺理成章地理解作者的想法。换言之，作者认为“1+1=2”，那么读者便自然而然地知道“1+2=3”。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序言的结构&quot;&gt;&lt;a href=&quot;#序言的结构&quot; class=&quot;headerlink&quot; title=&quot;序言的结构&quot;&gt;&lt;/a&gt;序言的结构&lt;/h3&gt;&lt;p&gt;虽然在之前的章节中已经提到过序言的组成部分了，但为了引出接下来要说的内容，我还是想要再重复一遍序言的构成—“背景-冲</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第四章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="序言" scheme="https://yb705.github.io/tags/%E5%BA%8F%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(三)-金字塔构建</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E9%87%91%E5%AD%97%E5%A1%94%E6%9E%84%E5%BB%BA/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E9%87%91%E5%AD%97%E5%A1%94%E6%9E%84%E5%BB%BA/</id>
    <published>2021-08-15T04:08:51.000Z</published>
    <updated>2021-08-15T04:24:55.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经知道了一篇文章的结构和逻辑顺序,那么在知道这些的情况下,我们就可以开始着手构建金字塔了.金字塔的构建有两种方法—-自上而下和自下而上,接下来我依次介绍两种方法.</p><h2 id="自上而下"><a href="#自上而下" class="headerlink" title="自上而下"></a>自上而下</h2><p>通常来说,自上而下比自下而上要更为容易.因为在写文章之前,我们肯定已经确定好了文章的主题,知道写这篇文章是想表达什么.之后,再根据核心思想去依次添加其它的内容,自上而下依次搭建.</p><p>接下来,说一下构建步骤:</p><blockquote><ol><li>确定文章主题/中心思想</li><li>设想受众的疑问</li><li>用序言表述背景-冲突-疑问-回答</li><li>用”疑问-回答”模式,回答读者问题</li><li>复用”疑问-回答”模式,回答读者的新问题</li></ol></blockquote><h2 id="自下而上"><a href="#自下而上" class="headerlink" title="自下而上"></a>自下而上</h2><p>有的时候,我们并不知道具体要表达什么,只是脑海中会浮现一些有某种关联的关键句.这个时候,我们就可以用自下而上的方法来找出这些关键句之间的联系,进而逆推中心思想,来构建金字塔.</p><p>接下来,说一下构建步骤:</p><blockquote><ol><li>整理关键句</li><li>确定关键句之间的逻辑关系</li><li>将关键句归纳整理成各个部分,逆推每部分的核心思想</li><li>将每部分的和思想总结在一起,逆推整篇文章的中心思想</li><li>写作</li></ol></blockquote><h2 id="初学者注意事项"><a href="#初学者注意事项" class="headerlink" title="初学者注意事项"></a>初学者注意事项</h2><h3 id="写文章时优先考虑自上而下模式"><a href="#写文章时优先考虑自上而下模式" class="headerlink" title="写文章时优先考虑自上而下模式"></a>写文章时优先考虑自上而下模式</h3><p>通常，在写作之前没有人会不知道自己想要表达什么。</p><p>因此，相对于依据没有明显联系的关键句去自下而上地搭建金字塔，我们更擅长通过已知将要去表达的中心思想去自上而下地写文章。</p><h3 id="在写序言的时候，要先阐述背景"><a href="#在写序言的时候，要先阐述背景" class="headerlink" title="在写序言的时候，要先阐述背景"></a>在写序言的时候，要先阐述背景</h3><p>序言的结构顺序是背景-冲突-疑问-解答。之所以要按照这个顺序去搭建序言，便是因为冲突是为了背景服务的。同时背景也可以更好让读者与作者从一开始便站在同一位置，防止读者产生与文章主题不符的疑问。</p><h3 id="序言尽量阐述读者已知的或者认可的内容，且与读者相关连"><a href="#序言尽量阐述读者已知的或者认可的内容，且与读者相关连" class="headerlink" title="序言尽量阐述读者已知的或者认可的内容，且与读者相关连"></a>序言尽量阐述读者已知的或者认可的内容，且与读者相关连</h3><p>当序言中出现读者不了解或者不认可的内容时，他们可能会产生新的疑问，而这个疑问有可能会是作者不希望读者产生的，或者是与文章主题无关的。</p><p>一般来说，读者会对与自己相关连的事情更感兴趣。如果碰到与自己无关的文章，则会表现得兴趣缺缺。</p><h3 id="在写序言的时候可以多花一点时间"><a href="#在写序言的时候可以多花一点时间" class="headerlink" title="在写序言的时候可以多花一点时间"></a>在写序言的时候可以多花一点时间</h3><h3 id="在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好"><a href="#在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好" class="headerlink" title="在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好"></a>在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好</h3><p>我们在思考某一件事情的时候，往往是按照演绎推理的顺序去思考的。但是有的时候思考的顺序并不等于表达的顺序。所以为了防止读者思考与文章表述不搭配的情况，继而出现思维混乱，在写文章的时候，归纳推理比演绎推理要更适用一些。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经知道了一篇文章的结构和逻辑顺序,那么在知道这些的情况下,我们就可以开始着手构建金字塔了.金字塔的构建有两种方法—-自上而下和自下而上,接</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第三章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B8%89%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="金字塔的搭建" scheme="https://yb705.github.io/tags/%E9%87%91%E5%AD%97%E5%A1%94%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(二)-金字塔内部的结构</title>
    <link href="https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/"/>
    <id>https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/</id>
    <published>2021-07-26T05:16:00.000Z</published>
    <updated>2021-08-15T04:24:05.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我们在表达或者是写作的时候,往往会根据一个想法联想出许多其他的想法.这时我们不能将所有的想法一股脑地都”丢”出去,如果这样做的话,受众就会被我们”砸懵”了.所以我们在表达之前一定要按照某种逻辑顺序,梳理好自己要表达的东西.</p><h2 id="纵向"><a href="#纵向" class="headerlink" title="纵向"></a>纵向</h2><p>“维度”这个词想必大家都不会陌生.通常,我们所写在纸上或者是键盘敲打出来的文字只有一种方向,也就是从头到尾.所以,我们就可以说文字的维度是一维的.或者说我们所写的文章就是纵向的.</p><p>那么知道了文章的方向,我们便可以用”疑问-回答”的模式去编排我们的文章.接下来我简单说明一下这种模式:</p><p>通常人们会对未知的事物更感兴趣,所以在阅读的时候,往往也会偏好于之前没有了解过的文章.因此,当读者在阅读到一段之前从未接触过的思想/段落时,心里便会有疑问:”为什么”,”怎么回事”……接下来,文章的下一部分便会顺其自然地对给予解释,并提出作者的观点.但在回答了读者的疑问的同时,往往又会引出新的问题,继而引起读者新的疑问:”为什么这么说?”,”接下来还会发生什么?”……就这样周而复始,依次迭代,一个清晰明了的纵向结构就构建了出来,如下图所示:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/image-20210724100115729.png" alt="image-20210724100115729"></p><p>那么”疑问-回答”这种结构有什么好处呢?</p><p>我觉得最大的作用就是将作者的思想清晰准确地表达了出来.也就是说,读者可以不认同作者的某个观点,或者是对某个疑问的解答,但是却可以清楚地知道作者思考的整体过程和逻辑顺序.读者的理解与作者的表达不会产生歧义.</p><p>在搭建”疑问-回答”模式中,还有两点是要注意的:</p><ol><li>在做好回答问题的准备之前,先不要提出问题</li><li>答案要放在问题后面</li></ol><h2 id="横向"><a href="#横向" class="headerlink" title="横向"></a>横向</h2><p>上面我们说了文章的内部结构搭建,接下来我们再来说一下文章的逻辑顺序.通常,一段文字的表达有两种逻辑顺序,一个是<strong>演绎</strong>,一个是<strong>归纳</strong>.这两个是互不兼容的,也就是说一个意思的表达不能既有演绎,又有归纳.接下来,我简单说明一下这两种表达逻辑.</p><h3 id="演绎"><a href="#演绎" class="headerlink" title="演绎"></a>演绎</h3><p>演绎是一种由一般情况推论出特殊情况的逻辑表达,举个例子:</p><blockquote><p>人都会死.</p><p>苏格拉底是人.</p><p>所以苏格拉底会死.</p></blockquote><p>总结下来,就是第一个思想是一个耳熟能详的大众观点或者现象.而第二个思想是针对第一个思想的主语/谓语提出的个体现象.第三个思想就是前两个思想”名正言顺”地推论总结.</p><h3 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h3><p>归纳是一种根据多个观点的某种共性作出推论的逻辑表达,举个例子:</p><blockquote><p>美军进驻伊拉克.</p><p>巴勒斯坦军进驻伊拉克.</p><p>以色列军进驻伊拉克.</p></blockquote><p>可以看到,上面的三句话有某种共性,就是某军进驻伊拉克.那么将共性归纳起来,我们就可以推断出伊拉克<strong>可能</strong>将会发生战争.需要注意的是,推断只是针对于一般情况,不包含特殊情况,也就是说推断出来的事情不会100%发生.</p><h2 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h2><p>上面已经说了,金字塔机构可以使作者与读者不断地进行疑问-回答式对话.但是,除非引发这种对话的话题与读者有相关性,否则很难吸引读者的注意力.保证产生相关性的唯一办法,就是确保对话直接回答了已经存在于读者头脑中的疑问.</p><p>而文章的序言可以通过追溯问题的起源与发展来给出这一问题.需要注意的是问题的起源与发展必然以叙述的形式出现,应当按照典型的叙述模式展开.</p><p>这种典型的讲故事的呈现方式——–<strong>背景,冲突,疑问,回答</strong>———能够保证在引导读者了解你的思维过程之前,确保与作者站在同一个位置上.</p><p>总之,序言以讲故事的形式告诉读者,关于作者正在讨论的主题他已经了解或将要了解的相关信息,从而引起读者的疑问,这个疑问也是整篇文章将要回答的问题.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>了解纵向关系,就可以确定某一层次上的思想必须包含哪些信息(即必须回答读者针对上一层次的思想提出的新疑问).</p><p>了解横向关系,就可以判断你组织在一起的思想是否用符合逻辑的方式表达信息(即时候采用了正确的归纳/演绎论述).</p><p>更重要的是,了解读者最初提出的疑问,将确保你组织和呈现的思想与读者的有关性(即文章中的思想有助于回答读者的问题).</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;我们在表达或者是写作的时候,往往会根据一个想法联想出许多其他的想法.这时我们不能将所有的想法一股脑地都”丢”出去,如果这样做的话,受众就会被我们”砸</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第二章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内部结构" scheme="https://yb705.github.io/tags/%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(一)-为什么要用金字塔</title>
    <link href="https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/"/>
    <id>https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/</id>
    <published>2021-07-26T05:15:48.000Z</published>
    <updated>2021-08-15T04:24:18.464Z</updated>
    
    <content type="html"><![CDATA[<h3 id="归类分组"><a href="#归类分组" class="headerlink" title="归类分组"></a>归类分组</h3><h4 id="共性"><a href="#共性" class="headerlink" title="共性"></a>共性</h4><p>在日常生活中,我们经常会或主动或被动地接受信息.而对于这些信息,大脑便会自发的按照某种关系将其归类划分.譬如古希腊人便将夜晚看到的星星,按照彼此之间的距离长短,并参考生活中的图像,划分成12个星座.</p><p>再譬如下面的图形:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713134006230.png" alt="image-20210713134006230"></p><p>大部分人第一眼看到上面图像的时候,便会依据散点之间的距离,自动将散点划分成左右两个图像,而我却并没有设置任何前提条件.而这便是刚才所说的:<strong>人们在接受信息的时候,会自动依据信息的共性,将其归类分组</strong>.</p><h4 id="奇妙数字”7”"><a href="#奇妙数字”7”" class="headerlink" title="奇妙数字”7”"></a>奇妙数字”7”</h4><p>曾经有位科学家通过实验得出来一个结论:在一般情况下,人类大脑能够同时接收到的信息数量是有限的,通常不会超过7条.</p><p>举个例子:我去便利店购物,要买”<strong>橘子,香蕉,苹果,纸巾,牙刷,毛巾,薯片,虾条,干脆面,筷子,碗,勺</strong>“.</p><p>就上面提到的12样东西来说,朋友们第一次看下来可以记住几个?</p><p>如果记住了5-7个,那说明你的记忆力已经达到平均水平了;如果都记住了,那我只能说你很强!!!</p><p>那通过上面的例子我们可以得出一个结论:那就是人类的短期记忆力是有限的.</p><p>但是假如我们将它用金字塔的形式进行归类分组呢?</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141003449.png" alt="image-20210713141003449" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141111222.png" alt="image-20210713141111222" style="zoom:50%;"></p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141308015.png" alt="image-20210713141308015" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141350913.png" alt="image-20210713141350913" style="zoom:50%;"></p><p>好的,那么现在你能记住的多少?<del>反正我都能记住.</del></p><h4 id="逻辑关系"><a href="#逻辑关系" class="headerlink" title="逻辑关系"></a>逻辑关系</h4><p>其实上面的分组过程并不复杂,就是通过物品的共性来寻找一个名词,然后再去划分每样物品的类别,譬如香蕉,橘子和苹果都属于水果.这样一来,我们便将需要记住的12个物品划分成了4类,而我们只需要记住这4类就可以了.然后每一类名字下面有三个从属物品,我们再去以此记忆每类从属.</p><p>这种方法之所以会加强我们的记忆量,主要有三个原因:</p><ol><li>我们的大脑是有联想功能的.依据总结出来的抽象名词,我们自发的联想到与这个名词有关系的共性事物,譬如说看到”水果”这个词,脑海里便会自觉的浮现出经常吃的水果,像是香蕉啊,苹果什么的.这种联想减轻了我们的记忆负担.高层次的思想总会提示低层次的思想.</li><li>需要记忆的东西数量减少了,需要大脑处理的东西由原来的12样变成了4样.</li><li>参考每样事物之间的抽象的逻辑关系,将无序便成了有序.而我们的大脑总是很容易记住有某种顺序的东西(逻辑顺序也是顺序的一种).譬如数字12345比32541要好记的多.</li></ol><h3 id="自上而下-结论先行"><a href="#自上而下-结论先行" class="headerlink" title="自上而下,结论先行"></a>自上而下,结论先行</h3><p>其实人们在表述的时候,一般会先说最重要的东西.而对大多数情况下,结论比过程更重要.所以<strong>理清表达思想的顺序,先总结后具体的表达顺序是十分重要的</strong>.</p><p>鉴于每个人的生活,工作,环境等不同,对于同一样事,每个人的认知是会出现差异的.实际上,人们在理解一段话的时候,一部分的精力用来识别文字本身,另一部分精力便是用来构造每句话之间的逻辑关系.而如果不考虑每个人的文化水平的差异,那么这种认知偏差便是来源于每句话之间的逻辑关系.既然如此,我们为什么不在一开始就将结论或者说逻辑架构提前说清楚呢?</p><p>有的时候,与其说读者容易接收理解作者整体的思维顺序,倒不如说是读者在跟着作者的顺序走,继而去理解作者的思维.</p><h3 id="自下而上-思考分析"><a href="#自下而上-思考分析" class="headerlink" title="自下而上,思考分析"></a>自下而上,思考分析</h3><p>平常,我们在阅读或者是写作的时候,发现文章会按照表达思想的不同划分成不同的段落,而每个段落所表达的思想最后汇总成全篇文章的中心思想,这就是作者要告诉我们的东西,同时整篇文章遵循金字塔结构:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210719133157281.png" alt="image-20210719133157281"></p><p>所谓的思考分析就是这样:</p><p>将表达同一思想的语句放在一起,组成一个段落,生成一个观点.在这个段落里,每句话都为这个观点的成立提供了论述,或者说是证明;</p><p>而每个段落的观点最后汇总在一起,推导/证明全篇文章的中心思想.</p><p>简单来说就是:<strong>自上而下,每一层都是下一层的抽象总结;自下而上,每一层都对上一层的思想提供了支持.</strong></p><p>但是在我们搭建金字塔的过程中要注意三点:</p><blockquote><p>纵向:每一层的思想均是下一层思想的概括总结</p><p>横向:每个组别都要按照正确的分类放在一起</p><p>横向:每个组别/段落之间要有正确的逻辑顺序</p></blockquote><p>接下来,我们分别简单地解释一下.</p><h4 id="1-每一层的思想均是下一层思想的概括总结"><a href="#1-每一层的思想均是下一层思想的概括总结" class="headerlink" title="1.每一层的思想均是下一层思想的概括总结"></a>1.每一层的思想均是下一层思想的概括总结</h4><p>这一点实际上并不需要过多的解释.每个段落依照共性总结出一个思想,不然的话就会南辕北辙,逻辑混乱.</p><p>郭德纲之前说过一个包袱:</p><blockquote><p>某人去水果店买水果,指着苹果问售货员:”这个苹果甜吗?”</p><p>售货员说:”甜.”</p><p>问:”这种苹果产地在哪里?”</p><p>售货员说:”这个苹果是国外进口的,您看上面还有英文呢.”</p><p>问:”那这个苹果多少钱一斤?”</p><p>售货员说:”国外的就贵一点,20块钱一斤.”</p><p>某人说:”我还真没吃过国外的苹果.行吧,那给我拿二斤葡萄.”</p></blockquote><p>很有意思,这就是典型的逻辑混乱.全篇在说苹果的事,但是最后却买了葡萄.</p><h4 id="2-每个组别都要按照正确的分类放在一起"><a href="#2-每个组别都要按照正确的分类放在一起" class="headerlink" title="2.每个组别都要按照正确的分类放在一起"></a>2.每个组别都要按照正确的分类放在一起</h4><p>在阅读/写作的时候,我们为什么要将这两句话放在一个段落里面呢?就是因为这两句话有共性,或者说是表达了同一个意思.譬如说,椅子和桌子都属于家具,牙膏和毛巾都属于洗漱用品,汉堡和薯条都属于肯德基(PS:我想应该没有人去肯德基买皮鞋吧…..).</p><p>那么如何比较两个事物之间的共性呢?就看他们的共性名词大不大.譬如说:牙刷和沐浴露都属于生活用品,但是牙刷属于洗漱用品,沐浴露属于洗浴用品,而生活用品包含洗漱和洗浴用品.这样一来,就可以比较牙刷和沐浴露之间的共性相关了.</p><h4 id="3-每个组别-段落之间要有正确的逻辑顺序"><a href="#3-每个组别-段落之间要有正确的逻辑顺序" class="headerlink" title="3.每个组别/段落之间要有正确的逻辑顺序"></a>3.每个组别/段落之间要有正确的逻辑顺序</h4><p>每个段落或者说段落中的每句话都要按照一定的逻辑顺序去排列.而我们通常所用到的逻辑顺序通常有以下几种:</p><blockquote><p>顺序逻辑:首先,然后,最后,</p><p>时间逻辑:第一天,第二天,昨天,今天,明天</p><p>空间逻辑:在水下,在水上,在沙滩上,在山底,在半山腰,在山顶</p><p>结构逻辑:最重要的,重要的,其次,最差</p><p>因果逻辑:因为,所以,虽然,但是</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;归类分组&quot;&gt;&lt;a href=&quot;#归类分组&quot; class=&quot;headerlink&quot; title=&quot;归类分组&quot;&gt;&lt;/a&gt;归类分组&lt;/h3&gt;&lt;h4 id=&quot;共性&quot;&gt;&lt;a href=&quot;#共性&quot; class=&quot;headerlink&quot; title=&quot;共性&quot;&gt;&lt;/a&gt;共性&lt;/h</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第一章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="思维结构" scheme="https://yb705.github.io/tags/%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——流形学习（t-SNE）</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/</id>
    <published>2021-07-26T04:50:37.000Z</published>
    <updated>2021-07-26T05:04:22.326Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经说过<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>通常是用于数据变换的首选方法，使人能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。而有一类可用于可视化的算法叫做<strong>流形学习算法</strong>，它允许进行更复杂的映射，通常也可以给出更好的可视化。其中一个特别有用的算法就是<strong>t-SNE算法</strong>。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><strong>流形学习算法主要用于可视化，因此很少用来生成两个以上的新特征</strong>。其中一些算法（包括t-SNE）计算训练数据的一种新表示，但不允许变换新数据。这意味着这些算法不能用于测试集：<strong>准确地说，它们只能用于训练数据</strong>。流形学习对探索性数据分析是很有用的，但如果最终目的是监督学习的话，则很少使用。</p><p>t-SNE背后的思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。t-SNE首先给出每个数据点的随机二维表示，然后尝试让原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些保存表示哪些点比较靠近的信息。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>数据是来自于scikit-learn包含的一个手写数字数据集，在这个数据集中，每个数据点都是0到9之间手写数字的一张8x8的灰度图像，图像如下：<br><del>PS：最近博主很忙，实在是没有时间找新数据来做了。。。</del> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>digits=load_digits()<br>fig,axes=plt.subplots(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>),subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br><span class="hljs-keyword">for</span> ax,img <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(axes.ravel(),digits.images):<br>    ax.imshow(img)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>为了与PCA进行比较，我们先用PCA将降到二维的数据可视化。对前两个主成分作图，并按照类别对数据点着色：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">2</span>)<span class="hljs-comment">###构建一个PCA模型</span><br>pca.fit(digits.data)<span class="hljs-comment">###将digits数据变换到前两个主成分上</span><br>digits_pca=pca.transform(digits.data)<br>colors=[<span class="hljs-string">&quot;#476A2A&quot;</span>,<span class="hljs-string">&quot;#7851B8&quot;</span>,<span class="hljs-string">&quot;#BD3430&quot;</span>,<span class="hljs-string">&quot;#4A2D4E&quot;</span>,<span class="hljs-string">&quot;#875525&quot;</span>,<span class="hljs-string">&quot;#A83683&quot;</span>,<span class="hljs-string">&quot;#4E656E&quot;</span>,<span class="hljs-string">&quot;#853541&quot;</span>,<span class="hljs-string">&quot;#3A3120&quot;</span>,<span class="hljs-string">&quot;#535D8E&quot;</span>]<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>())<br>plt.ylim(digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>())<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_pca[i,<span class="hljs-number">0</span>],digits_pca[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一主成分&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二主成分&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756071762" alt="在这里插入图片描述"></p><p>这里我们将每个类别对应的数字作为符号来显示每个类别的位置。从上图可以看出，除了0，4，6以外，大部分数字都是重叠在一起的。</p><p>接下来我们将t-SNE应用于同一数据集，并对结果进行比较。由于t-SNE不支持变换新数据，所以TSNE类没有transfrom方法。我们可以调用fit_transform方法来代替，它会构建模型并立刻返回变换后的数据，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE<br>tsne=TSNE(random_state=<span class="hljs-number">42</span>)<span class="hljs-comment">###使用fit_transform而不是fit,因为TSNE没有transform方法</span><br>digits_tsne=tsne.fit_transform(digits.data)<span class="hljs-comment">###运行时间较久</span><br></code></pre></td></tr></table></figure><p>接下来我们也将处理过的数据可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br>plt.ylim(digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_tsne[i,<span class="hljs-number">0</span>],digits_tsne[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一分量&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二分量&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756111304" alt="在这里插入图片描述"></p><p>结果自不必多说，与PCA相比，t-SNE的结果非常棒。所有类型都被明确分开。数字1到9被分成几块，但大多数类别都形成一个密集的组。<strong>要记住，这种方法并不知道类别标签：它完全是无监督的。但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开</strong>。</p><p>t-SNE算法有一些调节参数，不过默认参数的效果通常就很好。感兴趣的朋友可以尝试修改perplexity和early_exaggeration，但作用一般很小。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实博主在了解这个算法的时候就在思考，这个算法有什么作用。毕竟说的再好听，也要在现实中有用才行。</p><p>实际上，t-SNE直接用于降维，并后接分类器比较少见。</p><p>当我们意识到需要降维时，一般是发现了特征间的高度线性相关，而t-SNE主打的是非线性降维。如果我们发现了线性相关，可能用PCA处理就可以了。即使发现了“非线性相关性”，我们也不会尝试用t-SNE降维再搭配一个线性分类模型，而会直接选择非线性的分类模型去处理。复杂的非线性关系不适合强行降维再做分类，而应该用非线性模型直接处理。如果是高度稀疏的矩阵，也有适合的分类器直接用，也没必要降维。</p><p>所以想了想，觉得t-SNE应该比较适合可视化，就像上面的图像一样，了解和验证数据或者模型。至于降维的话，还有很多局限性有待解决。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经说过&lt;a href=&quot;https://blog.csdn.net/weixin_43580339/article/details/117</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="流形学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="数据可视化" scheme="https://yb705.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="图片分类" scheme="https://yb705.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>数据拟合实际应用</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</id>
    <published>2021-07-26T04:44:49.000Z</published>
    <updated>2021-08-15T04:08:55.235Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经学习了很多关于监督学习的算法，但是最近博主在看有关于数据分析的书籍的时候，忽然觉得在实际应用中，我们很少会用得到机器学习，数据挖掘方面的东西。我们所需要做的就是得到实际生活中的数据，并找出数据之间的关系，然后再根据这个关系去做一些运营，决策等行为，仅此而已。所以这篇我要说一下关于数据拟合的一些东西。（其实与监督学习的那些算法相比，数据拟合可以说是非常简单了。）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>数据拟合又称曲线拟合</strong>，俗称<strong>拉曲线</strong>，是一种把现有数据透过数学方法来代入一条数式的表示方式。科学和工程问题可以通过诸如采样、实验等方法获得若干离散的数据，根据这些数据，我们往往希望得到一个连续的函数（也就是曲线）或者更加密集的离散方程与已知数据相吻合，这过程就叫做**拟合(fitting)**。</p><p><del>PS：上面的解释是从百度上抄的。</del></p><p>用通俗一点的话来说，就是我们通过某种方法得到了一些未知关系的数据，然后找到一条函数曲线，来表达数据之间的关系，并保证大部分的数据点可以落在曲线上。</p><p>这里我说一下用python来实现多项式拟合数据的方法。网上还有用最小二乘法和高斯算法来进行数据拟合的方法，但是实际工作中并不需要这么复杂的应用，所以这里就不再赘述了，感兴趣的朋友可以自行学习。</p><h2 id="数据拟合"><a href="#数据拟合" class="headerlink" title="数据拟合"></a>数据拟合</h2><p>而我之所以要说多项式拟合数据，最重要的原因便是<strong>数学上可以证明，任意函数都可以表示为多项式形式</strong>。</p><p>接下来，我们将数据拟合成2次多项式，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x=[<span class="hljs-number">10</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>,<span class="hljs-number">70</span>,<span class="hljs-number">80</span>]<span class="hljs-comment">#定义x、y散点坐标</span><br>x=np.array(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;自变量x :\n&#x27;</span>,x)<br>y=np.sin(x)+np.tan(x)<span class="hljs-comment">#三角函数任意加减</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;因变量y :\n&#x27;</span>,y)<br>f=np.polyfit(x,y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用2次多项式拟合，可改变多项式阶数</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式系数:\n&#x27;</span>,f)<br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式表达式:\n&#x27;</span>,p)<br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;函数拟合出来的数值:\n&#x27;</span>,yvals)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>我们将拟合出来的函数用图像表示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760940342" alt="在这里插入图片描述"></p><p>之前我有提到，任意函数都可以表示为多项式形式。而<strong>如果拟合效果不理想，那只能说明多项式的次数不够</strong>，接下来我们试试拟合成7次多项式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">f=np.polyfit(x,y,<span class="hljs-number">7</span>)<br>p=np.poly1d(f)<br>yvals = p(x)<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760973004" alt="在这里插入图片描述"></p><p>可以看出结果较之前好了不少。</p><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>其实我之所以写下这篇文章便是因为最近看书的时候，看到了一个星巴克关于咖啡市场的A/B实验。这个实验是想要验证某富人区，咖啡价格对于新客户转化率的影响，并根据影响执行相应的运营决策。在这个实验中，我们只知道咖啡价格以及该区域门店的客户转化率：</p><blockquote><p>咖啡单价—10,20,30,40,50,60,70,80</p><p>客户转化率—2.30258509, 2.99573227, 3.40119738, 3.68887945, 3.91202301,4.09434456, 4.24849524, 4.38202663</p></blockquote><p>看到这个实验，我的第一反应就是用监督学习里面的逻辑回归去进行拟合建模。可是转念一想，貌似并没有必要搞的这么复杂，我只需要将这两个数据之间的关系用一个多项式函数表示出来就可以了。</p><p>PS：或许这个函数的次数会非常高，系数也会非常复杂，但是那有怎么样，反正也不是人工手算，有什么头痛难解的计算统统丢给电脑就可以了，不然的话我学编程干什么。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">log_y=np.array([<span class="hljs-number">2.30258509</span>, <span class="hljs-number">2.99573227</span>, <span class="hljs-number">3.40119738</span>, <span class="hljs-number">3.68887945</span>, <span class="hljs-number">3.91202301</span>,<span class="hljs-number">4.09434456</span>, <span class="hljs-number">4.24849524</span>, <span class="hljs-number">4.38202663</span>])<br>f=np.polyfit(x,log_y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用6次多项式拟合，可改变多项式阶数</span><br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,log_y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;价格提高&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;客户转化率&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;A/B测试&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272761009236" alt="在这里插入图片描述"></p><p>结果如上所示，可以看出<strong>随着价格的提高，新客户转化率是逐渐提高的（该说真不愧是有钱人），但是转化率的增长速度是逐渐降低的</strong>。</p><p>也就是说，<strong>当咖啡价格提高到一定程度的时候，富人们便不会再倾心于昂贵的咖啡。而星巴克所需要做的便是要找出这个价格的临界值，并在该区域根据这个价格展开一些运营手段。这样做既可以最高限度地提高新客户转化率，同时也能将销售利润最大化。</strong></p><p><del>PS：这就是数据分析的魅力所在。</del></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>数据拟合是用来观察某一自变量归对因变量造成的影响，或者是二者之间的关系，适用于x与y的这种一对一关系:</p><blockquote><p>y=a<em>x^n+bx^(n-1)+…+m</em>x^2+n*x+d</p><p>x-自变量     y-因变量</p></blockquote><p>而在数据挖掘/机器学习中，我们往往探讨多个特征值对目标值的共同作用，单独的去研究每个特征值的影响是非常麻烦的，而且用数据拟合去处理是非常片面的，因为有些特征值之间也会相互作用:</p><blockquote><p>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+…+w[p]x[p]+b </p><p>这里x[0]到x[p]表示耽搁数据点的特征(本例中特征个数为p+1),w和b是学习模型的参数，y是预测结果</p></blockquote><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经学习了很多关于监督学习的算法，但是最近博主在看有关于数据分析的书籍的时候，忽然觉得在实际应用中，我们很少会用得到机器学习，数据挖掘方面的</summary>
      
    
    
    
    <category term="数据分析" scheme="https://yb705.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    <category term="数据拟合" scheme="https://yb705.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88/"/>
    
    
    <category term="多项式拟合" scheme="https://yb705.github.io/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88/"/>
    
    <category term="A/B测试" scheme="https://yb705.github.io/tags/A-B%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——非负矩阵分解（NMF）</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/</id>
    <published>2021-07-26T04:44:32.000Z</published>
    <updated>2021-07-26T05:07:06.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>非负矩阵分解（NMF）是一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>，也可以用于降维。与PCA相同，我们试图将每个数据点写成一些分量的加权求和。但<strong>在PCA中，我们想要的是正负分量，并且能够解释尽可能多的数据方差；而在NMF中，我们希望分量和系数均为负，也就是说，我们希望分量和系数都大于或等于0</strong>。因此，<strong>NMF只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。</strong></p><p>将数据分解成非负加权求和的这个过程，对由多个独立源相加（或叠加）创建而成的数据特别有用，比如多人说话的音轨或包含很多乐器的音乐。在这种情况下，NMF可以识别出组合成数据的原始分量。总的来说，与PCA相比，NMF得到的分量更容易解释，因为负的分量和系数可能会导致难以解释的抵消效应。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>接下来，我们将NMF应用于人脸识别。</p><h2 id="NMF实际应用"><a href="#NMF实际应用" class="headerlink" title="NMF实际应用"></a>NMF实际应用</h2><p><strong>1.数据源</strong></p><p>数据是之前我们已经处理好的人脸图像数据，一共有15个人物，每个人有10张头像。想了解具体处理过程的可以去看一下<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>。</p><p>提数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<span class="hljs-comment">###https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</span><br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>2.建模</strong></p><p>提取完数据集之后，我们划分数据集为训练集和测试集，并用核向量算法SVM来进行建模和评估。</p><p>SVM算法讲解传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">支持向量机（SVM）算法之补充说明</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144125570.png" alt="在这里插入图片描述"></p><p>这样，我们就得到了一个精度为23.6%的模型。</p><p><strong>3.NMF处理</strong></p><p>NMF的主要参数是我们想要提取的分量个数——<strong>n_components</strong>。通常来说，这个数字要小于输入特征的个数（否则的话，将每个像素作为单独的分量就可以对数据进行解释）。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">20</span>,random_state=<span class="hljs-number">0</span>,max_iter=<span class="hljs-number">10000</span>).fit(X_train)<span class="hljs-comment">###增加最大迭代次数，不然会预警</span><br>X_train_nmf=nmf.transform(X_train)<br>X_test_nmf=nmf.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_nmf,y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_nmf)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p>需要注意的是，如果特征值过多的话，NMF默认的迭代次数便会限制模型的精度，并且预警。所以我们还需要设立下NMF的最大迭代次数。最后得到的结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144752949.png" alt="在这里插入图片描述"></p><p>通常来说，提取的特征越多，即n_components越大，模型的精度就越高，但是模型的训练时间也就越长。这里我就不再继续尝试了，感兴趣的朋友们可以试试改变n_components的值来提高模型精度。</p><h2 id="与PCA的比较"><a href="#与PCA的比较" class="headerlink" title="与PCA的比较"></a>与PCA的比较</h2><p>PCA对于数据特征的处理是找到特征重建的最佳方向。而NMF通常并不用于对数据进行重建或者编码。而是寻找用于数据中的有趣的模式。正如我们之前提到的一样，<strong>NMF最适合于具有叠加结构的数据，包括音频，基因表达和文本数据</strong>。接下来我们通过一段模拟信号来与PCA比较一下。</p><p>假设有一段信号，它是由三个不同的信号源组成的，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>S=mglearn.datasets.make_signals()<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">1</span>))<br>plt.plot(S,<span class="hljs-string">&quot;_&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Time&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Signal&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702145638480.png" alt="在这里插入图片描述"></p><p>不幸的是，我们无法观测到原始信号，只能观测到三个信号的叠加混合。而我们的目的便是<strong>将混合信号分解成原信号</strong>。假设我们有100台测量装置，每个测量装置都为我们提供了一系列测量结果。所以接下来我们将数据混合成100维的状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">A=np.random.RandomState(<span class="hljs-number">0</span>).uniform(size=(<span class="hljs-number">100</span>,<span class="hljs-number">3</span>))<span class="hljs-comment">###假设有100台装置测量混合信号</span><br>X=np.dot(S,A.T)<span class="hljs-comment">###将数据混合成100维的状态</span><br></code></pre></td></tr></table></figure><p>接下来，我们分别用NMF和PCA来还原这三个信号：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">3</span>)<br>H=pca.fit_transform(X)<br>nmf=NMF(n_components=<span class="hljs-number">3</span>,max_iter=<span class="hljs-number">10000</span>,random_state=<span class="hljs-number">0</span>)<br>S_=nmf.fit_transform(X)<br></code></pre></td></tr></table></figure><p>最后，我们将结果画出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">models=[X,S,S_,H]<br>names=[<span class="hljs-string">&quot;观测信号&quot;</span>,<span class="hljs-string">&quot;真实信号&quot;</span>,<span class="hljs-string">&quot;非负矩阵（NMF）还原信号&quot;</span>,<span class="hljs-string">&quot;主成分分析（PCA）还原信号&quot;</span>]<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),gridspec_kw=&#123;<span class="hljs-string">&quot;hspace&quot;</span>:<span class="hljs-number">1</span>&#125;,subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br>plt.figure(figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> model,name,ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(models,names,axes):<br>    ax.set_title(name)<br>    ax.plot(model[:,:<span class="hljs-number">3</span>],<span class="hljs-string">&quot;_&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到，NMF在发现原始信号源是得到了不错的成果，而PCA的表现却很差，仅使用第一个成分来解释数据中的大部分变化。</p><p>这里需要注意的是，<strong>NMF生成的分量是没有顺序的</strong>。在上面的例子中，NMF分量的顺序与原始信号完全相同（可以从三条线的颜色看出来），但这纯属偶然。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>除了PCA和NMF之外，还有许多算法可用于将每个数据点分解为一系列固定分量的加权求和。通常描述对分量和系数的约定会涉及到概率论。如果朋友们对这种类型的模式感兴趣，可以去看下scikit-learn中关于独立成分分析（ICA），因子分析（FA）和稀疏编码（字典学习）等。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;非负矩阵分解（NMF）是一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于&lt;a href=&quot;https://blog.csdn.net</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="非负矩阵分解" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
    
    <category term="分解混合信号" scheme="https://yb705.github.io/tags/%E5%88%86%E8%A7%A3%E6%B7%B7%E5%90%88%E4%BF%A1%E5%8F%B7/"/>
    
    <category term="特征重建" scheme="https://yb705.github.io/tags/%E7%89%B9%E5%BE%81%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析（PCA）应用——特征提取_人脸识别（下）</title>
    <link href="https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2021-06-27T12:39:34.000Z</published>
    <updated>2021-07-26T04:46:02.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在上一篇文章中，我简单说了下利用python对图像进行操作的基础知识，不了解这方面的小伙伴可以去查看下。（传送门——<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>）</p><p>接下来我们来看一下关于人脸识别的模型训练，以及PCA对机器学习流程的优化。</p><p>数据集就是我们在<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>中已经处理完的图像数据，这里就不再赘述了。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>人脸识别的一个常见任务就是看某个前所未见的人脸是否属于数据库中的某个已知人物。这在照片收集，社交媒体和安全应用中都有应用。解决这个问题的方法之一就是构建一个分类器，每个人都是单独的一个类别。但人脸数据库中通常有许多不同的人，而同一个人的图像很少（也就是说，每个类别的训练样例很少）。这使得大多数分类器的训练都很困难。另外，通常你还想要能够轻松添加新的人物，不需要重新训练一个大型模型。</p><p>一个简单的解决方法是使用单一最近临分类器，寻找与你要分类的人脸最为相似的人脸。由于上面我们设定的数据集中，每个人物都有10张图片，所以这个分类器原则上可以处理每个类别只有10个训练样例的情况。</p><p>PS：由于之前有讲过k近邻算法，所以这里就不在赘述了，感兴趣的朋友可以自行查看——<a href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法-分类实操</a>。</p><p>接下来，我们看下<strong>KNeighborsClassifier</strong>的表现如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>knn.fit(X_train,y_train)<span class="hljs-comment">###训练模型</span><br>prediction=knn.predict(X_test)<span class="hljs-comment">###对测试集进行预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test,prediction))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625152141759.png" alt="在这里插入图片描述"></p><p>我们得到的精度为18.4%。对于包含15个类别的分类问题来说，这实际上不算太差（随机猜测的精度约为1/15=6%)，但也不算太好。</p><p>那么接下来我们换成更为复杂的<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM核向量</a>算法，来试一试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160013600.png" alt="在这里插入图片描述"></p><p>可以看出，模型精度较knn近邻分类算法要有所提高，但是训练速度却有所下降。</p><h2 id="PCA特征提取"><a href="#PCA特征提取" class="headerlink" title="PCA特征提取"></a>PCA特征提取</h2><p>这里我们可以用到PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大的不同，使用这种原始表示很难获得面部特征。例如，如果利用像素距离，那么将人脸向右移动一个像素将会发生很大变化，得到一个完全不同的表示。我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用PCA的<strong>白化</strong>选项，它将主成分缩放到相同的尺度。变换后的结果与使用标准化（StandarScaler）相同。</p><p>PS：关于PCA的原理我有在之前讲过，感兴趣的小伙伴可以去看下——<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>那么接下来，我们对训练数据拟合PCA对象，并提取前50个主成分。然后对训练数据和测试数据进行变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">50</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<br>X_train_pca=pca.transform(X_train)<br>X_test_pca=pca.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_pca)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160907371.png" alt="在这里插入图片描述"></p><p>可以看到模型精度反而有所下降，这是因为我们对主成分数量的选择有问题，没有达到最优选取。通常主成分数量不会超过训练集的数据样本数，所以接下来我们通过遍历数字1-112，来挑选最合适的主成分数量:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">components_pca=pd.DataFrame(columns=[<span class="hljs-string">&quot;n_components&quot;</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>,<span class="hljs-string">&quot;knn_score&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">112</span>):<br>    pca=PCA(n_components=i+<span class="hljs-number">1</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<span class="hljs-comment">###拟合训练集</span><br>    X_train_pca=pca.transform(X_train)<span class="hljs-comment">###对训练集进行数据变换</span><br>    X_test_pca=pca.transform(X_test)<span class="hljs-comment">######对测试集进行数据变换</span><br>    clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>    clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>    clf_predict=clf.predict(X_test_pca)<span class="hljs-comment">#预测</span><br>    knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>    knn.fit(X_train_pca,y_train)<br>    knn_predict=knn.predict(X_test_pca)<span class="hljs-comment">###对测试集进行预测</span><br>    components_pca=components_pca.append([&#123;<span class="hljs-string">&quot;n_components&quot;</span>:i+<span class="hljs-number">1</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>:accuracy_score(y_test,clf_predict),<span class="hljs-string">&quot;knn_score&quot;</span>:accuracy_score(y_test,knn_predict)&#125;], ignore_index=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161509683.png" alt="在这里插入图片描述"></p><p>那么接下来，我们再选择能够使得上述模型达到最大精度的主成分数量：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161605292.png" alt="在这里插入图片描述"></p><p>自此，有关于人脸识别的基础流程便演示完了。</p><p>PS：说实话，在之前的监督学习里，完成了那么多次建模，就没有一个模型精度是低于80%的，像上面这么低的模型精度还真有点让人不适应。想要继续提高模型精度的朋友可以进行调参，也可以利用其它算法来建模，或者尝试其它处理数据的方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实，在要求人们评价人脸的相似度时，他们更可能会使用年龄，性别，面部表情和发型等属性，而这些属性很难从像素强度中推断出来。重要的是要记住，算法对数据（特别是视觉数据，比如人们非常熟悉的图像）的解释通常与人类的解释方式大不相同。</p><p>让我们回到PCA的具体案例。我们对PCA变换的介绍是：先旋转数据，然后删除方差较少的成分。另一种有用的解释是尝试找到一些数字（PCA旋转后的新特征值），使我们可以将测试点表示为主成分的加权求和。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;在上一篇文章中，我简单说了下利用python对图像进行操作的基础知识，不了解这方面的小伙伴可以去查看下。（传送门——&lt;a href=&quot;https:/</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="主成分分析" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
    
    <category term="特征提取" scheme="https://yb705.github.io/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
    <category term="人脸识别" scheme="https://yb705.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
</feed>
