<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yyb的花园</title>
  
  
  <link href="https://yb705.github.io/atom.xml" rel="self"/>
  
  <link href="https://yb705.github.io/"/>
  <updated>2021-08-15T04:36:57.930Z</updated>
  <id>https://yb705.github.io/</id>
  
  <author>
    <name>Yyb</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>无监督学习——凝聚聚类</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/</id>
    <published>2021-08-15T04:35:04.000Z</published>
    <updated>2021-08-15T04:36:57.930Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p><strong>凝聚聚类</strong>指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止规则为止。scikit-learn中实现的停止规则是簇的个数，因此相似的簇被合并，直到仅剩下指定个数的簇。还有一些<strong>链接准则</strong>，规定如何度量”最相似的簇“。这种度量总是定义在两个现有的簇之间。</p><p>scikit-learn中实现了以下三种选项：<br><strong>ward</strong>：默认选项。ward挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。<br><strong>average</strong>:average链接将簇中所有点之间的平均距离最小的两个簇合并。<br><strong>complete</strong>：complete链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。</p><p>ward适用于大多数数据集，在我们的例子中将使用它。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么average或complete可能效果更好。</p><p>接下来，我通过一个实际例子来说明这个算法。</p><h2 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h2><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/shub99/student-marks">Student Marks：https://www.kaggle.com/shub99/student-marks</a></p><p><img src="https://img-blog.csdnimg.cn/5f55d77402304711b55160282d76ebc1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这是一个简单的二维数据集，通过学生的学期成绩来判断这名学生最后可不可以升级（毕业）。该数据集包含100条数据记录，共有三个特征维度，分别是期中成绩（MID-SEM-MARKS），期末成绩（END-SEM-MARKS）以及升级结果（“0”:失败；“1”:成功)。</p><p><strong>2.数据处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\marks.txt&quot;</span><span class="hljs-comment">###https://www.kaggle.com/shub99/student-marks</span><br>marks=pd.read_csv(file_origin,sep=<span class="hljs-string">&#x27;\t&#x27;</span>,header=<span class="hljs-literal">None</span>)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p>需要注意的是，这份数据集的文件格式是txt，所以用<strong>pd.read_csv</strong>读取出来的数据是这个样子的：</p><p><img src="https://img-blog.csdnimg.cn/09c62b10516a4593b9393f690eb50c1f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>所以接下来还需要用方法<strong>split</strong>来依据特定字符“**,**”进行划分，示例代码如下所示：</p><p><img src="https://img-blog.csdnimg.cn/f9f9217012d246b7a4c72452489c1bb7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>同时，每条数据记录都是以字符串（string）的格式存储的，因此我们还需要用格式转换方法astype(“float”)来将原本的字符串格式（string）强制转换成浮点型（float），最终代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">marks[<span class="hljs-string">&quot;期中成绩&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">0</span>].astype(<span class="hljs-string">&quot;float&quot;</span>)<br>marks[<span class="hljs-string">&quot;期末成绩&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">1</span>].astype(<span class="hljs-string">&quot;float&quot;</span>)<br>marks[<span class="hljs-string">&quot;是否通过考试&quot;</span>]=marks[<span class="hljs-number">0</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot;,&quot;</span>).<span class="hljs-built_in">str</span>[<span class="hljs-number">2</span>]<br><span class="hljs-keyword">del</span> marks[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4f1e20f133084221b1c824d63e62b679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>3.使用凝聚聚类</strong></p><p>我们看一下凝聚聚类对这个数据集的效果如何。要注意，由于算法的工作原理，凝聚算法不能对新数据点作出预测。因此AgglomerativeClustering没有predict方法。为了构造模型并得到训练集上簇的成员关系，可以改用fit_predict方法，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering<br>agg=AgglomerativeClustering(n_clusters=<span class="hljs-number">2</span>)<br>X=marks.iloc[:,:-<span class="hljs-number">1</span>].values<span class="hljs-comment">####values方法很有用，专门用于将dataframe格式的某几列转化成相应维度的矩阵数组</span><br>assignment=agg.fit_predict(X)<br></code></pre></td></tr></table></figure><p>PS：推荐大家记一下values方法，这个方法可以把dataframe格式的数据完美转化成相应维度的矩阵数据（array），非常好用。</p><p>接下来，我们将聚类结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],assignment)<br>plt.xlabel(<span class="hljs-string">&quot;期中成绩&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;期末成绩&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/bb665ea4318847ea8f0e1d10a37d057e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到，算法完成了数据集的聚类。但是上面的图片并没有体现出分类的准确性，所以接下来我们通过对比源数据集中的特征分类来评估算法的精度：</p><p><img src="https://img-blog.csdnimg.cn/7d9c591ba8ec4d1bb16e508169935046.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>这里主要是通过统计已知结果和聚类结果的差值，来评估算法精度。所以最后，我们可以知道算法的精度为83%。</p><p><strong>4.层次聚类与树状图</strong></p><p>凝聚聚类生成了所谓的<strong>层次聚类</strong>。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。有时候，同时查看所有可能的聚类是有帮助的。所以接下来，我们考虑用一种工具来将层次聚类可视化。</p><p>不幸的是，目前scikit-learn没有绘制这种图像的功能。但可以利用SciPy轻松生成树状图。SciPy的聚类算法接口与scikit-learn的聚类算法稍有不同。SciPy提供了一个函数，接受数据数组X并计算出一个<strong>链接数组</strong>，它对层次聚类的相似度进行编码，然后我们可以将这个链接数组提供给SciPy的dendrogram函数来绘制树状图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram,ward<br>linkage_array=ward(X)<span class="hljs-comment">###将ward聚类应用于数据数组X，返回一个数组，制定执行凝聚聚类是跨离的距离</span><br>dendrogram(linkage_array)<span class="hljs-comment">###现在为包含簇之间距离的linkage_array绘制树状图</span><br><br><span class="hljs-comment">###开始画图</span><br>ax=plt.gca()<br><br>bounds=ax.get_xbound()<br>ax.plot(bounds)<br>ax.plot(bounds)<br>plt.xlabel(<span class="hljs-string">&quot;样本&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;簇之间的距离&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3bbb8e547c164212bf36248c3692b767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>树状图在底部显示数据点。然后以这些点（表示单点簇)作为叶节点绘制一棵树，每合并两个簇就添加一个新的父节点。</p><p>从下往上看，每层的数据点两两合并，依次类推，直到在顶层生成两个分支。这对应于算法中两个最大的簇。</p><p>树状图的y轴不仅说明凝聚算法中两个簇何时合并，每个分支的长度还表示被合并的簇之间的距离。同时我们还可以看到，最大的三个簇（绿色，红色，兰色）在合并成两个簇的过程中跨越了相对较远的距离。</p><p>不幸的是，凝聚聚类依然无法分析图形复杂的数据集，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/bc96c7c74a2346c281cf69914a1ed8b8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>但是，DBSCAN可以解决这个问题。碍于篇幅限制，下一篇文章我再来说一说这个算法。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;凝聚聚类&lt;/strong&gt;指的是许多基于相同原则构建的聚类算法，这一原则是：算法首先声明每个点是自己的簇，然后合并两个最相似的簇，直</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="凝聚聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="聚类可视化" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="层次聚类" scheme="https://yb705.github.io/tags/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——K均值聚类（下）</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2021-08-15T04:33:30.000Z</published>
    <updated>2021-08-15T04:34:52.470Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比较。希望大家在阅读下面的内容之前，已经了解了K均值，PCA与NMF算法的基础知识。</p><p>如果不清楚的话，可以点击下面的链接，来简单阅读下：<br>K均值：<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">无监督学习——K均值聚类（上）</a><br>PCA：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>；<a href="https://blog.csdn.net/weixin_43580339/article/details/118223829">主成分分析（PCA）应用（下）</a><br>NMF：<a href="https://blog.csdn.net/weixin_43580339/article/details/118416483">非负矩阵分解（NMF）</a></p><h2 id="图像重建与矢量量化"><a href="#图像重建与矢量量化" class="headerlink" title="图像重建与矢量量化"></a>图像重建与矢量量化</h2><p>虽然k均值是一种聚类算法，但在k均值和分解方法（比如之前讨论的PCA和NMF）之间存在一些有趣的相似之处。大家可能还记得，PCA试图找到数据中方差最大的方向，而NMF试图找到累加的分量，这通常对应于数据的“极值”或“部分”。两种方法都试图将数据点表示为一些分量之和。与之相反，k均值则尝试利用簇中心来表示每个数据点。你可以将其看作仅用一个分量来表示每个数据点，该分量有簇中心给出。这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点被称为<strong>矢量量化</strong>。<strong>实际上，对于k均值来说，重建就是在训练集中找到的最近的簇中心。</strong></p><p>接下来，我们通过一个实际例子来比较三者之间的关系。</p><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><p><strong>2.数据处理</strong></p><p>我之前写了一篇文章来讲图像数据处理方面的知识，需要了解的朋友可以自行阅读这篇文章，这里就不再赘述了。（传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>）</p><p>处理代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>3.划分数据集并进行建模</strong></p><p>下面我们划分数据集，并利用NMF，PCA和K均值来依次进行建模，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>nmf.fit(X_train)<br>pca=PCA(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>pca.fit(X_train)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans.fit(X_train)<span class="hljs-comment">###注意这是训练数据</span><br></code></pre></td></tr></table></figure><p>然后，我们依次利用训练好的模型，依次生成重建数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_reconstructed_pca=pca.inverse_transform(pca.transform(X_test))<span class="hljs-comment">###注意这是测试数据模型</span><br>X_reconstructed_kmeans=kmeans.cluster_centers_[kmeans.predict(X_test)]<br>X_reconstructed_nmf=np.dot(nmf.transform(X_test),nmf.components_)<br></code></pre></td></tr></table></figure><p><strong>4.恢复图像</strong></p><p>为了方便接下来的图像展示，我们首先将上面的重建数据整合在一起，构成一个二维列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>=[X_test,X_reconstructed_nmf,X_reconstructed_kmeans,X_reconstructed_pca]<br>X_reconstructed=[[],[],[],[]]<br>shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>)):<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>[i]:<br>        vector=np.matrix(m)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>        arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br>        X_reconstructed[i].append(arr2)<br></code></pre></td></tr></table></figure><p>接下来，为了比较方便，我们只选取前5张图像，并利用多图表结构将其展现出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>)) <br>plt.suptitle(<span class="hljs-string">&#x27;K均值，PCA与NMF的图像还原比较&#x27;</span>, fontsize=<span class="hljs-number">80</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_reconstructed)):<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        axes[l,n].imshow(X_reconstructed[l][n],cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>axes[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;原始图片&quot;</span>,fontsize=<span class="hljs-number">50</span>) <br>axes[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;NMF&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;K均值&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">3</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;PCA&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>plt.show()<span class="hljs-comment">###由于之前已经划分了数据集，这是利用训练出来的模型对测试数据集进行的图像恢复，所以只有38个图像，而不是原来的150个图像</span><br></code></pre></td></tr></table></figure><p>最终结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/cc0d37c9ecc046ed86a453402a9a870e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图就是利用了100个分量（簇中心)的k均值，PCA和NMF的图像重建对比，其中k均值的每张图像中仅使用了一个簇中心。可以看出，相对来说还是k均值的表现要好一点。当然，PCA和NMF亦可以通过调整参数来提高精度，感兴趣的朋友可以自行探究。</p><p><strong>5.矢量量化</strong></p><p>利用K均值做矢量量化的一个有趣之处在于，可以用比输入维度更多的簇来对数据进行编码。让我们回到two_moons数据。利用PCA或NMF，我们对这个数据无能为力，因为它只有两个维度。如果使用PCA和NMF将其降维到一维，将会完全破坏数据结构。但通过使用更多的簇中心，我们可以用K均值找到一种更具表现力的表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">10</span>,random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=<span class="hljs-string">&quot;Paired&quot;</span>,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],s=<span class="hljs-number">60</span>,marker=<span class="hljs-string">&quot;*&quot;</span>,c=<span class="hljs-built_in">range</span>(kmeans.n_clusters),linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/90543c394f3c49ad9401fd807e8228e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>我们使用了10个簇中心，也就是说，现在每10个点都被分配了0到9之间的一个数字。我们可以将其看作10个分量表示的数据（有10个新特征)，只有表示该点对应的簇中心的那个特征不为0，其它特征均为0。利用这个10维表示，我们就可以用线性模型来划分两个半月形，而利用原始的两个特征是不可能做到这一点的。</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>K均值是非常流行的聚类算法，因为它不仅相对容易理解和实现，而且运行速度也相对较快。同时K均值可以轻松扩展到大型数据集。</p><p>而K均值的缺点之一在于，它依赖随机初始化，也就是说，算法的输出依赖于随机种子。默认情况下scikit-learn用10种不同的随机初始化将算法运行10次，并返回最佳结果。当然，K均值还有一个缺点，就是簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（在现实世界中可能并不知道这个数字）。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。序</p><p>之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比较。希望大家在阅读下面的内容之前，已经了解了K均值，PCA与NMF算法的基础知识。</p><p>如果不清楚的话，可以点击下面的链接，来简单阅读下：<br>K均值：<a href="https://blog.csdn.net/weixin_43580339/article/details/119212829">无监督学习——K均值聚类（上）</a><br>PCA：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>；<a href="https://blog.csdn.net/weixin_43580339/article/details/118223829">主成分分析（PCA）应用（下）</a><br>NMF：<a href="https://blog.csdn.net/weixin_43580339/article/details/118416483">非负矩阵分解（NMF）</a></p><h2 id="图像重建与矢量量化-1"><a href="#图像重建与矢量量化-1" class="headerlink" title="图像重建与矢量量化"></a>图像重建与矢量量化</h2><p>虽然k均值是一种聚类算法，但在k均值和分解方法（比如之前讨论的PCA和NMF）之间存在一些有趣的相似之处。大家可能还记得，PCA试图找到数据中方差最大的方向，而NMF试图找到累加的分量，这通常对应于数据的“极值”或“部分”。两种方法都试图将数据点表示为一些分量之和。与之相反，k均值则尝试利用簇中心来表示每个数据点。你可以将其看作仅用一个分量来表示每个数据点，该分量有簇中心给出。这种观点将k均值看作是一种分解方法，其中每个点用单一分量来表示，这种观点被称为<strong>矢量量化</strong>。<strong>实际上，对于k均值来说，重建就是在训练集中找到的最近的簇中心。</strong></p><p>接下来，我们通过一个实际例子来比较三者之间的关系。</p><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><p><strong>2.数据处理</strong></p><p>我之前写了一篇文章来讲图像数据处理方面的知识，需要了解的朋友可以自行阅读这篇文章，这里就不再赘述了。（传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用（上）</a>）</p><p>处理代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>3.划分数据集并进行建模</strong></p><p>下面我们划分数据集，并利用NMF，PCA和K均值来依次进行建模，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>nmf.fit(X_train)<br>pca=PCA(n_components=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>pca.fit(X_train)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">100</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans.fit(X_train)<span class="hljs-comment">###注意这是训练数据</span><br></code></pre></td></tr></table></figure><p>然后，我们依次利用训练好的模型，依次生成重建数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_reconstructed_pca=pca.inverse_transform(pca.transform(X_test))<span class="hljs-comment">###注意这是测试数据模型</span><br>X_reconstructed_kmeans=kmeans.cluster_centers_[kmeans.predict(X_test)]<br>X_reconstructed_nmf=np.dot(nmf.transform(X_test),nmf.components_)<br></code></pre></td></tr></table></figure><p><strong>4.恢复图像</strong></p><p>为了方便接下来的图像展示，我们首先将上面的重建数据整合在一起，构成一个二维列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>=[X_test,X_reconstructed_nmf,X_reconstructed_kmeans,X_reconstructed_pca]<br>X_reconstructed=[[],[],[],[]]<br>shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>)):<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>[i]:<br>        vector=np.matrix(m)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>        arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br>        X_reconstructed[i].append(arr2)<br></code></pre></td></tr></table></figure><p>接下来，为了比较方便，我们只选取前5张图像，并利用多图表结构将其展现出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>)) <br>plt.suptitle(<span class="hljs-string">&#x27;K均值，PCA与NMF的图像还原比较&#x27;</span>, fontsize=<span class="hljs-number">80</span>, ha=<span class="hljs-string">&#x27;center&#x27;</span>)<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X_reconstructed)):<br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        axes[l,n].imshow(X_reconstructed[l][n],cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>axes[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;原始图片&quot;</span>,fontsize=<span class="hljs-number">50</span>) <br>axes[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;NMF&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;K均值&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>axes[<span class="hljs-number">3</span>,<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;PCA&quot;</span>,fontsize=<span class="hljs-number">50</span>)<br>plt.show()<span class="hljs-comment">###由于之前已经划分了数据集，这是利用训练出来的模型对测试数据集进行的图像恢复，所以只有38个图像，而不是原来的150个图像</span><br></code></pre></td></tr></table></figure><p>最终结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/cc0d37c9ecc046ed86a453402a9a870e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图就是利用了100个分量（簇中心)的k均值，PCA和NMF的图像重建对比，其中k均值的每张图像中仅使用了一个簇中心。可以看出，相对来说还是k均值的表现要好一点。当然，PCA和NMF亦可以通过调整参数来提高精度，感兴趣的朋友可以自行探究。</p><p><strong>5.矢量量化</strong></p><p>利用K均值做矢量量化的一个有趣之处在于，可以用比输入维度更多的簇来对数据进行编码。让我们回到two_moons数据。利用PCA或NMF，我们对这个数据无能为力，因为它只有两个维度。如果使用PCA和NMF将其降维到一维，将会完全破坏数据结构。但通过使用更多的簇中心，我们可以用K均值找到一种更具表现力的表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">10</span>,random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=<span class="hljs-string">&quot;Paired&quot;</span>,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],s=<span class="hljs-number">60</span>,marker=<span class="hljs-string">&quot;*&quot;</span>,c=<span class="hljs-built_in">range</span>(kmeans.n_clusters),linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/90543c394f3c49ad9401fd807e8228e4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>我们使用了10个簇中心，也就是说，现在每10个点都被分配了0到9之间的一个数字。我们可以将其看作10个分量表示的数据（有10个新特征)，只有表示该点对应的簇中心的那个特征不为0，其它特征均为0。利用这个10维表示，我们就可以用线性模型来划分两个半月形，而利用原始的两个特征是不可能做到这一点的。</strong></p><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>K均值是非常流行的聚类算法，因为它不仅相对容易理解和实现，而且运行速度也相对较快。同时K均值可以轻松扩展到大型数据集。</p><p>而K均值的缺点之一在于，它依赖随机初始化，也就是说，算法的输出依赖于随机种子。默认情况下scikit-learn用10种不同的随机初始化将算法运行10次，并返回最佳结果。当然，K均值还有一个缺点，就是簇形状的假设的约束性较强，而且还要求指定所要寻找的簇的个数（在现实世界中可能并不知道这个数字）。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们讲了聚类中比较常用的K均值算法，包括原理，相关参数以及实际操作。那么本篇文章，我们来讲一下更复杂一点的内容，即K均值，PCA与NMF之间的比</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="K均值聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="人脸识别" scheme="https://yb705.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
    <category term="图像聚类" scheme="https://yb705.github.io/tags/%E5%9B%BE%E5%83%8F%E8%81%9A%E7%B1%BB/"/>
    
    <category term="PCA与NMF" scheme="https://yb705.github.io/tags/PCA%E4%B8%8ENMF/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——K均值聚类（上）</title>
    <link href="https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/08/15/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2021-08-15T04:31:18.000Z</published>
    <updated>2021-08-15T04:33:20.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>近几年在机器学习领域里面，<strong>聚类</strong>是比较热门的一个词汇。它是将数据集划分成组的任务，这些组叫做<strong>簇</strong>。其目标是划分数据，使得一个簇内的数据点非常相似且簇内的数据点非常不同。与分类算法相似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。但是，与分类算法不同的是，<strong>聚类</strong>属于无监督学习，也就是说事先并不知道数据集的标签或者说特征值分类，而分类算法是监督学习，意味着已经提前知道了数据点的所属类别。接下来，我重点介绍下聚类里面比较常用的算法——<strong>k均值聚类</strong>。</p><h2 id="K均值聚类的原理讲解"><a href="#K均值聚类的原理讲解" class="headerlink" title="K均值聚类的原理讲解"></a>K均值聚类的原理讲解</h2><p><strong>1.算法介绍</strong></p><p>k均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的<strong>簇中心</strong>。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化，那么算法结束。</p><p><strong>2.代码讲解</strong></p><p>用scikit-learn应用K均值相当简单。下面，我们将其应用于模拟数据make_blobs，将Kmeans实例化，并设置我们要找的簇的个数。然后对数据调用fit方法。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br>X,y=make_blobs(random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###生成模拟的二维数据</span><br>kmeans=KMeans(n_clusters=<span class="hljs-number">3</span>)<span class="hljs-comment">###如果不指定n_clusters，它的默认值是8</span><br>kmeans.fit(X)<br></code></pre></td></tr></table></figure><p>PS：其中make_blobs的函数功能是生成各向同性的高斯斑点以进行聚类，感兴趣的同学可以自行百度研究下，这里就不在进行赘述了。</p><p><strong>(1) kmeans.labels_</strong></p><p>在算法运行期间，Kmeans为X中的每个训练数据点分配一个簇标签。我们可以在kmeans.labels_中找到这些标签，实际上<strong>kmeans.labels_就是算法结果（与kmeans.predict(X)的意义一样）</strong>：</p><p><img src="https://img-blog.csdnimg.cn/cdbf5685acc84df89d440048f1dcc040.png" alt="在这里插入图片描述"></p><p>如上图所示，聚类算法与分类算法有些相似，每个元素都有一个分配的标签。我们与数据集的原分类对比一下：</p><p><img src="https://img-blog.csdnimg.cn/10337246a5744094ac334c1ba5469dbc.png" alt="在这里插入图片描述"></p><p>两相对比可以看出，标签并不是真实的，因此标签本身并没有什么意义。算法给予你的唯一信息就是所有标签相同的数据点都是相似的。所以，<strong>我们不应该为其中一组的标签是0，另一组的标签是1赋予任何意义。</strong></p><p><strong>(2) kmeans.cluster_centers_</strong></p><p>之前提到的簇中心数据被保存在kmeans.cluster_centers_属性中：</p><p><img src="https://img-blog.csdnimg.cn/42edcd9c93314bb9a07738515cfd4596.png" alt="在这里插入图片描述"></p><p><strong>(3) 可视化</strong></p><p>接下来我们将数据分类结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,markers=<span class="hljs-string">&quot;o&quot;</span>)<br>mglearn.discrete_scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],markers=<span class="hljs-string">&quot;*&quot;</span>,markeredgewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/a59c850558184abca5faa7c85a0d8f3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>从上面的图像可以看出，数据被3个簇中心很好的分成了三个部分。</p><p>实际上，我们也可以使用更多或更少的簇中心：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>fig,axes=plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,ax=axes[<span class="hljs-number">0</span>])<br>kmeans=KMeans(n_clusters=<span class="hljs-number">5</span>)<span class="hljs-comment">###使用五个簇</span><br>kmeans.fit(X)<br>mglearn.discrete_scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],kmeans.labels_,ax=axes[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3dbf8441317240cda24a78382fa0b28b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="K均值聚类的实际应用"><a href="#K均值聚类的实际应用" class="headerlink" title="K均值聚类的实际应用"></a>K均值聚类的实际应用</h2><p><strong>1.数据来源</strong></p><p><a href="https://www.kaggle.com/adityakadiwal/water-potability">水质测试数据：https://www.kaggle.com/adityakadiwal/water-potability</a></p><p><img src="https://img-blog.csdnimg.cn/0c19bb2d6a5246cd899e4f5bbe674429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>该数据集共包含3277个数据点，并通过水质硬度，酸碱度等9个特征值，将水分成了两类。（0和1指代水的类别）</p><p>接下来，为了演示无监督学习，我们将其当作不知分类结果的数据集，并用K均值算法进行聚类。</p><p><strong>2.数据处理</strong></p><p>老规矩，还是先读取数据，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\water_potability.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/adityakadiwal/water-potability</span><br>water=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="https://img-blog.csdnimg.cn/e4694529b78047c0a672a6e92261d360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到这份数据中有很多空值，为了保证数据的完整性，我们将包含空值的行全部删掉（也可以用平均数来代替)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">water=water.dropna()<span class="hljs-comment">####去掉包含空值的行</span><br>water_test=water.drop(<span class="hljs-string">&quot;Potability&quot;</span>,axis=<span class="hljs-number">1</span>)<span class="hljs-comment">#####去掉分类结果</span><br>y=water[<span class="hljs-string">&quot;Potability&quot;</span>]<br></code></pre></td></tr></table></figure><p><strong>3.应用K均值聚类并与原结果进行比较</strong></p><p>我们对上面已经处理好的数据集应用K均值聚类算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###对应源数据中的两个分类结果</span><br>kmeans.fit(water_test)<br>y_pred=kmeans.predict(water_test)<br></code></pre></td></tr></table></figure><p>处理好之后，我们将聚类结果与源数据的分类进行差值计算。如果分类结果相同，则差值等于0，因此，通过统计差值中有多少个数值为0，来对聚类结果进行测评：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>y_original=np.array(y)<br>a=y_pred-y_original<span class="hljs-comment">###将结果进行比较，计算两者之间的差值</span><br><span class="hljs-built_in">len</span>(a[a==<span class="hljs-number">0</span>])/<span class="hljs-built_in">len</span>(a)<span class="hljs-comment">###如果分类结果相同，则等于0，所以统计差值中有多少个数值为0</span><br></code></pre></td></tr></table></figure><p>结果如下所示，无监督学习的K均值聚类精度为52.8%</p><p><img src="https://img-blog.csdnimg.cn/244906cb82864d078380d4005b6331b3.png" alt="在这里插入图片描述"></p><h2 id="K均值的精度"><a href="#K均值的精度" class="headerlink" title="K均值的精度"></a>K均值的精度</h2><p>我们从上面的结果可以看出来，实际应用中K均值的精度并没有很高。一是因为与监督学习中的分类器相比，K均值聚类属于无监督学习，也就是说事先并不知道分类结果，没有办法进行模型训练。二是因为即使你知道给定数据中簇的“正确”个数，k均值可能也不是总能找到它们。每个簇仅由其中心定义，这意味着每个簇都是凸形。因此，K均值只能找到相对简单的形状。K均值还假设所有簇在某种程度上具有相同的“直径”，它总是将簇之间的边界刚好画在簇中心的中间位置。</p><p>举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br>X,y=make_moons(n_samples=<span class="hljs-number">200</span>,noise=<span class="hljs-number">0.05</span>,random_state=<span class="hljs-number">0</span>)<br>kmeans=KMeans(n_clusters=<span class="hljs-number">2</span>)<span class="hljs-comment">###使用两个簇</span><br>kmeans.fit(X)<br>y_pred=kmeans.predict(X)<span class="hljs-comment">###与labels_相同，为新数据点分配簇标签</span><br>plt.scatter(X[:,<span class="hljs-number">0</span>],X[:,<span class="hljs-number">1</span>],c=y_pred,cmap=mglearn.cm2,s=<span class="hljs-number">60</span>)<br>plt.scatter(kmeans.cluster_centers_[:,<span class="hljs-number">0</span>],kmeans.cluster_centers_[:,<span class="hljs-number">1</span>],marker=<span class="hljs-string">&quot;*&quot;</span>,c=[mglearn.cm2(<span class="hljs-number">0</span>),mglearn.cm2(<span class="hljs-number">1</span>)],s=<span class="hljs-number">100</span>,linewidth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/414b500b5abd402d9c4e41988d468704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>对于上面的数据集，很明显，我们是希望聚类算法能够发现两个半月形，但是在这里利用K均值算法是不可能做到这一点的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇主要讲了关于K均值聚类的一些基础的东西，包括代码原理，实际应用以及应用中的一些局限性。限于篇幅原因，下一篇我再说一些有关K均值聚类稍微复杂一点的内容。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;近几年在机器学习领域里面，&lt;strong&gt;聚类&lt;/strong&gt;是比较热门的一个词汇。它是将数据集划分成组的任务，这些组叫做&lt;strong&gt;簇&lt;/s</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="K均值聚类" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/"/>
    
    
    <category term="聚类" scheme="https://yb705.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="簇" scheme="https://yb705.github.io/tags/%E7%B0%87/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(六)-应用逻辑顺序</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/</id>
    <published>2021-08-15T04:21:55.000Z</published>
    <updated>2021-08-15T04:35:48.976Z</updated>
    
    <content type="html"><![CDATA[<h2 id="应用逻辑顺序"><a href="#应用逻辑顺序" class="headerlink" title="应用逻辑顺序"></a>应用逻辑顺序</h2><p>当我们在写文章的时候，往往会把存在某种联系的思想放在同一组，进行讲解。之所以要把某些思想放在一起，便是因为它们之间存在着某种逻辑关系，既方便作者进行分析，也有助于读者的理解。而这种逻辑关系有三种，分别是因果关系，结构关系，重要性关系。接下来，我们就简单介绍下这三种关系。</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210806161601588.png" alt="image-20210806161601588"></p><h4 id="1-时间（因果）顺序"><a href="#1-时间（因果）顺序" class="headerlink" title="1.时间（因果）顺序"></a>1.时间（因果）顺序</h4><p>因果关系，顾名思义就是某些原因导致了某些结果。举个例子：</p><blockquote><p>受到市场波动，政策调控，疫情叠加的影响，7月份二三线城市中有七成左右的城市的土地成交面积环比下降超过10%。那么我们来依次分析这三个原因都带来了哪些影响。。。。。。</p></blockquote><p>上面的内容就是一个典型的因果关系的思想分组，即多个原因共同导致了一个结果。当然，不只有“先因后果”，还有“先果后因”。实际上，在生活或者工作中，我们往往会先设立一个目标，或者说是要取得某个结果，然后作出计划或者是列出步骤，来依次执行。举个简单的例子：</p><blockquote><p>公司想要在今年第二季度达到100w的业绩。那么为了达成这个目标，我们第一步要先做。。。；第二步再去做。。。；最后再完成。。。</p></blockquote><p>就像上面的例子中所说的，为了完成某个结果，而产生的过程或者系统，这段内容所依据的逻辑顺序就是时间顺序（如第一天，第二天；第一步，第二步等等）。</p><h4 id="2-空间（结构）顺序"><a href="#2-空间（结构）顺序" class="headerlink" title="2.空间（结构）顺序"></a>2.空间（结构）顺序</h4><p>结构逻辑往往是针对于某个可以分割的整体对象。譬如：</p><blockquote><p>一个销售公司往往包括后台职能部门，销售部门，以及客服部门。这些部门往往各行其职，其中后台职能部主要负责运营，人事，财务等，销售部门负责产品的具体销售，而客服则负责售后服务。</p></blockquote><p>当然，这个对象可以是具体的，比如电脑及其各个组成零件，非洲大陆及其组成国家等；也可以是概念的，比如消费者市场的构成，客户交易过程的组成部分（可监控分析异常值，优化流程，提高效率，降低成本）等。</p><h4 id="3-程度（聚类）顺序"><a href="#3-程度（聚类）顺序" class="headerlink" title="3.程度（聚类）顺序"></a>3.程度（聚类）顺序</h4><p>通常来说，我们往往会把有某种共性的思想分到一起，譬如说它们是由于成本降低所导致的三个后果，或者这两个优化举措都是董事会提出的。类似于这种有某些差异性，但是却又因为它们之间的共同性分到了一起的逻辑顺序，就是聚类结构。当然，我们也可以依照重要性的不同程度来划分组别。譬如说最重要的分到一类，次重要的分到一类，不重要的放到一类。</p><p>在日常写作的过程中，我们能用到的逻辑顺序可以是上面提到的单独的一种，也可以是三种混合使用，但无论如何，必须要包含至少一种。如果没有的话，建议回过头来检查一下文章的结构，看看有没有问题。</p><p>接下来详细说一下这三个常用的逻辑顺序。</p><h3 id="因果顺序"><a href="#因果顺序" class="headerlink" title="因果顺序"></a>因果顺序</h3><p>因果顺序是这三种顺序里面最简单，最容易理解的。因为它非常符合人类的正常思考逻辑，就像是数数一样。如果要求你从1数到5，那么你会不假思索地数出1，2，3，4，5。那么对比数数，我们在工作过程中也会经常提到诸如第一步，第二步，第三步；第一天，第二天，第三天；首先，然后，最后等词语。所以，这也就是为什么我私下里更喜欢把因果顺序称为自然顺序。不过，虽然因果顺序比较简单，但是在使用的过程中也要有几点需要注意，接下来，我来一一讲解。</p><h4 id="分清因果"><a href="#分清因果" class="headerlink" title="分清因果"></a>分清因果</h4><p>我前几天读了《精益数据分析》这本书，发现书里所提到的双边市场的概念用于因果顺序非常合适。那么套用在双边市场里，因果关系就被拆分成两个对象，即原因—结果。在具体的应用过程中，我们只需要对这两者负责，不用去考虑第三方，并且这两者又是相互作用的。所以将这两者分清，继而去单独分析就显得非常重要。举个例子：</p><blockquote><ol><li>“三条红线”限制了房企的现金流，导致公司的资金紧张。</li><li>为了参与第一季度的土地拍买，许多房企不得不抽调全国的布局资金来去抓住重点城市。</li><li>部分城市提高了申请银行房贷的门槛。</li><li>部分城市公告了要限制售价的重点地段。</li><li>大部分城市的商用住宅成交面积的环比，同比均有不同程度的下降。</li><li>很多城市的房价有所下降。</li><li>房产销售市场迎来了久违的降温。</li></ol></blockquote><p>上面描述的内容就给人一种似是而非的感觉，没有办法分清谁是原因，谁是结果。不着急，我们慢慢解决。我们依次查看每句话里面所包含的原因与结果：</p><blockquote><ol><li>“三条红线”和第一季度的土地拍卖使得房企公司的资金紧张。</li><li>房贷申请变得困难，使得成交量和成交面积较之前有所下降。</li><li>部分地段的房屋售价受到限制，使得最后的成交房价有所下降。</li><li>多种政策调控使得房产市场降温。</li></ol></blockquote><p>其实到这里，上面的内容已经整理的差不多了。整个梳理过程中，主要的困难就是原因和结果的区分，或者说是什么原因导致了什么结果，甚至还涉及到了子原因和子结果。譬如前三句话中，每一句话都涉及到了子结果（房企的资金紧张，成交面积下降，房价下降），而这三个子结果又共同导致了第四句话（市场降温）。</p><p>PS：当我们拿到一段内容，难以梳理其中关系的时候，可以尝试列出表格，或者构建一个金字塔模型（不要忘了我们学的是什么）。然后将每句话的原因和结果填写到表格或模型当中，进行梳理。这种有依据的梳理会使逻辑非常清晰，并且减少遗漏。</p><h4 id="揭示隐含逻辑"><a href="#揭示隐含逻辑" class="headerlink" title="揭示隐含逻辑"></a>揭示隐含逻辑</h4><p>我们在写文章时并不是追求越详细越好，毕竟有的时候，事无巨细也就意味着啰哩啰唆。适当的隐含往往能引起思考，抓住读者的注意力。当然，这其中的度要把握好，过于隐含可能会给读者带来疑惑，或者不符合读者的预期，而过于详细则会使文章冗余烦杂。举个栗子：</p><blockquote><p>在非洲，矿产的过度开采，给环境造成了非常严重的影响。</p><p>制度的改革给公司节省了大量的人工成本。</p></blockquote><p>像是上面的两句话就比较恰到好处。譬如矿产的过度开采给环境造成了影响，但是却不必罗列出来具体造成了什么样的影响，为什么会造成这样的影响。如果真的把每一条都详细地说明一遍，那对读者可真是莫大的考验啊。</p><h4 id="检查思想分组"><a href="#检查思想分组" class="headerlink" title="检查思想分组"></a>检查思想分组</h4><p>无论是写作，还是生活，工作当中的其它活动，检查都是一个非常重要的行为，尤其是从事数据方面的工作，一个小小的数字出现问题，就会造成一系列的分析错误，引起连锁反应。（PS：别问博主为什么很有经验）</p><p>在写作过程中，如果我们想要知道逻辑顺序有没有问题，主要方法就是顺着逻辑结构去检查。举个栗子：</p><blockquote><ol><li>市场调查</li><li>制定计划</li><li>实施计划</li><li>公司进入快速增长期，要加快投入。</li><li>公司进入缓慢增长期，将优化投入。</li><li>公司进入衰退期，要发现问题，及时修正。</li><li>公司进入亏损期，减少投入，必要时可放弃投入。</li></ol></blockquote><p>不知道大家有没有发现，上面这段分组内容的问题就是将原因与结果写到了一起，其中前三句的结果就是4-7句。那么该如何修改呢？</p><p>其实后四句总结一下就是公司对不同阶段的适应，最后就可以改成下面这样：</p><blockquote><ol><li>市场调查</li><li>制定计划</li><li>实施计划</li><li>公司要采取不同的措施来应对不同的阶段。</li></ol></blockquote><h3 id="结构顺序"><a href="#结构顺序" class="headerlink" title="结构顺序"></a>结构顺序</h3><p>什么是结构顺序？结构顺序就是使用示意图或者地图，查看交易流程，分析组织架构，阅读说明书等行为的顺序。它既可以针对具体的事物，如某种电器及其组成零件，也可以针对抽象的概念，如市场的构成。因此，结构顺序并没有特定的使用范围。下面，我就来讲解有关于结构顺序的内容。</p><h4 id="构建结构顺序"><a href="#构建结构顺序" class="headerlink" title="构建结构顺序"></a>构建结构顺序</h4><p>在搭建结构顺序的过程中，要注意“一个整体”和“两个原则”。“一个整体”指的是要针对一个可以分割成部分的整体；而“两个原则”针对的是各个部分，具体来讲就是：</p><ol><li>被分组的各个部分不仅要有各自的<strong>独立性</strong>，也要有相互之间的<strong>排斥性</strong>。</li><li>组成整体的各个部分要具有<strong>有穷性</strong>，不能有遗漏。</li></ol><p>举个简单地例子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812111642107.png" alt="image-20210812111642107"></p><p>某个轮胎公司<strong>只</strong>包含三个部门（职能包括财务，人事等），且这三个部门都是独立存在的，相互之间不存在重叠的情况。</p><h4 id="描述结构顺序"><a href="#描述结构顺序" class="headerlink" title="描述结构顺序"></a>描述结构顺序</h4><p>有的朋友可能有疑问：对于结构逻辑，在描述具体内容的时候应该按照什么顺序去描写？很简单，在我们搭建金字塔的时候，只要按照填写架构内容的顺序去描述就可以了。拿上面的例子来说，我们就可以按照销售，研发，职能的顺序来去具体地讨论各个部分。</p><p>实际上，在描述具体内容的时候，我们也可以搭配时间顺序，空间顺序，重要性顺序来去描述。比如，我现在想要讨论全球气候的变化问题，那么就可以按照空间顺序，从北到南，依次讨论北极，俄罗斯，亚洲，南极等各个地位的气候变化。</p><h4 id="修改结构顺序"><a href="#修改结构顺序" class="headerlink" title="修改结构顺序"></a>修改结构顺序</h4><p>在工作当中，改善往往比创新更常见，毕竟创新要更困难一些。所以有的时候我们并不需要从无到有地搭建一个结构，而只需要对原有的结构进行整理修改。接下来我就通过一个例子来去说明下如何修改原有结构：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812132110446.png" alt="image-20210812132110446"></p><p>上面是某个综合投资的集团公司领导组织，看起来比较混乱，并且有职能重叠的部分。现在老板要求你对公司的高层架构做一些调整，来提高工作效率，节省人力成本。这次先说修改后的结果：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210812133217111.png" alt="image-20210812133217111"></p><p>其实，调整结构顺序主要还是依照前面的“一个整体”和“两个原则”去做的。总的来说就是先将各个思想总结成一个名词，或者是打上标签，然后依照标签区分大分类与小分类的包含关系，最后确定要调整的主体，并围绕着两个原则确定各个部分的分组。</p><h4 id="检查结构顺序"><a href="#检查结构顺序" class="headerlink" title="检查结构顺序"></a>检查结构顺序</h4><p>与前面提到的因果顺序相同，在完成一篇文章之后，我们也要去顺着文章的顺序结构去检查。</p><p>最后再强调一下，我们在写文章的时候一定要先想好文章的主题，并列出结构，然后顺着文章结构去写，来避免遗漏。毕竟如果不按照提前构思好的结构去写，那么就算有遗漏，我们也很难有所察觉。</p><h3 id="重要性顺序"><a href="#重要性顺序" class="headerlink" title="重要性顺序"></a>重要性顺序</h3><p>最后，我们来说一下重要性顺序。重要性顺序指的是对一类具有共同特质的事物进行陈述，分析的顺序。接下来介绍一下它的相关内容。</p><h4 id="构建分组"><a href="#构建分组" class="headerlink" title="构建分组"></a>构建分组</h4><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/image-20210813125240155.png" alt="image-20210813125240155"></p><p>构建分组就是根据问题的共同特征来进行分类。如上图所示，我之所以要把5个问题分到一组，便是因为这5个问题可能都来自于财务部门，或者它们都是由市场震荡引起的。需要注意的是每组的问题数量并没有要求，也就是说只要符合逻辑，不管每组所包含的问题数量的差距有多么大，都是可以接受的。</p><p>完成了分组之后，接下来我们就要考虑组内事物的排列顺序了。一般情况下，我们是按照每个事物所包含的共同特质的高低来进行排序的。换句话说，假如我们把每个问题所包含的共同特质的高低，或者说每个问题的重要性，定义为1，2，3，4，5。其中，1为最低，5为最高。那么我们就可以按照54321，从高到低，由重要到不重要的顺序，去排列组内的问题。当然，按照12345的顺序去排列也是可以的，因为“从弱到强”往往更能带动读者的情绪。实际上，无论是12345还是54321，都只是作者的写作习惯的不同而已。只不过，有的时候，我们的写作目的是阐述分析，而不是为了触动读者感情。</p><h4 id="检查，辨别分组"><a href="#检查，辨别分组" class="headerlink" title="检查，辨别分组"></a>检查，辨别分组</h4><p>现实生活中，大部分事情都不是一蹴而就的。因此，对于一个初创或者假定的分组，我们需要反复地检查辨别其中的分组逻辑。这样做可以使我们避免错漏，并且更好地理解分组基础。接下来，我通过一个例子来说明如何进行分组。</p><blockquote><ol><li>库存数据的时间戳的格式有些错乱。</li><li>提交库存数据的周期并不符合分析师的要求。</li><li>库存数据的分析结果总是落后于市场的变化。</li><li>分析师们很难从前后矛盾的库存数据中得到有用的信息。</li><li>库存数据的特征维度错乱，无法对应。</li><li>表格中每条数据的标注字体各不相同。</li></ol></blockquote><p>不知道大家在阅读上面内容的时候是什么感受，反正我是没有摸清作者想要表达的东西。那么如何将这些内容分辨清楚呢？</p><p>首先，我们根据每句话所表达的意思来定义一些名词，或者是分类的类别，譬如说：库存数据格式有问题，提交时间不合适等。</p><p>然后，我们依照这些类别来对具体内容进行划分，如下所示：</p><table><thead><tr><th>内容</th><th>类别</th></tr></thead><tbody><tr><td>1.库存数据的时间戳的格式有些错乱；6.表格中每条数据的标注字体各不相同。</td><td>库存数据的格式有问题</td></tr><tr><td>2.提交库存数据的周期并不符合分析师的要求；3.库存数据的分析结果总是落后于市场的变化</td><td>库存数据的时间周期需要调整</td></tr><tr><td>4.分析师们很难从前后矛盾的库存数据中得到有用的信息；5.库存数据的特征维度错乱，无法对应。</td><td>库存数据的内容混乱</td></tr></tbody></table><p>最后，我们再根据文章的写作目的，架构以及题材，来确定组与组之间要遵循什么顺序去排列。譬如说，如果我想阐述服务器故障对库存数据造成了影响，那么文章中应该重点描述数据的格式与内容混乱的问题；如果我想说分析师们从数据中得出错误结论，那么文章前半部分就应该先说明库存数据的时间周期需要调整。</p><p>把上面的步骤总结一下就是;</p><ol><li>找出相关内容的分类类别。</li><li>将具体内容进行分类。</li><li>确定各组别的排列顺序。</li></ol><p>在本篇文章的最后，我想再强调一点：检查逻辑顺序是判断分组是否恰当的重要手段。当你遇到一组归纳性思想，需要找出其真实意义时，一定要先快速浏览一下该组中的所有思想。从中能否发现某种逻辑顺序（时间顺序，结构顺序，重要性顺序）？如果不能，那能否发现这样分组的基础（过程或流程，结构，类别），并采用某种逻辑顺序进行梳理？如果某一组罗列的思想过多，你能否发现它们的共同特性，并根据这些共同特性将思想进行细分，归纳，然后用一种逻辑顺序组织起来？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;应用逻辑顺序&quot;&gt;&lt;a href=&quot;#应用逻辑顺序&quot; class=&quot;headerlink&quot; title=&quot;应用逻辑顺序&quot;&gt;&lt;/a&gt;应用逻辑顺序&lt;/h2&gt;&lt;p&gt;当我们在写文章的时候，往往会把存在某种联系的思想放在同一组，进行讲解。之所以要把某些思想放在一起，便是因为它</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="思考的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第六章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E6%80%9D%E8%80%83%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E5%85%AD%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="应用逻辑顺序" scheme="https://yb705.github.io/tags/%E5%BA%94%E7%94%A8%E9%80%BB%E8%BE%91%E9%A1%BA%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(五)-归纳推理与演绎推理</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/</id>
    <published>2021-08-15T04:16:02.000Z</published>
    <updated>2021-08-15T04:25:08.011Z</updated>
    
    <content type="html"><![CDATA[<h2 id="演绎推理和归纳推理"><a href="#演绎推理和归纳推理" class="headerlink" title="演绎推理和归纳推理"></a>演绎推理和归纳推理</h2><p>之前我们讲过金字塔结构的文章有两个表达方向，分别是纵向和横向，其中”纵向“的内容，指的是自上而下表达和自下而上分析。那么本篇文章，我们主要讨论下</p><p>同一层次思想的横向表达逻辑，即<strong>演绎推理</strong>和<strong>归纳推理</strong>。</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804114439350.png" alt="image-20210804114439350"></p><h3 id="演绎推理"><a href="#演绎推理" class="headerlink" title="演绎推理"></a>演绎推理</h3><h4 id="演绎推理的步骤"><a href="#演绎推理的步骤" class="headerlink" title="演绎推理的步骤"></a>演绎推理的步骤</h4><p>从上面的推理图形我们可以看出，演绎推理就是个单线程的线性推理。那么我们将其化繁为简，总结成三步：</p><ol><li>提出大家广泛认可的一件事情/现象</li><li>从普遍中提出某个个体</li><li>依照前两句话顺理成章地推论出个体必然会发生的事情</li></ol><p>当然，除了上面的这种总结之外，我们还可以将其概括成另一种模式：</p><ol><li>第一个思想是提出大众认可的事情/现象</li><li>第二个思想则是针对第一个思想中提到的主语或谓语来做特殊个体的阐述</li><li>通过前两个思想合理地推导出第三个结论思想</li></ol><h4 id="演绎推理的缺点"><a href="#演绎推理的缺点" class="headerlink" title="演绎推理的缺点"></a>演绎推理的缺点</h4><p>相较于归纳推理来说，演绎推理是一个很容易理解的内容。但在实际写作过程中，还是希望大家尽量不要用演绎推理，因为它有个很难让人忽视的缺点。下面我用一个栗子来解释这个缺点。</p><blockquote><ol><li>非洲是一片矿藏非常丰富的大陆。</li><li>非洲人民自己开采矿物，并将矿物出口给其它国家。</li><li>非洲人民通过矿物买卖所赚的钱来购买自己所需要的生活用品。</li><li>较多的矿物被出口给其它国家，导致非洲各国自身的矿产需求得不到满足。</li><li>由于非洲国家严重匮乏矿产使用，所以大部分的基础设施均没有搭建。</li><li>基础设施的缺失对社会/国家经济的影响非常大，因此，大部分非洲人民都处于贫穷的生活状态之中。</li></ol></blockquote><p>PS：不知道大家在看完上面这段话是什么感受，反正我是长出了一口气。</p><p>上面的这段话就是一段典型的用演绎推理来表达的内容。从第一句到第六句呈线性推演逻辑，且逻辑本身非常清晰，不存在分歧。但是我认为“这段话很啰嗦”的这个观点应该也是不存在分歧吧。可以看出，这段话自身的思考逻辑并没有问题，但是它的表达逻辑却有问题，最终导致段落冗余复杂，“劝退”读者。</p><p>优化改进往往比推倒重来所需要的成本低，那么要如何改善演绎推理的缺点呢？</p><h4 id="演绎推理的实际使用"><a href="#演绎推理的实际使用" class="headerlink" title="演绎推理的实际使用"></a>演绎推理的实际使用</h4><p>在阐述具体的改善方法之前，我们先来看一个例子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804141552455.png" alt="image-20210804141552455"></p><p>这也是一个典型的演绎推理的思路流程，同样有着之前我们提到的问题——冗余复杂。当读者在阅读到“出现问题的原因”这部分的时候，他就需要去回想之前”制度出现的问题“这部分，并将原因一与问题一对应起来，同理原因二也需要与问题二联系起来。那当读者阅读到“解决问题的措施”这部分内容的时候呢？这时，他就需要寻找前面的内容，并将措施一，原因一，问题一这三者一一对应。说实话，别说读者了，就算作者重新阅读自己写的这部分内容恐怕都会感到烦躁吧。</p><p>上文提过，表达逻辑有两种，一个是演绎逻辑，一个是归纳逻辑。那么<strong>当其中一种方法单靠自己已经无法修正自身的缺点时，我们就可以尝试搭配另一种方法来去改善</strong>，所以接下来我们在上面说到的例子中加入归纳推理：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210804145807426.png" alt="image-20210804145807426"></p><p>结果自不必多说，相较于之前的单演绎推理来说，演绎+归纳的组合使得思考逻辑和表达逻辑，都要清晰很多，且简单易懂，没有冗余。</p><h4 id="内容要在表达之前"><a href="#内容要在表达之前" class="headerlink" title="内容要在表达之前"></a>内容要在表达之前</h4><p>通过上面的讨论，可以看出在文章写作过程中，我们还要先明确表达的内容，再确定表达的方式。</p><p>譬如，在上面的例子中，当我们想要阐述“制度需要改革”这一中心思想时，要先明确它的支持内容都有哪些，先不需要考虑表达问题，只是简单地罗列出来，如下所示：</p><table><thead><tr><th>问题</th><th>原因</th><th>措施</th></tr></thead><tbody><tr><td>问题一</td><td>原因一</td><td>措施一</td></tr><tr><td>问题二</td><td>原因二</td><td>措施二</td></tr><tr><td>问题三</td><td>原因三</td><td>措施三</td></tr></tbody></table><p>然后，我们再考虑如何以一种<strong>合理且清晰</strong>的逻辑推理，将这些内容表达出来。（演绎，归纳，演绎+归纳）</p><h4 id="演绎推理的优点及使用原则"><a href="#演绎推理的优点及使用原则" class="headerlink" title="演绎推理的优点及使用原则"></a>演绎推理的优点及使用原则</h4><p>前面“贬低”了演绎推理这么久，那么是不是说演绎推理本身就是一无是处的呢？当然不是。我们将之前所提到的问题总结成一句话，就是正确的方法用在了错误的地方。实际上，演绎推理是比归纳推理更加符合人们的思考逻辑，且更加简单易懂。但是当读者需要向前翻阅十几页文章内容，来去寻找当前内容的对应部分的时候，再简单易懂的内容，也会变成烦躁难耐。</p><p>那演绎推理的使用范围在哪里呢？</p><p>相较于文章框架/大纲，演绎推理更加适用于底层的思维，即用两三句就可以表达的内容。这样既可以避免文章整体的冗余复杂，又可以保证关键句/底层语句之间的逻辑清晰，便于读者理解。</p><p>所以，在使用演绎推理的时候，要注意两点原则：</p><ol><li>演绎推理的过程不要超过4步</li><li>推导出来的结论不要超过两个</li></ol><h3 id="归纳推理"><a href="#归纳推理" class="headerlink" title="归纳推理"></a>归纳推理</h3><p>相较于演绎推理的易于思考，难于表达的特点来说，归纳推理正好相反。其原因便在于，当我们拿到一些有某种关系的内容时，大脑需要思考，并提供创造性思维，来将其中的共性总结出来。换言之，就是归纳推理要比演绎推理多需要一些主观能动性。 </p><p>那么接下来，我来说一下归纳推理的具体步骤。</p><h4 id="归纳推理的步骤"><a href="#归纳推理的步骤" class="headerlink" title="归纳推理的步骤"></a>归纳推理的步骤</h4><p>归纳推理的第一步，也是最重要的一步，就是寻找内容之间的共性，并将其总结出来。简单来说，就是用一个名词，或者是一句简单的话，来包括所有思维的相同点。举个栗子：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86%E4%B8%8E%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/image-20210805115709310.png" alt="image-20210805115709310"></p><p>可以看到底层的每个观点都可以找到顶层思想的痕迹。也就是说，虽然底层的同类思想之间有着各种差异性，但是顶层思想却包含了它们的共同性，这就是归纳。</p><p>当然，在我们找完共性之后，还要记着要分别从自上而下和自下而上两个方向，再来检查两遍，毕竟“好事多磨”嘛。</p><p>那么总结一下，我们就知道归纳推理的建立是需要两个技能的：</p><blockquote><ol><li>正确定义一组思想</li><li>总结共性，并剔除同类思想之间的不相称性。</li></ol></blockquote><h3 id="归纳推理与演绎推理的区别"><a href="#归纳推理与演绎推理的区别" class="headerlink" title="归纳推理与演绎推理的区别"></a>归纳推理与演绎推理的区别</h3><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><p>在阐述归纳推理与演绎推理的区别之前，我想先分享两个例子，来帮助大家更好地理解演绎推理和归纳推理。</p><p>首先，先说演绎推理，大家先看下面的例子：</p><blockquote><ol><li>工党支持医疗社会化。</li><li>政府中有一些人支持医疗社会化。</li></ol><p>结论：政府中有一些人是工党。</p></blockquote><p>如果大家没有感受什么异样的话，可以再看看下面这个例子：</p><blockquote><ol><li>兔子跑得很快。</li><li>马跑得很快。</li></ol><p>结论：马是兔子。</p></blockquote><p>这样，大家应该感觉到有问题了吧。这说明，有一些演绎推论只是看起来是演绎流程，但是它们的内在逻辑是有问题的。那就是第二个思想并没有针对第一个思想中的主语或谓语提出讨论，而只是把两个看起来很像的句子放到一起。所以，这种情况不属于演绎推理，而是属于归纳推理。</p><p>那么既然提到了归纳推理，那我们正好也说一个例子：</p><blockquote><p>美国本土发现新增新冠病例。</p><p>日本本土发现新增新冠病例。</p><p>意大利本土发现新增新冠病例。</p><p>英国本土发现新增新冠病例。</p><p>法国本土发现新增新冠病例。</p><p>德国本土发现新增新冠病例。</p><p>新加坡本土发现新增新冠病例。</p><p>泰国本土发现新增新冠病例。</p><p>俄罗斯本土发现新增新冠病例。</p><p>结论：世界上大部分国家均出现了新冠病毒的新增患者。</p></blockquote><p>上面这段话是一段典型的归纳推理，先不提内容怎么样，我想大家看到的第一反应就是啰嗦吧。实际上，这就是一段新闻，但需要注意的是新闻本身是不包含任何主观思想的。也就是说，在任何阐述作者主题思想的文章中，新闻都是没有立足之地的。如果一定要有，那么新闻所扮演的也是支持文章主题思想的一个角色，<strong>不宜太长</strong>，只要篇幅恰到好处就可以了。（明明两三句就足以支撑文章内容，为什么一定要这么冗余呢？）</p><p>PS：生活中也是这样，须知有些事情过犹不及，恰到好处往往才是最难的。</p><h4 id="两者之间的区别"><a href="#两者之间的区别" class="headerlink" title="两者之间的区别"></a>两者之间的区别</h4><p>前面说了这么多，总算是要讲到正题了。</p><p>演绎关系的建立，要求推理过程中的第二步对第一步作出评述，并推导出一个结论。而归纳关系则基于句子结构，必须找到各句主语或谓语之间的相同点，并根据这一相同点的出结论。</p><p>还要注意的是，大脑对归纳论述和演绎论述的完整性有一种预期，这使作者将思维“投射”到前方，预测作者的下一个句子。如果读者预期的结果与作者实际表达的不同，读者就会感到疑惑，不解，烦躁。因此，在呈现归纳或演绎过程之前，应当先告诉读者文章的主题思想，使读者能够更容易地跟上你的思路。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;演绎推理和归纳推理&quot;&gt;&lt;a href=&quot;#演绎推理和归纳推理&quot; class=&quot;headerlink&quot; title=&quot;演绎推理和归纳推理&quot;&gt;&lt;/a&gt;演绎推理和归纳推理&lt;/h2&gt;&lt;p&gt;之前我们讲过金字塔结构的文章有两个表达方向，分别是纵向和横向，其中”纵向“的内容，指</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第五章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%BA%94%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="归纳推理" scheme="https://yb705.github.io/tags/%E5%BD%92%E7%BA%B3%E6%8E%A8%E7%90%86/"/>
    
    <category term="演绎推理" scheme="https://yb705.github.io/tags/%E6%BC%94%E7%BB%8E%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(四)-序言的具体写法</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/</id>
    <published>2021-08-15T04:11:08.000Z</published>
    <updated>2021-08-15T04:23:45.867Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h3><p>虽然在之前的章节中已经提到过序言的组成部分了，但为了引出接下来要说的内容，我还是想要再重复一遍序言的构成—“背景-冲突-疑问-解答”。具体结构如下所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210728133414453.png" alt="image-20210728133414453"></p><h3 id="序言，是一篇“故事”"><a href="#序言，是一篇“故事”" class="headerlink" title="序言，是一篇“故事”"></a>序言，是一篇“故事”</h3><p>当读者在拿到一篇文章准备阅读之前，他的大脑中往往会充斥很多与文章主题无关的想法。这时读者是很难去阅读文章内容的，因为那些想法在牵扯他的注意力。那么如何才能够抓住读者呢？—讲“故事”。</p><blockquote><p>深夜，两个衣衫褴褛的醉鬼行走在大街上。。。。。。</p></blockquote><p>或许在看到这句话之前，你可能在想“中午吃什么？”，“什么时候才能下班啊？”，“外面下雨了吗？”……</p><p>但是在看到这句话之后，你的想法就有可能变成了“晚上会发生什么可怕的事情吗？”，“他们为什么还不回家？”，“接下来会发生什么？”……</p><p>而当你产生上面的疑问的时候，就说明无关的想法已经被剔除了，当下你的注意力已经被这句话牢牢地抓住了，甚至还会有点期待接下来要说的内容。</p><p>所以当我们在写序言部分的时候，推荐采取<strong>平铺陈述</strong>的方法去描写，就像是在写一篇故事一样。或许，<strong>一段好的序言，就是一篇好的“故事”。</strong></p><h3 id="何时引入背景"><a href="#何时引入背景" class="headerlink" title="何时引入背景"></a>何时引入背景</h3><p>“背景”有两个特征，一个是符合文章主题，另一个就是读者已知或者是读者认可的与自身相关连的内容，并且可以引起读者的好奇，探索心。</p><p>那么什么时候可以引入“背景”呢？</p><p>一般说来，在“背景”内容前后的某句话有一个特征，那就是锁住特定的时间和特定的地点，举个栗子：</p><blockquote><p>7月28日至29日，受台风“烟花”减弱后的热带低压影响，河南仍有强降雨。</p><p>在24日举行的东京奥运会女子10米气步枪决赛中，中国选手杨倩夺得冠军，为中国代表团揽入本届奥运会第一枚金牌。</p></blockquote><p>可以看出，在上面的两句话中都有特定的时间和特定的地点，而接在这两句话之后的内容就可以顺理成章地阐述文章的”背景了“。</p><h3 id="什么是冲突"><a href="#什么是冲突" class="headerlink" title="什么是冲突"></a>什么是冲突</h3><p>这里的”冲突“并不是指两者之间的矛盾，在大多数情况下，它更像是某种转折。</p><p><strong>在序言中，”冲突“是推动故事情节发展的因素。</strong>举个栗子：</p><blockquote><p>产品经理是指在公司中针对某一项或是某一类的产品进行规划和管理的人员，可以说他们充斥着产品的一生。<strong>但是</strong>，最近有一些公司的总裁提出，产品经理在逐渐变成一个可有可无的职位。</p></blockquote><p>可以看到，在上面的例子中，”但是“后面的内容就是序言的”冲突“部分。它不仅”乘上“推动了”背景“的叙事发展，还”启下“引出了读者的疑问—”为什么产品经理在逐渐变得可有可无？“</p><h3 id="序言的搭建顺序"><a href="#序言的搭建顺序" class="headerlink" title="序言的搭建顺序"></a>序言的搭建顺序</h3><p>平常，我们在构建序言的时候，可以按照正常顺序依次搭建，即”背景-冲突-疑问-回答”。但并不是说所有的序言都要按照这个顺序来去阐述。有的时候，也可以考虑“冲突-背景-疑问”，“疑问-背景-冲突”等顺序。</p><p>实际上，有一些文学作品，像是《盗墓笔记》，《诡秘之主》等人气小说中有很多内容便是利用倒序，插叙，伏笔等非顺序的叙事手法来描写的。而这种叙事顺序不仅没有让读者的思路混乱，反而营造了一种引人入胜的感觉，非常吸引读者。</p><p>所以，我们在构思阐述序言的时候，不必按照某种固定的顺序，只要符合正常的表达逻辑就可以。</p><h3 id="关键句要点"><a href="#关键句要点" class="headerlink" title="关键句要点"></a>关键句要点</h3><p>在文章中，关键句要点不仅要符合文章主题，提醒读者，还要充当文章的整体框架。</p><p>实际上，每个关键要点都是文章中的部分思想总结。换言之，就是下层内容支持关键要点，而关键要点汇总成文章的主题。结构如下图所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210729132213211.png" alt="image-20210729132213211"></p><p>而列出关键要点的作用就是提前跟读者说好接下来文章的内容，将文章交给读者，让读者去选择去阅读接下来的哪个部分。同时，提前知道关键思想也可以帮助读者更好地去理解文章的内容。（毕竟，怀揣着结论去了解过程要比根据流程去探索结论更加容易些。）</p><h3 id="序言有多长"><a href="#序言有多长" class="headerlink" title="序言有多长"></a>序言有多长</h3><p>序言的长度要符合文章主题与读者的需要。如果过长的话，作者就需要花费多余的精力去将过多的熟悉的内容“提示”给读者，这样做的话，不仅会让文章变得冗余繁杂，也会消耗读者的耐心；如果过短的话，序言部分就会很难引起读者的初始疑问，继而让读者难以进入文章的引导。</p><p>所以说序言就是“一双腿”，既不用太长，也不用太短。只要能够保证读者与作者刚好站在同一高度上就是最合适的。</p><h3 id="关键要点也可以用引言"><a href="#关键要点也可以用引言" class="headerlink" title="关键要点也可以用引言"></a>关键要点也可以用引言</h3><p>之前提到了，序言有四个要素“背景-冲突-疑问-解答”。那么一个包含了关键要点的段落中，是不是也可以用“背景-冲突-疑问-解答”的引言形式作为开头呢？</p><p>答案当然是可以的。只不过，文章开头的序言部分是与文章主题相关的，用于引起初始疑问，而段落的引言部分是与关键要点相关，用于引起第N个疑问。</p><p><strong>同时，也要注意每个段落的小标题。不要使用“简介”，“序言”，“总结”等空泛的名词，因为它们很难引起读者的兴趣。这类名词虽然完整地概括了段落，但是却并没有将任何实际内容提示给读者，颇有一种“废话”的意味。当然，如果不知道如何给段落取一个好的小标题的话，那么直接参考本篇文章的小标题就可以了。</strong></p><p>最后再说一下在写引言的过程中需要注意的三个点：</p><blockquote><ol><li>引言不是“告诉”读者需要知道的内容，而是“提示”读者。</li><li>与序言相同，引言也包含四个要素，即“背景-冲突-疑问-解答”。</li><li>引言的长度要适中，要同时满足文章的要求和读者的需要。</li></ol></blockquote><h3 id="序言常见模式"><a href="#序言常见模式" class="headerlink" title="序言常见模式"></a>序言常见模式</h3><p>总结是将看似不同的事情放在一起，提炼它们之间的相同点，整理出一套常见模式的过程。对于有着某种共性的事情，我们可以通过套用这种模式来吸取经验教训，提高完成质量。所以总结是工作学习中非常重要的一项技能，也是一个从“经事”到“长智”的过程 。</p><p>因此，我们把商务文章中常见的序言整理成4套模式，来加深我们对序言的理解，提高写作效率。</p><h4 id="发出指示式"><a href="#发出指示式" class="headerlink" title="发出指示式"></a>发出指示式</h4><p>在日常工作过程中，我们经常会遇到公司老板要求员工区完成某项工作的情况。（这里的动词是“要求”或“指示”，而不是“请求”，“提示”。）</p><p>在这种情形下，我们可以将序言写成下面的模式：</p><blockquote><p>背景：公司要完成X</p><p>冲突：在完成X之前，我们要先完成Y</p><p>疑问：如何完成Y</p></blockquote><h4 id="请求批准式"><a href="#请求批准式" class="headerlink" title="请求批准式"></a>请求批准式</h4><p>在日常工作中，当现有的环境条件已经无法满足工作的要求时，员工一般会对公司提出优化请求。</p><p>举个栗子：车间要求工人们一天共要完成50个零件，但是即使算上加班的工作量，工人们一天也只能完成40个零件。所以为了满足公司的工作要求，设备部门就要向公司提交申请，更换一批可以提高生产效率的零件加工机器。</p><p>把上面的情形总结下来就是：</p><blockquote><p>背景：我们遇到了一个问题</p><p>冲突：我们制定了解决方案，该方案需要XXX支持</p><p>疑问：我应该批准吗</p></blockquote><h4 id="解决问题式"><a href="#解决问题式" class="headerlink" title="解决问题式"></a>解决问题式</h4><p>很多时候，尤其是在提供咨询的时候，我们写作的目的，就是告诉某人如何解决他所遇到的问题，即向读者解释解决问题的方法。这类文章的关键就在于步骤：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210730163043105.png" alt="image-20210730163043105"></p><p>那么这类序言总结一下就是：</p><blockquote><p>背景：必须要做X</p><p>冲突：还未做好完成X的准备</p><p>疑问：如何做？</p></blockquote><p><strong>这里还需要强调一下，开始写作之前，在纸上列出两个流程进行对比非常重要。也许你认为，你已经在这个领域工作很长时间了，完全了解问题出在哪里，但是，如果你不将这两个流程列出来进行比较，那么遗漏某个重要元素的可能性就很大。</strong></p><h4 id="优化比较式"><a href="#优化比较式" class="headerlink" title="优化比较式"></a>优化比较式</h4><p>上面提到的解决问题模式主要是给出解决问题的<strong>某个</strong>方法，但有的时候，上级或许会说“多拿几个方案出来”。需要注意的是，上级的这句话也许并不是在追求方案的数量，而是在他的心里可能已经有了某种解决方法，但他还是希望员工可以提供一些其它的思路，并通过对比不同方案的优劣性，来选择相对更好的那一个。</p><p>PS：所以说揣摩上级的心理还是很重要的</p><p>那么，在这种环境下，序言就可以总结成下面的模式：</p><blockquote><p>背景：遇到了什么问题</p><p>冲突：对问题提出了多个解决方案</p><p>疑问：选择哪种方案比较合适</p></blockquote><p>同时还要留意的是，在回答阶段不仅要解释各个方案的内容，还要比较方案之间的优缺点，以便作出选择，如下图所示：</p><p><img src="/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E5%BA%8F%E8%A8%80%E7%9A%84%E5%85%B7%E4%BD%93%E5%86%99%E6%B3%95/image-20210803121204279.png" alt="image-20210803121204279"></p><h3 id="序言的实际使用"><a href="#序言的实际使用" class="headerlink" title="序言的实际使用"></a>序言的实际使用</h3><p>我们在前面讲了很多有关于写序言的注意事项。但“纸上得来终觉浅,绝知此事要躬行”，所以接下来我们讲一下商务工作中常见的两类文章实例，一个是<strong>项目建议书</strong>，一个是<strong>项目阶段总结</strong>。</p><h4 id="项目建议书"><a href="#项目建议书" class="headerlink" title="项目建议书"></a>项目建议书</h4><p>项目建议书，顾名思义就是对某个项目作出建议，并罗列实施步骤。这类商务文章多用于咨询公司中咨询顾问对客户某个问题的回答，或者是帮助，可以说是<strong>咨询公司的“生命线”</strong>。那么该如何阐述项目建议书的序言部分呢？</p><blockquote><p>背景：客户有什么问题？</p><p>冲突：需要我做什么/需要提供什么帮助？</p><p>疑问：我对事件的处理有什么建议？</p><p>回答：依次描述具体措施/建议，并解释为什么这么做。</p></blockquote><h4 id="项目阶段总结"><a href="#项目阶段总结" class="headerlink" title="项目阶段总结"></a>项目阶段总结</h4><p>项目阶段总结主要用于某个项目或者是工程中阶段性的总结（PS：好一句废话…）。它主要有两个用途，一是对接上级领导，进行周期性/阶段性的工作汇报；二是对接客户，让客户了解流程进度，并及时地进行沟通交流。所以序言可以按照下面的内容来去构思：</p><blockquote><p>背景：介绍项目整体内容，并说明目前所处的阶段</p><p>冲突：在已经完成的内容中遇到了什么问题？/在接下来的工作中可能会遇到什么问题？</p><p>疑问：如何解决将要面对的问题？是否需要什么支持？</p><p>回答：提出需要的支持，依次阐述解决问题的步骤。</p></blockquote><h3 id="序言的作用"><a href="#序言的作用" class="headerlink" title="序言的作用"></a>序言的作用</h3><p>本文前面讲了这么多关于序言的内容，相信大家对于序言应该有了一个清晰的认识。那么最后，我再简单说下序言的作用。</p><p>不知道大家对于律师的当庭辩护有没有了解（感兴趣的朋友可以去听一听罗翔老师的讲课），我觉得序言就像是律师的开场白一样，起到一个框架的作用，限制接下来所讲到的内容不超出框架的范围。也就是说读者可以不同意作者的观点，但他的思考却不会超过序言所限定的范围。简单来说就是，作者认为“1+1=2”，但是读者却觉得有的时候“1+1=3”，这时读者便与作者产生了分歧，可读者却是知道作者为什么会得出“1+1=2”的结论。</p><p>同时读者可以更加顺理成章地理解作者的想法。换言之，作者认为“1+1=2”，那么读者便自然而然地知道“1+2=3”。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;序言的结构&quot;&gt;&lt;a href=&quot;#序言的结构&quot; class=&quot;headerlink&quot; title=&quot;序言的结构&quot;&gt;&lt;/a&gt;序言的结构&lt;/h3&gt;&lt;p&gt;虽然在之前的章节中已经提到过序言的组成部分了，但为了引出接下来要说的内容，我还是想要再重复一遍序言的构成—“背景-冲</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第四章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="序言" scheme="https://yb705.github.io/tags/%E5%BA%8F%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(三)-金字塔构建</title>
    <link href="https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E9%87%91%E5%AD%97%E5%A1%94%E6%9E%84%E5%BB%BA/"/>
    <id>https://yb705.github.io/2021/08/15/%E9%87%91%E5%AD%97%E5%A1%94-%E9%87%91%E5%AD%97%E5%A1%94%E6%9E%84%E5%BB%BA/</id>
    <published>2021-08-15T04:08:51.000Z</published>
    <updated>2021-08-15T04:24:55.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经知道了一篇文章的结构和逻辑顺序,那么在知道这些的情况下,我们就可以开始着手构建金字塔了.金字塔的构建有两种方法—-自上而下和自下而上,接下来我依次介绍两种方法.</p><h2 id="自上而下"><a href="#自上而下" class="headerlink" title="自上而下"></a>自上而下</h2><p>通常来说,自上而下比自下而上要更为容易.因为在写文章之前,我们肯定已经确定好了文章的主题,知道写这篇文章是想表达什么.之后,再根据核心思想去依次添加其它的内容,自上而下依次搭建.</p><p>接下来,说一下构建步骤:</p><blockquote><ol><li>确定文章主题/中心思想</li><li>设想受众的疑问</li><li>用序言表述背景-冲突-疑问-回答</li><li>用”疑问-回答”模式,回答读者问题</li><li>复用”疑问-回答”模式,回答读者的新问题</li></ol></blockquote><h2 id="自下而上"><a href="#自下而上" class="headerlink" title="自下而上"></a>自下而上</h2><p>有的时候,我们并不知道具体要表达什么,只是脑海中会浮现一些有某种关联的关键句.这个时候,我们就可以用自下而上的方法来找出这些关键句之间的联系,进而逆推中心思想,来构建金字塔.</p><p>接下来,说一下构建步骤:</p><blockquote><ol><li>整理关键句</li><li>确定关键句之间的逻辑关系</li><li>将关键句归纳整理成各个部分,逆推每部分的核心思想</li><li>将每部分的和思想总结在一起,逆推整篇文章的中心思想</li><li>写作</li></ol></blockquote><h2 id="初学者注意事项"><a href="#初学者注意事项" class="headerlink" title="初学者注意事项"></a>初学者注意事项</h2><h3 id="写文章时优先考虑自上而下模式"><a href="#写文章时优先考虑自上而下模式" class="headerlink" title="写文章时优先考虑自上而下模式"></a>写文章时优先考虑自上而下模式</h3><p>通常，在写作之前没有人会不知道自己想要表达什么。</p><p>因此，相对于依据没有明显联系的关键句去自下而上地搭建金字塔，我们更擅长通过已知将要去表达的中心思想去自上而下地写文章。</p><h3 id="在写序言的时候，要先阐述背景"><a href="#在写序言的时候，要先阐述背景" class="headerlink" title="在写序言的时候，要先阐述背景"></a>在写序言的时候，要先阐述背景</h3><p>序言的结构顺序是背景-冲突-疑问-解答。之所以要按照这个顺序去搭建序言，便是因为冲突是为了背景服务的。同时背景也可以更好让读者与作者从一开始便站在同一位置，防止读者产生与文章主题不符的疑问。</p><h3 id="序言尽量阐述读者已知的或者认可的内容，且与读者相关连"><a href="#序言尽量阐述读者已知的或者认可的内容，且与读者相关连" class="headerlink" title="序言尽量阐述读者已知的或者认可的内容，且与读者相关连"></a>序言尽量阐述读者已知的或者认可的内容，且与读者相关连</h3><p>当序言中出现读者不了解或者不认可的内容时，他们可能会产生新的疑问，而这个疑问有可能会是作者不希望读者产生的，或者是与文章主题无关的。</p><p>一般来说，读者会对与自己相关连的事情更感兴趣。如果碰到与自己无关的文章，则会表现得兴趣缺缺。</p><h3 id="在写序言的时候可以多花一点时间"><a href="#在写序言的时候可以多花一点时间" class="headerlink" title="在写序言的时候可以多花一点时间"></a>在写序言的时候可以多花一点时间</h3><h3 id="在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好"><a href="#在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好" class="headerlink" title="在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好"></a>在依照逻辑顺序进行表述的时候，归纳推理比演绎推理要更好</h3><p>我们在思考某一件事情的时候，往往是按照演绎推理的顺序去思考的。但是有的时候思考的顺序并不等于表达的顺序。所以为了防止读者思考与文章表述不搭配的情况，继而出现思维混乱，在写文章的时候，归纳推理比演绎推理要更适用一些。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经知道了一篇文章的结构和逻辑顺序,那么在知道这些的情况下,我们就可以开始着手构建金字塔了.金字塔的构建有两种方法—-自上而下和自下而上,接</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第三章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B8%89%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="金字塔的搭建" scheme="https://yb705.github.io/tags/%E9%87%91%E5%AD%97%E5%A1%94%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(二)-金字塔内部的结构</title>
    <link href="https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/"/>
    <id>https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/</id>
    <published>2021-07-26T05:16:00.000Z</published>
    <updated>2021-08-15T04:24:05.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我们在表达或者是写作的时候,往往会根据一个想法联想出许多其他的想法.这时我们不能将所有的想法一股脑地都”丢”出去,如果这样做的话,受众就会被我们”砸懵”了.所以我们在表达之前一定要按照某种逻辑顺序,梳理好自己要表达的东西.</p><h2 id="纵向"><a href="#纵向" class="headerlink" title="纵向"></a>纵向</h2><p>“维度”这个词想必大家都不会陌生.通常,我们所写在纸上或者是键盘敲打出来的文字只有一种方向,也就是从头到尾.所以,我们就可以说文字的维度是一维的.或者说我们所写的文章就是纵向的.</p><p>那么知道了文章的方向,我们便可以用”疑问-回答”的模式去编排我们的文章.接下来我简单说明一下这种模式:</p><p>通常人们会对未知的事物更感兴趣,所以在阅读的时候,往往也会偏好于之前没有了解过的文章.因此,当读者在阅读到一段之前从未接触过的思想/段落时,心里便会有疑问:”为什么”,”怎么回事”……接下来,文章的下一部分便会顺其自然地对给予解释,并提出作者的观点.但在回答了读者的疑问的同时,往往又会引出新的问题,继而引起读者新的疑问:”为什么这么说?”,”接下来还会发生什么?”……就这样周而复始,依次迭代,一个清晰明了的纵向结构就构建了出来,如下图所示:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/image-20210724100115729.png" alt="image-20210724100115729"></p><p>那么”疑问-回答”这种结构有什么好处呢?</p><p>我觉得最大的作用就是将作者的思想清晰准确地表达了出来.也就是说,读者可以不认同作者的某个观点,或者是对某个疑问的解答,但是却可以清楚地知道作者思考的整体过程和逻辑顺序.读者的理解与作者的表达不会产生歧义.</p><p>在搭建”疑问-回答”模式中,还有两点是要注意的:</p><ol><li>在做好回答问题的准备之前,先不要提出问题</li><li>答案要放在问题后面</li></ol><h2 id="横向"><a href="#横向" class="headerlink" title="横向"></a>横向</h2><p>上面我们说了文章的内部结构搭建,接下来我们再来说一下文章的逻辑顺序.通常,一段文字的表达有两种逻辑顺序,一个是<strong>演绎</strong>,一个是<strong>归纳</strong>.这两个是互不兼容的,也就是说一个意思的表达不能既有演绎,又有归纳.接下来,我简单说明一下这两种表达逻辑.</p><h3 id="演绎"><a href="#演绎" class="headerlink" title="演绎"></a>演绎</h3><p>演绎是一种由一般情况推论出特殊情况的逻辑表达,举个例子:</p><blockquote><p>人都会死.</p><p>苏格拉底是人.</p><p>所以苏格拉底会死.</p></blockquote><p>总结下来,就是第一个思想是一个耳熟能详的大众观点或者现象.而第二个思想是针对第一个思想的主语/谓语提出的个体现象.第三个思想就是前两个思想”名正言顺”地推论总结.</p><h3 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h3><p>归纳是一种根据多个观点的某种共性作出推论的逻辑表达,举个例子:</p><blockquote><p>美军进驻伊拉克.</p><p>巴勒斯坦军进驻伊拉克.</p><p>以色列军进驻伊拉克.</p></blockquote><p>可以看到,上面的三句话有某种共性,就是某军进驻伊拉克.那么将共性归纳起来,我们就可以推断出伊拉克<strong>可能</strong>将会发生战争.需要注意的是,推断只是针对于一般情况,不包含特殊情况,也就是说推断出来的事情不会100%发生.</p><h2 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h2><p>上面已经说了,金字塔机构可以使作者与读者不断地进行疑问-回答式对话.但是,除非引发这种对话的话题与读者有相关性,否则很难吸引读者的注意力.保证产生相关性的唯一办法,就是确保对话直接回答了已经存在于读者头脑中的疑问.</p><p>而文章的序言可以通过追溯问题的起源与发展来给出这一问题.需要注意的是问题的起源与发展必然以叙述的形式出现,应当按照典型的叙述模式展开.</p><p>这种典型的讲故事的呈现方式——–<strong>背景,冲突,疑问,回答</strong>———能够保证在引导读者了解你的思维过程之前,确保与作者站在同一个位置上.</p><p>总之,序言以讲故事的形式告诉读者,关于作者正在讨论的主题他已经了解或将要了解的相关信息,从而引起读者的疑问,这个疑问也是整篇文章将要回答的问题.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>了解纵向关系,就可以确定某一层次上的思想必须包含哪些信息(即必须回答读者针对上一层次的思想提出的新疑问).</p><p>了解横向关系,就可以判断你组织在一起的思想是否用符合逻辑的方式表达信息(即时候采用了正确的归纳/演绎论述).</p><p>更重要的是,了解读者最初提出的疑问,将确保你组织和呈现的思想与读者的有关性(即文章中的思想有助于回答读者的问题).</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;我们在表达或者是写作的时候,往往会根据一个想法联想出许多其他的想法.这时我们不能将所有的想法一股脑地都”丢”出去,如果这样做的话,受众就会被我们”砸</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第二章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内部结构" scheme="https://yb705.github.io/tags/%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>金字塔(一)-为什么要用金字塔</title>
    <link href="https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/"/>
    <id>https://yb705.github.io/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/</id>
    <published>2021-07-26T05:15:48.000Z</published>
    <updated>2021-08-15T04:24:18.464Z</updated>
    
    <content type="html"><![CDATA[<h3 id="归类分组"><a href="#归类分组" class="headerlink" title="归类分组"></a>归类分组</h3><h4 id="共性"><a href="#共性" class="headerlink" title="共性"></a>共性</h4><p>在日常生活中,我们经常会或主动或被动地接受信息.而对于这些信息,大脑便会自发的按照某种关系将其归类划分.譬如古希腊人便将夜晚看到的星星,按照彼此之间的距离长短,并参考生活中的图像,划分成12个星座.</p><p>再譬如下面的图形:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713134006230.png" alt="image-20210713134006230"></p><p>大部分人第一眼看到上面图像的时候,便会依据散点之间的距离,自动将散点划分成左右两个图像,而我却并没有设置任何前提条件.而这便是刚才所说的:<strong>人们在接受信息的时候,会自动依据信息的共性,将其归类分组</strong>.</p><h4 id="奇妙数字”7”"><a href="#奇妙数字”7”" class="headerlink" title="奇妙数字”7”"></a>奇妙数字”7”</h4><p>曾经有位科学家通过实验得出来一个结论:在一般情况下,人类大脑能够同时接收到的信息数量是有限的,通常不会超过7条.</p><p>举个例子:我去便利店购物,要买”<strong>橘子,香蕉,苹果,纸巾,牙刷,毛巾,薯片,虾条,干脆面,筷子,碗,勺</strong>“.</p><p>就上面提到的12样东西来说,朋友们第一次看下来可以记住几个?</p><p>如果记住了5-7个,那说明你的记忆力已经达到平均水平了;如果都记住了,那我只能说你很强!!!</p><p>那通过上面的例子我们可以得出一个结论:那就是人类的短期记忆力是有限的.</p><p>但是假如我们将它用金字塔的形式进行归类分组呢?</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141003449.png" alt="image-20210713141003449" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141111222.png" alt="image-20210713141111222" style="zoom:50%;"></p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141308015.png" alt="image-20210713141308015" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141350913.png" alt="image-20210713141350913" style="zoom:50%;"></p><p>好的,那么现在你能记住的多少?<del>反正我都能记住.</del></p><h4 id="逻辑关系"><a href="#逻辑关系" class="headerlink" title="逻辑关系"></a>逻辑关系</h4><p>其实上面的分组过程并不复杂,就是通过物品的共性来寻找一个名词,然后再去划分每样物品的类别,譬如香蕉,橘子和苹果都属于水果.这样一来,我们便将需要记住的12个物品划分成了4类,而我们只需要记住这4类就可以了.然后每一类名字下面有三个从属物品,我们再去以此记忆每类从属.</p><p>这种方法之所以会加强我们的记忆量,主要有三个原因:</p><ol><li>我们的大脑是有联想功能的.依据总结出来的抽象名词,我们自发的联想到与这个名词有关系的共性事物,譬如说看到”水果”这个词,脑海里便会自觉的浮现出经常吃的水果,像是香蕉啊,苹果什么的.这种联想减轻了我们的记忆负担.高层次的思想总会提示低层次的思想.</li><li>需要记忆的东西数量减少了,需要大脑处理的东西由原来的12样变成了4样.</li><li>参考每样事物之间的抽象的逻辑关系,将无序便成了有序.而我们的大脑总是很容易记住有某种顺序的东西(逻辑顺序也是顺序的一种).譬如数字12345比32541要好记的多.</li></ol><h3 id="自上而下-结论先行"><a href="#自上而下-结论先行" class="headerlink" title="自上而下,结论先行"></a>自上而下,结论先行</h3><p>其实人们在表述的时候,一般会先说最重要的东西.而对大多数情况下,结论比过程更重要.所以<strong>理清表达思想的顺序,先总结后具体的表达顺序是十分重要的</strong>.</p><p>鉴于每个人的生活,工作,环境等不同,对于同一样事,每个人的认知是会出现差异的.实际上,人们在理解一段话的时候,一部分的精力用来识别文字本身,另一部分精力便是用来构造每句话之间的逻辑关系.而如果不考虑每个人的文化水平的差异,那么这种认知偏差便是来源于每句话之间的逻辑关系.既然如此,我们为什么不在一开始就将结论或者说逻辑架构提前说清楚呢?</p><p>有的时候,与其说读者容易接收理解作者整体的思维顺序,倒不如说是读者在跟着作者的顺序走,继而去理解作者的思维.</p><h3 id="自下而上-思考分析"><a href="#自下而上-思考分析" class="headerlink" title="自下而上,思考分析"></a>自下而上,思考分析</h3><p>平常,我们在阅读或者是写作的时候,发现文章会按照表达思想的不同划分成不同的段落,而每个段落所表达的思想最后汇总成全篇文章的中心思想,这就是作者要告诉我们的东西,同时整篇文章遵循金字塔结构:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210719133157281.png" alt="image-20210719133157281"></p><p>所谓的思考分析就是这样:</p><p>将表达同一思想的语句放在一起,组成一个段落,生成一个观点.在这个段落里,每句话都为这个观点的成立提供了论述,或者说是证明;</p><p>而每个段落的观点最后汇总在一起,推导/证明全篇文章的中心思想.</p><p>简单来说就是:<strong>自上而下,每一层都是下一层的抽象总结;自下而上,每一层都对上一层的思想提供了支持.</strong></p><p>但是在我们搭建金字塔的过程中要注意三点:</p><blockquote><p>纵向:每一层的思想均是下一层思想的概括总结</p><p>横向:每个组别都要按照正确的分类放在一起</p><p>横向:每个组别/段落之间要有正确的逻辑顺序</p></blockquote><p>接下来,我们分别简单地解释一下.</p><h4 id="1-每一层的思想均是下一层思想的概括总结"><a href="#1-每一层的思想均是下一层思想的概括总结" class="headerlink" title="1.每一层的思想均是下一层思想的概括总结"></a>1.每一层的思想均是下一层思想的概括总结</h4><p>这一点实际上并不需要过多的解释.每个段落依照共性总结出一个思想,不然的话就会南辕北辙,逻辑混乱.</p><p>郭德纲之前说过一个包袱:</p><blockquote><p>某人去水果店买水果,指着苹果问售货员:”这个苹果甜吗?”</p><p>售货员说:”甜.”</p><p>问:”这种苹果产地在哪里?”</p><p>售货员说:”这个苹果是国外进口的,您看上面还有英文呢.”</p><p>问:”那这个苹果多少钱一斤?”</p><p>售货员说:”国外的就贵一点,20块钱一斤.”</p><p>某人说:”我还真没吃过国外的苹果.行吧,那给我拿二斤葡萄.”</p></blockquote><p>很有意思,这就是典型的逻辑混乱.全篇在说苹果的事,但是最后却买了葡萄.</p><h4 id="2-每个组别都要按照正确的分类放在一起"><a href="#2-每个组别都要按照正确的分类放在一起" class="headerlink" title="2.每个组别都要按照正确的分类放在一起"></a>2.每个组别都要按照正确的分类放在一起</h4><p>在阅读/写作的时候,我们为什么要将这两句话放在一个段落里面呢?就是因为这两句话有共性,或者说是表达了同一个意思.譬如说,椅子和桌子都属于家具,牙膏和毛巾都属于洗漱用品,汉堡和薯条都属于肯德基(PS:我想应该没有人去肯德基买皮鞋吧…..).</p><p>那么如何比较两个事物之间的共性呢?就看他们的共性名词大不大.譬如说:牙刷和沐浴露都属于生活用品,但是牙刷属于洗漱用品,沐浴露属于洗浴用品,而生活用品包含洗漱和洗浴用品.这样一来,就可以比较牙刷和沐浴露之间的共性相关了.</p><h4 id="3-每个组别-段落之间要有正确的逻辑顺序"><a href="#3-每个组别-段落之间要有正确的逻辑顺序" class="headerlink" title="3.每个组别/段落之间要有正确的逻辑顺序"></a>3.每个组别/段落之间要有正确的逻辑顺序</h4><p>每个段落或者说段落中的每句话都要按照一定的逻辑顺序去排列.而我们通常所用到的逻辑顺序通常有以下几种:</p><blockquote><p>顺序逻辑:首先,然后,最后,</p><p>时间逻辑:第一天,第二天,昨天,今天,明天</p><p>空间逻辑:在水下,在水上,在沙滩上,在山底,在半山腰,在山顶</p><p>结构逻辑:最重要的,重要的,其次,最差</p><p>因果逻辑:因为,所以,虽然,但是</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;归类分组&quot;&gt;&lt;a href=&quot;#归类分组&quot; class=&quot;headerlink&quot; title=&quot;归类分组&quot;&gt;&lt;/a&gt;归类分组&lt;/h3&gt;&lt;h4 id=&quot;共性&quot;&gt;&lt;a href=&quot;#共性&quot; class=&quot;headerlink&quot; title=&quot;共性&quot;&gt;&lt;/a&gt;共性&lt;/h</summary>
      
    
    
    
    <category term="金字塔思维" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/"/>
    
    <category term="表达的逻辑" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/"/>
    
    <category term="第一章" scheme="https://yb705.github.io/categories/%E9%87%91%E5%AD%97%E5%A1%94%E6%80%9D%E7%BB%B4/%E8%A1%A8%E8%BE%BE%E7%9A%84%E9%80%BB%E8%BE%91/%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
    
    
    <category term="读书笔记" scheme="https://yb705.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="内化笔记" scheme="https://yb705.github.io/tags/%E5%86%85%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="思维结构" scheme="https://yb705.github.io/tags/%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——流形学习（t-SNE）</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/</id>
    <published>2021-07-26T04:50:37.000Z</published>
    <updated>2021-07-26T05:04:22.326Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经说过<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>通常是用于数据变换的首选方法，使人能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。而有一类可用于可视化的算法叫做<strong>流形学习算法</strong>，它允许进行更复杂的映射，通常也可以给出更好的可视化。其中一个特别有用的算法就是<strong>t-SNE算法</strong>。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><strong>流形学习算法主要用于可视化，因此很少用来生成两个以上的新特征</strong>。其中一些算法（包括t-SNE）计算训练数据的一种新表示，但不允许变换新数据。这意味着这些算法不能用于测试集：<strong>准确地说，它们只能用于训练数据</strong>。流形学习对探索性数据分析是很有用的，但如果最终目的是监督学习的话，则很少使用。</p><p>t-SNE背后的思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。t-SNE首先给出每个数据点的随机二维表示，然后尝试让原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些保存表示哪些点比较靠近的信息。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>数据是来自于scikit-learn包含的一个手写数字数据集，在这个数据集中，每个数据点都是0到9之间手写数字的一张8x8的灰度图像，图像如下：<br><del>PS：最近博主很忙，实在是没有时间找新数据来做了。。。</del> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>digits=load_digits()<br>fig,axes=plt.subplots(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>),subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br><span class="hljs-keyword">for</span> ax,img <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(axes.ravel(),digits.images):<br>    ax.imshow(img)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>为了与PCA进行比较，我们先用PCA将降到二维的数据可视化。对前两个主成分作图，并按照类别对数据点着色：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">2</span>)<span class="hljs-comment">###构建一个PCA模型</span><br>pca.fit(digits.data)<span class="hljs-comment">###将digits数据变换到前两个主成分上</span><br>digits_pca=pca.transform(digits.data)<br>colors=[<span class="hljs-string">&quot;#476A2A&quot;</span>,<span class="hljs-string">&quot;#7851B8&quot;</span>,<span class="hljs-string">&quot;#BD3430&quot;</span>,<span class="hljs-string">&quot;#4A2D4E&quot;</span>,<span class="hljs-string">&quot;#875525&quot;</span>,<span class="hljs-string">&quot;#A83683&quot;</span>,<span class="hljs-string">&quot;#4E656E&quot;</span>,<span class="hljs-string">&quot;#853541&quot;</span>,<span class="hljs-string">&quot;#3A3120&quot;</span>,<span class="hljs-string">&quot;#535D8E&quot;</span>]<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>())<br>plt.ylim(digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>())<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_pca[i,<span class="hljs-number">0</span>],digits_pca[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一主成分&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二主成分&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756071762" alt="在这里插入图片描述"></p><p>这里我们将每个类别对应的数字作为符号来显示每个类别的位置。从上图可以看出，除了0，4，6以外，大部分数字都是重叠在一起的。</p><p>接下来我们将t-SNE应用于同一数据集，并对结果进行比较。由于t-SNE不支持变换新数据，所以TSNE类没有transfrom方法。我们可以调用fit_transform方法来代替，它会构建模型并立刻返回变换后的数据，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE<br>tsne=TSNE(random_state=<span class="hljs-number">42</span>)<span class="hljs-comment">###使用fit_transform而不是fit,因为TSNE没有transform方法</span><br>digits_tsne=tsne.fit_transform(digits.data)<span class="hljs-comment">###运行时间较久</span><br></code></pre></td></tr></table></figure><p>接下来我们也将处理过的数据可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br>plt.ylim(digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_tsne[i,<span class="hljs-number">0</span>],digits_tsne[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一分量&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二分量&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756111304" alt="在这里插入图片描述"></p><p>结果自不必多说，与PCA相比，t-SNE的结果非常棒。所有类型都被明确分开。数字1到9被分成几块，但大多数类别都形成一个密集的组。<strong>要记住，这种方法并不知道类别标签：它完全是无监督的。但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开</strong>。</p><p>t-SNE算法有一些调节参数，不过默认参数的效果通常就很好。感兴趣的朋友可以尝试修改perplexity和early_exaggeration，但作用一般很小。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实博主在了解这个算法的时候就在思考，这个算法有什么作用。毕竟说的再好听，也要在现实中有用才行。</p><p>实际上，t-SNE直接用于降维，并后接分类器比较少见。</p><p>当我们意识到需要降维时，一般是发现了特征间的高度线性相关，而t-SNE主打的是非线性降维。如果我们发现了线性相关，可能用PCA处理就可以了。即使发现了“非线性相关性”，我们也不会尝试用t-SNE降维再搭配一个线性分类模型，而会直接选择非线性的分类模型去处理。复杂的非线性关系不适合强行降维再做分类，而应该用非线性模型直接处理。如果是高度稀疏的矩阵，也有适合的分类器直接用，也没必要降维。</p><p>所以想了想，觉得t-SNE应该比较适合可视化，就像上面的图像一样，了解和验证数据或者模型。至于降维的话，还有很多局限性有待解决。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经说过&lt;a href=&quot;https://blog.csdn.net/weixin_43580339/article/details/117</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="流形学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="数据可视化" scheme="https://yb705.github.io/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="图片分类" scheme="https://yb705.github.io/tags/%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>数据拟合实际应用</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</id>
    <published>2021-07-26T04:44:49.000Z</published>
    <updated>2021-08-15T04:08:55.235Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经学习了很多关于监督学习的算法，但是最近博主在看有关于数据分析的书籍的时候，忽然觉得在实际应用中，我们很少会用得到机器学习，数据挖掘方面的东西。我们所需要做的就是得到实际生活中的数据，并找出数据之间的关系，然后再根据这个关系去做一些运营，决策等行为，仅此而已。所以这篇我要说一下关于数据拟合的一些东西。（其实与监督学习的那些算法相比，数据拟合可以说是非常简单了。）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>数据拟合又称曲线拟合</strong>，俗称<strong>拉曲线</strong>，是一种把现有数据透过数学方法来代入一条数式的表示方式。科学和工程问题可以通过诸如采样、实验等方法获得若干离散的数据，根据这些数据，我们往往希望得到一个连续的函数（也就是曲线）或者更加密集的离散方程与已知数据相吻合，这过程就叫做**拟合(fitting)**。</p><p><del>PS：上面的解释是从百度上抄的。</del></p><p>用通俗一点的话来说，就是我们通过某种方法得到了一些未知关系的数据，然后找到一条函数曲线，来表达数据之间的关系，并保证大部分的数据点可以落在曲线上。</p><p>这里我说一下用python来实现多项式拟合数据的方法。网上还有用最小二乘法和高斯算法来进行数据拟合的方法，但是实际工作中并不需要这么复杂的应用，所以这里就不再赘述了，感兴趣的朋友可以自行学习。</p><h2 id="数据拟合"><a href="#数据拟合" class="headerlink" title="数据拟合"></a>数据拟合</h2><p>而我之所以要说多项式拟合数据，最重要的原因便是<strong>数学上可以证明，任意函数都可以表示为多项式形式</strong>。</p><p>接下来，我们将数据拟合成2次多项式，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x=[<span class="hljs-number">10</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>,<span class="hljs-number">70</span>,<span class="hljs-number">80</span>]<span class="hljs-comment">#定义x、y散点坐标</span><br>x=np.array(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;自变量x :\n&#x27;</span>,x)<br>y=np.sin(x)+np.tan(x)<span class="hljs-comment">#三角函数任意加减</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;因变量y :\n&#x27;</span>,y)<br>f=np.polyfit(x,y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用2次多项式拟合，可改变多项式阶数</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式系数:\n&#x27;</span>,f)<br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式表达式:\n&#x27;</span>,p)<br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;函数拟合出来的数值:\n&#x27;</span>,yvals)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>我们将拟合出来的函数用图像表示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760940342" alt="在这里插入图片描述"></p><p>之前我有提到，任意函数都可以表示为多项式形式。而<strong>如果拟合效果不理想，那只能说明多项式的次数不够</strong>，接下来我们试试拟合成7次多项式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">f=np.polyfit(x,y,<span class="hljs-number">7</span>)<br>p=np.poly1d(f)<br>yvals = p(x)<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760973004" alt="在这里插入图片描述"></p><p>可以看出结果较之前好了不少。</p><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>其实我之所以写下这篇文章便是因为最近看书的时候，看到了一个星巴克关于咖啡市场的A/B实验。这个实验是想要验证某富人区，咖啡价格对于新客户转化率的影响，并根据影响执行相应的运营决策。在这个实验中，我们只知道咖啡价格以及该区域门店的客户转化率：</p><blockquote><p>咖啡单价—10,20,30,40,50,60,70,80</p><p>客户转化率—2.30258509, 2.99573227, 3.40119738, 3.68887945, 3.91202301,4.09434456, 4.24849524, 4.38202663</p></blockquote><p>看到这个实验，我的第一反应就是用监督学习里面的逻辑回归去进行拟合建模。可是转念一想，貌似并没有必要搞的这么复杂，我只需要将这两个数据之间的关系用一个多项式函数表示出来就可以了。</p><p>PS：或许这个函数的次数会非常高，系数也会非常复杂，但是那有怎么样，反正也不是人工手算，有什么头痛难解的计算统统丢给电脑就可以了，不然的话我学编程干什么。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">log_y=np.array([<span class="hljs-number">2.30258509</span>, <span class="hljs-number">2.99573227</span>, <span class="hljs-number">3.40119738</span>, <span class="hljs-number">3.68887945</span>, <span class="hljs-number">3.91202301</span>,<span class="hljs-number">4.09434456</span>, <span class="hljs-number">4.24849524</span>, <span class="hljs-number">4.38202663</span>])<br>f=np.polyfit(x,log_y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用6次多项式拟合，可改变多项式阶数</span><br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,log_y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;价格提高&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;客户转化率&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;A/B测试&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272761009236" alt="在这里插入图片描述"></p><p>结果如上所示，可以看出<strong>随着价格的提高，新客户转化率是逐渐提高的（该说真不愧是有钱人），但是转化率的增长速度是逐渐降低的</strong>。</p><p>也就是说，<strong>当咖啡价格提高到一定程度的时候，富人们便不会再倾心于昂贵的咖啡。而星巴克所需要做的便是要找出这个价格的临界值，并在该区域根据这个价格展开一些运营手段。这样做既可以最高限度地提高新客户转化率，同时也能将销售利润最大化。</strong></p><p><del>PS：这就是数据分析的魅力所在。</del></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>数据拟合是用来观察某一自变量归对因变量造成的影响，或者是二者之间的关系，适用于x与y的这种一对一关系:</p><blockquote><p>y=a<em>x^n+bx^(n-1)+…+m</em>x^2+n*x+d</p><p>x-自变量     y-因变量</p></blockquote><p>而在数据挖掘/机器学习中，我们往往探讨多个特征值对目标值的共同作用，单独的去研究每个特征值的影响是非常麻烦的，而且用数据拟合去处理是非常片面的，因为有些特征值之间也会相互作用:</p><blockquote><p>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+…+w[p]x[p]+b </p><p>这里x[0]到x[p]表示耽搁数据点的特征(本例中特征个数为p+1),w和b是学习模型的参数，y是预测结果</p></blockquote><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们已经学习了很多关于监督学习的算法，但是最近博主在看有关于数据分析的书籍的时候，忽然觉得在实际应用中，我们很少会用得到机器学习，数据挖掘方面的</summary>
      
    
    
    
    <category term="数据分析" scheme="https://yb705.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    <category term="数据拟合" scheme="https://yb705.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88/"/>
    
    
    <category term="多项式拟合" scheme="https://yb705.github.io/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88/"/>
    
    <category term="A/B测试" scheme="https://yb705.github.io/tags/A-B%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习——非负矩阵分解（NMF）</title>
    <link href="https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/</id>
    <published>2021-07-26T04:44:32.000Z</published>
    <updated>2021-07-26T05:07:06.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>非负矩阵分解（NMF）是一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>，也可以用于降维。与PCA相同，我们试图将每个数据点写成一些分量的加权求和。但<strong>在PCA中，我们想要的是正负分量，并且能够解释尽可能多的数据方差；而在NMF中，我们希望分量和系数均为负，也就是说，我们希望分量和系数都大于或等于0</strong>。因此，<strong>NMF只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。</strong></p><p>将数据分解成非负加权求和的这个过程，对由多个独立源相加（或叠加）创建而成的数据特别有用，比如多人说话的音轨或包含很多乐器的音乐。在这种情况下，NMF可以识别出组合成数据的原始分量。总的来说，与PCA相比，NMF得到的分量更容易解释，因为负的分量和系数可能会导致难以解释的抵消效应。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>接下来，我们将NMF应用于人脸识别。</p><h2 id="NMF实际应用"><a href="#NMF实际应用" class="headerlink" title="NMF实际应用"></a>NMF实际应用</h2><p><strong>1.数据源</strong></p><p>数据是之前我们已经处理好的人脸图像数据，一共有15个人物，每个人有10张头像。想了解具体处理过程的可以去看一下<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>。</p><p>提数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<span class="hljs-comment">###https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</span><br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>2.建模</strong></p><p>提取完数据集之后，我们划分数据集为训练集和测试集，并用核向量算法SVM来进行建模和评估。</p><p>SVM算法讲解传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">支持向量机（SVM）算法之补充说明</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144125570.png" alt="在这里插入图片描述"></p><p>这样，我们就得到了一个精度为23.6%的模型。</p><p><strong>3.NMF处理</strong></p><p>NMF的主要参数是我们想要提取的分量个数——<strong>n_components</strong>。通常来说，这个数字要小于输入特征的个数（否则的话，将每个像素作为单独的分量就可以对数据进行解释）。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">20</span>,random_state=<span class="hljs-number">0</span>,max_iter=<span class="hljs-number">10000</span>).fit(X_train)<span class="hljs-comment">###增加最大迭代次数，不然会预警</span><br>X_train_nmf=nmf.transform(X_train)<br>X_test_nmf=nmf.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_nmf,y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_nmf)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p>需要注意的是，如果特征值过多的话，NMF默认的迭代次数便会限制模型的精度，并且预警。所以我们还需要设立下NMF的最大迭代次数。最后得到的结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144752949.png" alt="在这里插入图片描述"></p><p>通常来说，提取的特征越多，即n_components越大，模型的精度就越高，但是模型的训练时间也就越长。这里我就不再继续尝试了，感兴趣的朋友们可以试试改变n_components的值来提高模型精度。</p><h2 id="与PCA的比较"><a href="#与PCA的比较" class="headerlink" title="与PCA的比较"></a>与PCA的比较</h2><p>PCA对于数据特征的处理是找到特征重建的最佳方向。而NMF通常并不用于对数据进行重建或者编码。而是寻找用于数据中的有趣的模式。正如我们之前提到的一样，<strong>NMF最适合于具有叠加结构的数据，包括音频，基因表达和文本数据</strong>。接下来我们通过一段模拟信号来与PCA比较一下。</p><p>假设有一段信号，它是由三个不同的信号源组成的，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>S=mglearn.datasets.make_signals()<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">1</span>))<br>plt.plot(S,<span class="hljs-string">&quot;_&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Time&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Signal&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702145638480.png" alt="在这里插入图片描述"></p><p>不幸的是，我们无法观测到原始信号，只能观测到三个信号的叠加混合。而我们的目的便是<strong>将混合信号分解成原信号</strong>。假设我们有100台测量装置，每个测量装置都为我们提供了一系列测量结果。所以接下来我们将数据混合成100维的状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">A=np.random.RandomState(<span class="hljs-number">0</span>).uniform(size=(<span class="hljs-number">100</span>,<span class="hljs-number">3</span>))<span class="hljs-comment">###假设有100台装置测量混合信号</span><br>X=np.dot(S,A.T)<span class="hljs-comment">###将数据混合成100维的状态</span><br></code></pre></td></tr></table></figure><p>接下来，我们分别用NMF和PCA来还原这三个信号：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">3</span>)<br>H=pca.fit_transform(X)<br>nmf=NMF(n_components=<span class="hljs-number">3</span>,max_iter=<span class="hljs-number">10000</span>,random_state=<span class="hljs-number">0</span>)<br>S_=nmf.fit_transform(X)<br></code></pre></td></tr></table></figure><p>最后，我们将结果画出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">models=[X,S,S_,H]<br>names=[<span class="hljs-string">&quot;观测信号&quot;</span>,<span class="hljs-string">&quot;真实信号&quot;</span>,<span class="hljs-string">&quot;非负矩阵（NMF）还原信号&quot;</span>,<span class="hljs-string">&quot;主成分分析（PCA）还原信号&quot;</span>]<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),gridspec_kw=&#123;<span class="hljs-string">&quot;hspace&quot;</span>:<span class="hljs-number">1</span>&#125;,subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br>plt.figure(figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> model,name,ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(models,names,axes):<br>    ax.set_title(name)<br>    ax.plot(model[:,:<span class="hljs-number">3</span>],<span class="hljs-string">&quot;_&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到，NMF在发现原始信号源是得到了不错的成果，而PCA的表现却很差，仅使用第一个成分来解释数据中的大部分变化。</p><p>这里需要注意的是，<strong>NMF生成的分量是没有顺序的</strong>。在上面的例子中，NMF分量的顺序与原始信号完全相同（可以从三条线的颜色看出来），但这纯属偶然。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>除了PCA和NMF之外，还有许多算法可用于将每个数据点分解为一系列固定分量的加权求和。通常描述对分量和系数的约定会涉及到概率论。如果朋友们对这种类型的模式感兴趣，可以去看下scikit-learn中关于独立成分分析（ICA），因子分析（FA）和稀疏编码（字典学习）等。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;非负矩阵分解（NMF）是一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于&lt;a href=&quot;https://blog.csdn.net</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="非负矩阵分解" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    
    
    <category term="分解混合信号" scheme="https://yb705.github.io/tags/%E5%88%86%E8%A7%A3%E6%B7%B7%E5%90%88%E4%BF%A1%E5%8F%B7/"/>
    
    <category term="特征重建" scheme="https://yb705.github.io/tags/%E7%89%B9%E5%BE%81%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析（PCA）应用——特征提取_人脸识别（下）</title>
    <link href="https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2021-06-27T12:39:34.000Z</published>
    <updated>2021-07-26T04:46:02.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在上一篇文章中，我简单说了下利用python对图像进行操作的基础知识，不了解这方面的小伙伴可以去查看下。（传送门——<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>）</p><p>接下来我们来看一下关于人脸识别的模型训练，以及PCA对机器学习流程的优化。</p><p>数据集就是我们在<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>中已经处理完的图像数据，这里就不再赘述了。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>人脸识别的一个常见任务就是看某个前所未见的人脸是否属于数据库中的某个已知人物。这在照片收集，社交媒体和安全应用中都有应用。解决这个问题的方法之一就是构建一个分类器，每个人都是单独的一个类别。但人脸数据库中通常有许多不同的人，而同一个人的图像很少（也就是说，每个类别的训练样例很少）。这使得大多数分类器的训练都很困难。另外，通常你还想要能够轻松添加新的人物，不需要重新训练一个大型模型。</p><p>一个简单的解决方法是使用单一最近临分类器，寻找与你要分类的人脸最为相似的人脸。由于上面我们设定的数据集中，每个人物都有10张图片，所以这个分类器原则上可以处理每个类别只有10个训练样例的情况。</p><p>PS：由于之前有讲过k近邻算法，所以这里就不在赘述了，感兴趣的朋友可以自行查看——<a href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法-分类实操</a>。</p><p>接下来，我们看下<strong>KNeighborsClassifier</strong>的表现如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>knn.fit(X_train,y_train)<span class="hljs-comment">###训练模型</span><br>prediction=knn.predict(X_test)<span class="hljs-comment">###对测试集进行预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test,prediction))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625152141759.png" alt="在这里插入图片描述"></p><p>我们得到的精度为18.4%。对于包含15个类别的分类问题来说，这实际上不算太差（随机猜测的精度约为1/15=6%)，但也不算太好。</p><p>那么接下来我们换成更为复杂的<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM核向量</a>算法，来试一试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160013600.png" alt="在这里插入图片描述"></p><p>可以看出，模型精度较knn近邻分类算法要有所提高，但是训练速度却有所下降。</p><h2 id="PCA特征提取"><a href="#PCA特征提取" class="headerlink" title="PCA特征提取"></a>PCA特征提取</h2><p>这里我们可以用到PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大的不同，使用这种原始表示很难获得面部特征。例如，如果利用像素距离，那么将人脸向右移动一个像素将会发生很大变化，得到一个完全不同的表示。我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用PCA的<strong>白化</strong>选项，它将主成分缩放到相同的尺度。变换后的结果与使用标准化（StandarScaler）相同。</p><p>PS：关于PCA的原理我有在之前讲过，感兴趣的小伙伴可以去看下——<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>那么接下来，我们对训练数据拟合PCA对象，并提取前50个主成分。然后对训练数据和测试数据进行变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">50</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<br>X_train_pca=pca.transform(X_train)<br>X_test_pca=pca.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_pca)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160907371.png" alt="在这里插入图片描述"></p><p>可以看到模型精度反而有所下降，这是因为我们对主成分数量的选择有问题，没有达到最优选取。通常主成分数量不会超过训练集的数据样本数，所以接下来我们通过遍历数字1-112，来挑选最合适的主成分数量:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">components_pca=pd.DataFrame(columns=[<span class="hljs-string">&quot;n_components&quot;</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>,<span class="hljs-string">&quot;knn_score&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">112</span>):<br>    pca=PCA(n_components=i+<span class="hljs-number">1</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<span class="hljs-comment">###拟合训练集</span><br>    X_train_pca=pca.transform(X_train)<span class="hljs-comment">###对训练集进行数据变换</span><br>    X_test_pca=pca.transform(X_test)<span class="hljs-comment">######对测试集进行数据变换</span><br>    clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>    clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>    clf_predict=clf.predict(X_test_pca)<span class="hljs-comment">#预测</span><br>    knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>    knn.fit(X_train_pca,y_train)<br>    knn_predict=knn.predict(X_test_pca)<span class="hljs-comment">###对测试集进行预测</span><br>    components_pca=components_pca.append([&#123;<span class="hljs-string">&quot;n_components&quot;</span>:i+<span class="hljs-number">1</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>:accuracy_score(y_test,clf_predict),<span class="hljs-string">&quot;knn_score&quot;</span>:accuracy_score(y_test,knn_predict)&#125;], ignore_index=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161509683.png" alt="在这里插入图片描述"></p><p>那么接下来，我们再选择能够使得上述模型达到最大精度的主成分数量：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161605292.png" alt="在这里插入图片描述"></p><p>自此，有关于人脸识别的基础流程便演示完了。</p><p>PS：说实话，在之前的监督学习里，完成了那么多次建模，就没有一个模型精度是低于80%的，像上面这么低的模型精度还真有点让人不适应。想要继续提高模型精度的朋友可以进行调参，也可以利用其它算法来建模，或者尝试其它处理数据的方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实，在要求人们评价人脸的相似度时，他们更可能会使用年龄，性别，面部表情和发型等属性，而这些属性很难从像素强度中推断出来。重要的是要记住，算法对数据（特别是视觉数据，比如人们非常熟悉的图像）的解释通常与人类的解释方式大不相同。</p><p>让我们回到PCA的具体案例。我们对PCA变换的介绍是：先旋转数据，然后删除方差较少的成分。另一种有用的解释是尝试找到一些数字（PCA旋转后的新特征值），使我们可以将测试点表示为主成分的加权求和。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;在上一篇文章中，我简单说了下利用python对图像进行操作的基础知识，不了解这方面的小伙伴可以去查看下。（传送门——&lt;a href=&quot;https:/</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="主成分分析" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
    
    <category term="特征提取" scheme="https://yb705.github.io/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
    <category term="人脸识别" scheme="https://yb705.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析（PCA）应用——特征提取_人脸识别（上）</title>
    <link href="https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>https://yb705.github.io/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2021-06-27T12:36:27.000Z</published>
    <updated>2021-07-26T04:42:08.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我在另一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">《无监督学习与主成分分析（PCA）》</a>中已经讲过关于PCA的原理，以及它的其中一个应用——<strong>降维</strong>。那么本篇文章我来说一下PCA的另一个应用——<strong>特征提取</strong>。</p><p>特征提取背后的思想是，可以找到一种数据表示，比给定的原始表示更适合分析。特征提取很有用，它的一个很好的应用实例就是最近几年很火的<strong>人脸（图像）识别</strong>。</p><p>考虑到有很多小伙伴不了解图像的处理，所以我们分成上下两篇来进行讲解。</p><p><strong>本篇先讲解图像的基础以及python通常是如何处理图像的。</strong></p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624154112480.png" alt="在这里插入图片描述"></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。好的，那么接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><h2 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h2><p>我们每拿到一份新数据，一定要对数据进行整理，<strong>了解数据的基本信息</strong>，譬如数据量，如何命名，数据维度等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(all_folds))<br>n=os.listdir(<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\Richard_Gere\\&#x27;</span>)<br><span class="hljs-built_in">print</span>(n[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><p><strong>os模块</strong>是一个python中专门用来遍历文件的第三方模块，具体原理就不在这里赘述了，感兴趣的朋友可以自己搜一下。那么运行上述代码后，我们就可以得知在lfw_funneled文件夹中，一共有5749文件，也就是说一共有5749个人的人脸图像，并且每个人的图像均是以名字+数字的方式来命名的.jpg图像文件。</p><p>那么接下来我们再看下每个人都有多少张人脸图像，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]  <br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624155722308.png" alt="在这里插入图片描述"></p><p>这样一来，我们就知道了每个人有多少张人脸图像，也方便我们接下来进行数据集的选取和划分。</p><p>可以看出数据非常庞大，我们不可能对所有数据进行机器学习（电脑硬件达不到）。同时我们还要降低<strong>数据倾斜</strong>对模型精度的影响，那么我们这里只选取图片数量为10的人脸来当作数据集。</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624160729765.png" alt="在这里插入图片描述"></p><p>PS：如果某人的人脸出现次数过多的话，会造成数据倾斜，大大影响特征提取。</p><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><h3 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a><strong>基础介绍</strong></h3><p>这里先简单说一下什么是图像。图像由<strong>像素</strong>组成，通常存储为<strong>红绿蓝（RGB）强度（三维维度）</strong>。图像中的对象通常由上千个像素组成，它们只有放在一起才有意义。而我们所需要做的便是读取图像，将图像的像素转化为numpy数组，然后再通过操作numpy数组来去处理图像，最后再还原。</p><p>python里面有一个<strong>PIL</strong>的第三方模块，是专门用来处理数据的，如下图所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624162232736.png" alt="在这里插入图片描述"></p><p>一般的像素值是以三维的形式存储的，其中有一个维度是专门用来存储像素颜色的。考虑到接下来的数据处理速度及提高模型精度，我们便剔除颜色维度，用图像的<strong>灰度值</strong>版本来进行处理，代码如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/2021062416343973.png" alt="在这里插入图片描述"></p><h3 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a><strong>图像操作</strong></h3><h4 id="1-图片转换成灰度值"><a href="#1-图片转换成灰度值" class="headerlink" title="1.图片转换成灰度值"></a><strong>1.图片转换成灰度值</strong></h4><p>好的，图像的基础处理方法讲解完了，接下来我们便对选出来的包含150张图片的数据集依次进行处理，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p>将读取的像素信息转化为numpy数组后，分别存储在各自对应的列表里面，并组合成一个字典，方便接下来的使用。接下来简单讲解一下：</p><ol><li>读取的RGB像素值如果直接转化为numpy数组的话会是三维数组，转换为一维数组后是可以用作接下来的机器学习的，但会大大降低训练速度。</li><li>灰度值转化成的numpy数组是一个二维数组，如果直接用于机器学习的话是没有办法读取使用的，所以需要用.ravel()来将二维数组转化为一维数组，也就是将两行的数据强行拉成一行数据。</li><li>如果数据集的标签值（目标值）”target“是文本的话，在接下来的训练部分中，机器便会无法识别，并报错，所以需要转换为数字。并且<strong>为了对应前面特征值的维度（数据维度是150行），这里需要将target也转换成numpy数组。</strong></li><li>这里之所以存储成字典，而不是DataFrame格式，也是因为存储维度的问题。如果存储成DataFrame的话，便需要62500（每张图片的像素数量是250x250）列来存储每个像素，这样的DataFrame太大了，不利于后面的处理，所以这里就以numpy数组的形式存储成字典。</li></ol><p>这里再说一下字典中的“images”，“data”的维度，如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210625150016818.png" alt="在这里插入图片描述"></p><p>图片的像素是以250x250的二维numpy数组的形式存储在”images”中，而为了接下来的机器学习，便将二维数组转换为一维numpy数组存储在“data”中（250x250=62500)。</p><p>PS：可以通过矩阵变换，将原有的一维数组还原成二维灰度值，具体原理就不多说了，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br>vector=np.matrix(flat_arr)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br></code></pre></td></tr></table></figure><h4 id="2-灰度值还原成图片"><a href="#2-灰度值还原成图片" class="headerlink" title="2.灰度值还原成图片"></a><strong>2.灰度值还原成图片</strong></h4><p>接下来，我们可以把灰度值再还原成图片，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>i = <span class="hljs-number">0</span><br>plt.figure(figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>))<br><span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> faces_dict[<span class="hljs-string">&quot;images&quot;</span>]:<br>    <span class="hljs-comment">#总共150张图，把图像分割成15X10</span><br>    plt.subplot(<span class="hljs-number">15</span>,<span class="hljs-number">10</span>,i+<span class="hljs-number">1</span>)<br>    plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>    <span class="hljs-comment">#关闭x，y轴显示</span><br>    plt.xticks([])<br>    plt.yticks([])<br>    plt.xlabel(faces_dict[<span class="hljs-string">&quot;target&quot;</span>][i])<br>    i=i+<span class="hljs-number">1</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/2021062515065469.png" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>好的，关于python处理图像方面的基础便先说道这里。总的来说，便是利用numpy函数来存储组成图像的像素信息，之后通过操作numpy数组来去达到变换图像的目的。</p><p>下一篇，我会讲解关于人脸识别的模型训练，以及PCA对训练过程的优化。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;我在另一篇文章&lt;a href=&quot;https://blog.csdn.net/weixin_43580339/article/details/1179</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="主成分分析" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
    
    <category term="特征提取" scheme="https://yb705.github.io/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
    <category term="人脸识别" scheme="https://yb705.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>天龙八部-段誉篇</title>
    <link href="https://yb705.github.io/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/"/>
    <id>https://yb705.github.io/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/</id>
    <published>2021-06-22T12:34:10.000Z</published>
    <updated>2021-06-27T12:45:40.696Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>“如果在我的小说中选一个角色让我做，我愿做天龙八部中的段誉，他身上没有以势压人的霸道，总给人留有余地。”——金庸</p></blockquote><span id="more"></span><h3 id="生平简介"><a href="#生平简介" class="headerlink" title="生平简介"></a>生平简介</h3><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=874374245,3319416477&fm=26&fmt=auto&gp=0.jpg" alt="img" style="zoom: 45%;">    </div>     <div class="column" style="float:left;width:75%">         在《天龙八部》里面，段誉就属于酱油型男主。虽然书中从头到尾都有他，但他一般都是在旁边看戏。从第一回青衫磊落险峰行，一直到第十回剑气碧烟横，这个内容大概就是说他泡了两个妹子，一个叫钟灵，一个叫木婉清，后来就发现这两个人竟然都是自己同父异母的妹妹。之后,他便被鸠摩智抓到苏州去了，在那里，他碰到了他的一生之敌王语嫣。之后段誉就一直跟着慕容复这些人，在这一路上就是盯着这个王姑娘。虽然人家没怎么理他，但是他还是没有放弃。就这么一直从第十三回追到了第四十五回，终于等到了转机。书中写到西夏公主要招亲，而慕容复为了得到西夏的势力，便也去报名参加。可这样一来，王语嫣就没办法嫁给自己心爱的表哥了。于是一气之下，她就选择自杀，而段誉正好趁虚而入，一下子就翻盘了。    </div></div><h3 id="人物原型"><a href="#人物原型" class="headerlink" title="人物原型"></a>人物原型</h3><p>有人说萧峰是武松的复制，其实段誉也是有原型的。读过《红楼梦》的朋友们应该一眼就能看出来，段誉的原型就是《红楼梦》里面的贾宝玉。这两个人都是属于那种比较叛逆的富家子弟，一个是不想读书当大官，一个是不想学武功当皇帝。但是两个人的喜好却是一样的，那就是都喜欢“妹妹”。</p><p>在《天龙八部》的第二回玉壁月华明中，写到段誉掉进了无量山的琅环福地之后，在里面见到了神仙姐姐的玉像，然后又学会了逍遥派的北冥神功和凌波微步。而在《红楼梦》的第五回里面，就写的是贾宝玉神游太虚幻境，他也是在里面见到了美女，并且也是学到了功夫，只不过一个是打人的功夫，而一个是床上的功夫。在小说的结局中，贾宝玉当了和尚，而段誉最后也是当了和尚。</p><p>（PS：段誉最后传位给了段正兴，也就是《射雕英雄传》里面“南帝”段智兴的父亲。后来段智兴也出家成为了一灯大师，所以大理的皇家好像一直都有这种出家的传统。）</p><h3 id="身世"><a href="#身世" class="headerlink" title="身世"></a>身世</h3><p>其实书中关于段誉身世的描写算是高潮部分了。说是段誉一家都被慕容复跟“恶贯满盈”段延庆给抓了。因为段誉是大理皇帝的继承人，所以慕容复为了实现皇帝梦，就必须先把段誉给杀了，然后再逼段正醇传位于自己。正当他要下手之时，只听得段誉的母亲刀白凤说道：“天龙寺外，菩提树下，花子邋遢，观音长发。”这十六个字说来极轻，但在段延庆听来却如晴天霹雳一般。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=2170625573,283933444&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>当年段延庆在天龙寺外一心求死之时，突然来了一个白衣女子主动与他交合，他觉得这肯定是观音菩萨前来点化他，于是又重拾做人的信心。此时见刀白凤解开发际，万缕青丝披将下来，竟与当年那位天龙寺外菩提树下的观音菩萨别无二样。原来这是刀白凤当年为了报复段正醇的滥情，而作出的一件糊涂事。然后刀白凤又把段誉的生辰八字告诉了他，正好与那天差了十个月。这时段延庆终于反应过来，原来自己才是段誉的亲生父亲。</p><p>段延庆这一生从未有过男女之情、阖家之乐，此时突然冒出来一个亲生儿子，刹时间惊喜交集，心神激荡，只觉世上什么名利富贵，什么帝王之业，都万万比不上这个儿子。虽然后来段誉死活不肯认他，但是他觉得既然段正醇死了，那皇位肯定是传给段誉了。当年段正明抢了自己的皇位，现在皇位又回到了自己的儿子身上，只能说是天道好轮回。于是心花怒放，飘然而去了。</p><p>之后刀白凤就告诉段誉：“你的亲生父亲并不是段正醇，而是那个段延庆，你之前的那些妹妹跟你也没有什么血缘关系，你想娶哪个就娶哪个，最好一块都娶了。”而段誉听后却也不知道是该高兴还是该难受。书中最后段誉便带着他的妹妹们回到大理，继承了皇位。</p><h3 id="感情"><a href="#感情" class="headerlink" title="感情"></a>感情</h3><p>其实我们看到古龙小说中描写的陆小凤，楚留香这样的人，不管他们走到哪里都有女人跟着。这并不说他们缺少女人，只是因为他们对女性有一种欣赏和爱惜的感情。而段誉对王语嫣的感情，其实也是类似于这种感情，或者说是有点幻想性质的感情。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622193647.png" alt="微信截图_20210622193647"></p><p>书里说段誉在看到神仙姐姐的玉像后，瞬间就被迷住了，并且玉像的主人让他磕了一千个头，还要他学武功，他也照做了。要知道这可是连他爹都叫不动的事儿啊，结果却被一个“<strong>等身手办</strong>”给说服了。后来段誉在苏州见到了王语嫣，自此之后他就被迷的神魂颠倒，便是因为王语嫣跟之前的雕像长得实在太像了。</p><p>在老版里面段誉最后就跟王语嫣在一起了。但是在新版里面，金庸先生又在后面加了一大段，说是王语嫣并不知道段誉的身世，她以为段誉也是自己的哥哥。于是王语嫣就给段誉发脾气，说要当尼姑。而段誉经过王语嫣的一顿闹，突然觉得这个王姑娘并不怎么爱自己，只是在无可奈何之下才对自己妥协的。而自己对王姑娘的感情，也可能只是因为自己把她当成了那座玉像而已。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=3839996475,2478899382&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>其实很多人看小说，都觉得里面的王语嫣肯定特别美丽。小说翻拍的电视剧也是请了<strong>刘亦菲</strong>来饰演这个角色。其实这里我们犯的错误跟段誉一样，也是误把王语嫣当成了神仙姐姐。我们都知道金庸先生笔下那些女主都很漂亮，书里面也是喜欢用一大堆的华丽的辞藻来形容她们。比如《神雕侠侣》里面的小龙女，书中说杨过第一次见到小龙女：<u>只觉这少女清丽秀雅，莫可逼视，神色间却冰冷淡漠，当真洁若冰雪，却也是冷若冰雪，实不知她是喜是怒，是愁是乐，竟不自禁的感到恐怖：「这姑娘是水晶做的，还是个雪人儿？到底是人是鬼，还是菩萨仙女？」</u>。还有《书剑恩仇录》里面的香香公主，书里面说是两边的人要打仗了，可看到她之后竟然都放下了兵器。大家要是无法想象的话，可以看一看《还珠格格》里面的香妃，大概就是那个意思。还有就是《鹿鼎记》和《碧血剑》里面写的陈圆圆，那可是历史上的有名的大美女，李闯王为她放弃了江山，吴三桂更是为了她放清兵入关，当了“汉奸”。《碧血剑》中描写说陈圆圆一出现，全场的人都被她迷住了，就连练了混元功的袁承志都差点把持不住。</p><p>但奇怪的是，同样作为女神的王语嫣，书里面对她的描写却都是围绕着那个玉像，总是说她怎么像那个雕像，而不是说她到底有多美。而书中仅有的几句夸赞的话也都是出于段誉之口，旁人见了她无非就是说什么美貌可爱啊、什么娇滴滴的小姑娘啊。但是在段誉眼里，她却是各种仙气，各种女神范。在新修版里面，金庸先生更是明确强调了这一点，就是无论王语嫣长什么样子，都不及段誉自己心中构成的形象。实际上，他真正爱的就是那个想象出来的完美女性，也就是所谓的心魔。</p><p>后来金庸先生为了点醒那些把自己带入成段誉的读者，就又在后面下了重手。书中写到王语嫣想要永葆青春，于是就跟段誉一起去了一个叫做不老长春谷的地方，结果什么也没找着。失望之余，段誉又带她去了当年的琅环玉洞。玉洞里，王语嫣看到那个神仙姐姐的玉像之后，就觉得自己再怎么好看也总有老去的一天，不能像这玉像一般永葆青春。一气之下，她就把玉像给砸了，然后就愤愤而去了。</p><p>可以说上面写的这段，一下子就把王语嫣从神坛上拉了下来，从一个高冷的女生彻底变成了一个神经质的泼妇。所以直到此刻，段誉才真正地看清了这位神仙姐姐的本相。但是很多读者依然沉浸在以前的那个童话故事里，他们还是不肯接受。甚至有些人更是破口大骂，说金庸老糊涂了，把好好的小说改成这样。</p><p>其实看一部作品啊，有时候可以结合一下作者的生平事迹，这样就可以理解书里面的一些奇怪的地方。</p><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/9345d688d43f879436f34c7cdf1b0ef41bd53a57" alt="img" style="zoom: 60%;">    </div>     <div class="column" style="float:left;width:75%">           金庸先生年轻的时候追过一个演员，叫做夏梦，她当时被称为是东方的奥黛丽赫本。当初在金庸追夏梦的时候，夏梦已经结婚了。她就告诉金庸说：虽然你特别有才华，我也是非常欣赏你，但可惜我已经结婚了，所以咱们就算了。但是金庸还是不死心，他就一直追，就跟书里面的段誉是一模一样。而到了最后，他还是没有成功。于是他就把这段恋情写到了他的作品里。不光是《天龙八部》，可以说金庸先生所有作品里的女主都或多或少的有一点夏梦的影子。在2000年的时候，他修改了《天龙八部》的结局，搞了一个新修版。这个时候，他已经七十多岁了。作为一个古稀之年的老人，他对以前的这些事情肯定已经看开了，所以他就把自己的人生感悟写到了书里面。金庸先生觉得自己对夏梦的感情是心魔，那段誉对王语嫣的感情又何尝不是？    </div></div><p>《天龙八部》虽说有点悲剧的色彩，但是在最后，所有人都解脱了。旧版小说里写段誉最后和王语嫣在一起，看起来是有情人终成眷属。其实这两个人并没有真正的解脱，反而更像是一个王子和公主的童话，作者对于两人结局的处理也是显得过于简单和草率。而在新版里面，这两个人才真正地看清了自己，段誉也破了神仙姐姐的心魔，而王语嫣也明白自己爱的还是原来的那个慕容复表哥。所以书里最后写到阿碧和王语嫣一起陪着已经疯了的慕容复，继续做他的皇帝梦。段誉看到这个景象之后就觉得各有各的缘法，我觉得他们可怜，其实他们心中焉知不是心满意足。</p><h3 id="心魔"><a href="#心魔" class="headerlink" title="心魔"></a>心魔</h3><p>我们前面说金庸先生在追夏梦的时候，跟段誉是一样的，也是被心魔所困。其实很多人都会有这种经历，就是有些人追了好几年的所谓的男神女神，等追到手后就发现这个人并没有想象中的那么好，就像是淘宝的买家秀和卖家秀一样。那就是因为在追求的过程中，自己会不自觉地把对方的形象进行魔改，所以最后就造成了一个特别大的心理落差。</p><p>不仅是爱情方面，其实很多问题都是由于这个心魔导致的。比较典型的就是<strong>中国式家长</strong>，父母觉得只要自己本着为孩子好的这个出发点，那自己做的任何荒唐的事情都是对孩子有好处的。最近刚高考完，学生们都在填志愿，很多父母都觉得：哎呀，这个好啊、那个有前途啊，这个工作稳定啊。他们完全不考虑孩子的天赋跟兴趣。还有一些女生的家长，一直就不让孩子在大学里谈恋爱，说这个学校里的男孩子都不靠谱，谈恋爱耽误学习什么的。但是一到毕业，父母又马上要求孩子带个男朋友回家。说实话，挺难为人的。因为这些家长只愿意对自己意淫出来的这个“为孩子好”的心魔去负责。<strong>那么孩子到底好不好呢？</strong></p><h3 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h3><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=211747070,732967533&fm=26&fmt=auto&gp=0.jpg" alt="img" style="zoom: 85%;">    </div>     <div class="column" style="float:left;width:75%">           最后再说一说鸠摩智。其实这个鸠摩智对应的就是天龙八部里面的迦楼罗。迦楼罗是印度神话里的一种半人半鸟的神，以龙为食。而段誉对应的正是天龙八部里面的龙。这个龙不是我们现在说的那个龙，而是一种水中的大毒蛇。古印度人就对他特别的尊敬，因为他是负责降雨的神。而在中国，古人也是经常把皇帝比作真龙天子。而上面的这些特点，基本上都能跟段誉对上。书里说段誉虽然老是被鸠摩智欺负，但最后却又不小心用北冥神功把鸠摩智的内力全部吸走了。传说迦楼罗就是因为吃了很多毒蛇，体内积蓄毒气极多，最后中毒而死，死后留下一颗纯青琉璃心。而书里面也说鸠摩智在被段誉吸干内力后就醒悟了，最后一心投入到佛学工作中去。    </div></div><p>其实《天龙八部》里的萧峰，段誉，虚竹还分别对应了佛教里面的贪、嗔、痴三毒。段誉的问题就跟鸠摩智一样，都是贪。只不过鸠摩智是贪于武功，而段誉则是贪于美色。那为什么说段誉是贪于美色呢？</p><p>最近博主看到的一个故事或许可以形容段誉的行为。说是有一对青年男女搞对象，女生发现男生经常会看一些美女的视频，图片，甚至在大街上走路的时候，眼睛也会不自觉的往其他女生身上瞟。于是女生就很生气，质问男生：“你明明已经有女朋友了，为什么还会看其他的美女？难道是我长得不够好看吗？”男生回答说：“没有，你长得很好看。但是你见过哪个男孩子一辈子只有一件玩具的？”</p><p><strong>我觉得把女生比作玩具是很王八蛋的想法。</strong>但上面的故事倒也从另一方面反映了段誉的感情经历，从一开始的钟灵，木婉清，再到后来的王语嫣，甚至是那种对于王语嫣的“求而不得”的感情。最后，幻想破碎，才发现只有共患难的人才是适合自己的。</p><p><strong>所以说啊, 喜欢和合适不是一回事, 恋爱和结婚也不是一回事。</strong></p><p>小的时候,博主觉得”有情人终成眷属”之所以很难做到,都是现实中的物质原因所导致的,或是金钱,或是工作等等.但现在,我却觉得两个相互喜欢的人能够走到最后真的是太不容易了,因为他们不仅要拥有中彩票的运气来遇见对方,还要<strong>同时</strong>经受住物质与感情的双重考验.</p><p><strong>或许,这就是感情珍贵的地方之一吧.</strong></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;“如果在我的小说中选一个角色让我做，我愿做天龙八部中的段誉，他身上没有以势压人的霸道，总给人留有余地。”——金庸&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="武侠小说" scheme="https://yb705.github.io/categories/%E6%AD%A6%E4%BE%A0%E5%B0%8F%E8%AF%B4/"/>
    
    <category term="天龙八部" scheme="https://yb705.github.io/categories/%E6%AD%A6%E4%BE%A0%E5%B0%8F%E8%AF%B4/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8/"/>
    
    
    <category term="金庸先生" scheme="https://yb705.github.io/tags/%E9%87%91%E5%BA%B8%E5%85%88%E7%94%9F/"/>
    
    <category term="段誉" scheme="https://yb705.github.io/tags/%E6%AE%B5%E8%AA%89/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习与主成分分析</title>
    <link href="https://yb705.github.io/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <id>https://yb705.github.io/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</id>
    <published>2021-06-19T10:14:21.000Z</published>
    <updated>2021-08-15T03:55:14.914Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在之前的文章中，我讲了很多监督学习的算法（线性模型，SVM，决策树，神经网络等），那么接下来，我们要开始接触无监督学习了。首先，我们先说下相关概念。</p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>与监督学习不同，<strong>在无监督学习中，学习算法只有输入数据，并且从数据中提取需要的知识</strong>。而其中有两种常用类型：<strong>数据集变换</strong>和<strong>聚类</strong>。</p><p><strong>无监督变换</strong>是创建数据新的表示的算法，与数据的原始表示相比，新的表示可能更容易被人或其它机器学习算法所理解。而<strong>无监督变换的一个常见应用就是降维</strong>，它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新的方法，<strong>用较少的特征就可以概括数据信息的重要特性</strong>。降维的一个常见应用是将数据降为二维之后进行可视化。（PS：这里的降维和“降维打击”里面的降维是两回事。。。）</p><p><strong>无监督变换的另一个应用是找到”构成“数据的各个部分</strong>。这方面的一个例子就是对文本文档集合进行主题提取。</p><p>而与之相反，<strong>聚类</strong>算法就是将数据划分成不同的组，每组包含相似的物项。譬如说人脸识别，可以将相同的某个人的照片分在一组。</p><p>实际上，<strong>无监督学习的一个主要挑战就是评估算法是否学习到了有用的东西</strong>。因为无监督学习一般用于不包含任何标签信息的数据，所以我们不知道正确的输出应该是什么。因此很难判断一个模型是否”表现良好“。<strong>通常来说，评估无监督算法结果的唯一方法就是人工检查</strong>。</p><p>PS：博主之前在58同城做过微聊审核相关的工作。整体的审核流程大致就是用机器学习建立一个模型，去评估用户有没有说违规的话，之后随机抽取模型的审核结果，再进行人工复审。当复审的错误率达到某个阈值的时候，就需要向技术部门阐明情况，提出修改模型的要求了。</p><p>因此，如果数据科学家想要更好地理解数据，那么无监督算法通常可用于探索性的目的，而不是作为大型自动化系统的一部分（这点与监督学习是不同的）。因此<strong>无监督算法的另一个常见应用就是作为监督算法的预处理步骤</strong>。</p><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p>前面说过，利用无监督学习进行数据变换可能有很多种目的。最常见的就是可视化，压缩数据（降维），以及寻找信息量更大的数据表示以用于进一步的处理。为了实现这些目的，最简单也是最常用的一种算法就是<strong>主成分分析（PCA）</strong>。</p><p>PS：接下来要说理论的东西了，很枯燥，但是希望各位朋友可以耐心的看完，下面的内容对于算法的理解很有帮助。</p><p>主成分分析（PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。</p><p>接下来，我用通俗一点的话来解释下：</p><p>首先，<strong>模型就是数据集所表现出来的信息的集合体或者说构成体</strong>。通常，在机器学习的过程中，特征的个数过多会增加模型的复杂度。而我们所希望的<strong>理想状态就是用最少的特征表示数据集最多的信息</strong>。</p><p>在许多情形下，特征之间是有一定的相关关系的（如线性相关：一个特征可以用另一个特征线性表示）。而当两个特征之间有一定的相关关系时，可以理解为两个特征所反映的此数据集的信息有一定的重叠。（譬如特征x和特征y，其中y=a*x+b）。</p><p>而主成分分析就是对于原先数据集的所有特征进行处理。删去多余的重复的特征，建立尽可能少的特征，使得这些新特征两两不相关。并且这些新特征在反映数据集的信息方面尽可能保持原有信息。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>来自于kaggle的一份关于心脏病患者分类的数据集：<a href="https://www.kaggle.com/ronitf/heart-disease-uci">https://www.kaggle.com/ronitf/heart-disease-uci</a></p><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617134807955.jpg" alt="在这里插入图片描述"></p><p>这份数据集并不大，只包括303份数据样本（该数据不包含空值)。其中数据特征包括患者年龄，性别，心率，血糖量，血压等13个维度以及分类目标-target。而我们所需要做的，就是依照这些特征值来进行建模，从而依照模型的某种趋势来判断患者心脏是否健康。</p><h2 id="PCA实际应用-降维"><a href="#PCA实际应用-降维" class="headerlink" title="PCA实际应用-降维"></a>PCA实际应用-降维</h2><p><strong>1.数据导入</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\heart.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/ronitf/heart-disease-uci</span><br>heart=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.标准化</strong></p><p>由于PCA的数学原理是依照方差最大的方向来去标记主成分，所以我们先对数据进行标准化，使得各个维度的方差均为1.所以我们先用StandardScaler来对数据进行缩放，代码如下：</p><p>（PS：具体实现原理及过程就不再赘述了，不了解的朋友可以看下<a href="https://blog.csdn.net/weixin_43580339/article/details/117774014">python机器学习之数据预处理与缩放</a>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>train=heart.drop([<span class="hljs-string">&quot;target&quot;</span>],axis=<span class="hljs-number">1</span>)<br>standard=StandardScaler()<br>standard.fit(train)<br>X_scaled=standard.transform(train)<br></code></pre></td></tr></table></figure><p>注意别忘了把数据集中的分类目标提取出来（<code>heart.drop([&quot;target&quot;],axis=1)</code>），分类目标是我们的分类目的或者说结果，而这里的标准化是不针对分类目标的。</p><p><strong>3.PCA拟合转换</strong></p><p>学习并应用PCA变换与应用预处理变换一样简单，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">2</span>)<span class="hljs-comment">###为了接下来做图方便一些，就只保留数据的前两个部分，实际上这里也可以多保留些主成分</span><br>pca.fit(X_scaled)<span class="hljs-comment">###对数据进行拟合pca模型</span><br>X_pca=pca.transform(X_scaled)<span class="hljs-comment">#将数据变换到前两个主成分上</span><br></code></pre></td></tr></table></figure><p>为了调整降低数据的维度，我们需要在创建PCA对象时指定想要保留的主成分个数（<strong>n_components=2</strong>）。之后将PCA对象实例化，调用<strong>fit</strong>方法找到主成分，再调用<strong>transform</strong>来旋转并降维。默认情况下，PCA仅变换数据，但保留所有的主成分。</p><p>我们来看下变换前后的数据规模：</p><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/2021061714251032.jpg" alt="在这里插入图片描述"></p><p>可以看出数据由之前的303x13，变成了303x2，样本数量没有变化，特征维度由之前的13个变成了2个。</p><p>在拟合过程中，主成分被保存在components_属性中，其中每一行对应于一个主成分，它们按照重要性排序（第一主成分排在首位，以此类推）。列对应于PCA的原始特征属性。</p><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617144959959.png" alt="在这里插入图片描述"></p><h2 id="PCA实际应用-高维数据可视化"><a href="#PCA实际应用-高维数据可视化" class="headerlink" title="PCA实际应用-高维数据可视化"></a>PCA实际应用-高维数据可视化</h2><p>正如我们之前所说的，PCA的另一个应用就是将高维数据可视化。要知道，对于有两个以上特征的数据，我们是很难绘制散点图的，譬如说上面包含13个维度的心脏患病数据。但是，我们可以用散点图画出PCA变换之后的两个主成分，并着色分类，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>result=np.c_[X_pca,heart[<span class="hljs-string">&quot;target&quot;</span>]]<span class="hljs-comment">###将处理结果和数据集的目标值结合起来，这样就是一个新的数据集了。</span><br><span class="hljs-comment">###新数据集与原数据集的信息相差不大，甚至剔除了部分重叠数据造成的影响</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>result_1=result[result[:,<span class="hljs-number">2</span>]==<span class="hljs-number">1</span>]<span class="hljs-comment">###target类别1</span><br>result_0=result[result[:,<span class="hljs-number">2</span>]==<span class="hljs-number">0</span>]<span class="hljs-comment">###target类别0</span><br>plt.scatter(result_1[:,<span class="hljs-number">0</span>],result_1[:,<span class="hljs-number">1</span>])<span class="hljs-comment">###画类别1的散点图</span><br>plt.scatter(result_0[:,<span class="hljs-number">0</span>],result_0[:,<span class="hljs-number">1</span>])<span class="hljs-comment">###画类别0的散点图</span><br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&#x27;第一主成分&#x27;</span>)<span class="hljs-comment">###横坐标轴标题</span><br>plt.ylabel(<span class="hljs-string">&#x27;第二主成分&#x27;</span>)<span class="hljs-comment">###纵坐标轴标题</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617143301881.png" alt="在这里插入图片描述"></p><p>上面的散点图绘制了第一主成分和第二主成分的关系，然后利用类别信息对数据点进行着色。这让我们相信，即使是线性分类器（在这个空间中学习一条直线)也可以在区分这两个类别时表现的相当不错。</p><p>在这里提醒大家一点，要注意pca是一种无监督的方法，在旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>缺点</strong></p><p>pca有一个缺点，就是通常不容易对图中的两个轴进行解释。虽然主成分是原始特征的组合，但这些组合往往非常复杂，我们可以用热力图表现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">6</span>))<br>plt.matshow(pca.components_)<br>plt.yticks([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-string">&quot;第一主成分&quot;</span>,<span class="hljs-string">&quot;第二主成分&quot;</span>])<span class="hljs-comment">###横坐标轴刻度</span><br>plt.colorbar()<br>plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(train.columns))),train.columns)<span class="hljs-comment">###纵坐标轴刻度</span><br>plt.xlabel(<span class="hljs-string">&#x27;特征值&#x27;</span>)<span class="hljs-comment">###横坐标轴标题</span><br>plt.ylabel(<span class="hljs-string">&#x27;主成分&#x27;</span>)<span class="hljs-comment">###纵坐标轴标题</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617144107125.jpg" alt="在这里插入图片描述"></p><p>PS：横坐标轴是13个特征维度</p><p>上图可以看出每个主成分均是由13个特征值组成的，还展现出了各个特征对于成分构成的重要性（颜色越深越重要），但是我们却无法明确的指出这些特征如何构成主成分的。</p><p><strong>优点</strong></p><p>至于优点，我们从之前的散点图便可以看出来，那就是用简单的模型便可以对数据进行分类。通常来说，SVM核向量算法要比普通的线性模型算法Logistic更为复杂，且模型精度要更高一些。（<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">SVM</a>与<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">Logistic</a>的原理之前有在其他的文章中讲过，感兴趣的朋友可以点击链接去看一下，这里就不多做解释了。）</p><p>那么为了突出刚才提到的优点，接下来我们用svm对没有经过pca变换的数据进行建模：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>X_train,X_test,y_train,y_test=train_test_split(train,heart[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">1</span>)<br>svm=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;没有经过主成分分析的SVM模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test))))<br><span class="hljs-comment">###没有经过主成分分析（pca）的模型评分</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617145818809.png" alt="在这里插入图片描述"></p><p>再利用Logistic算法对变换后的数据进行建模：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>result_pca=pd.DataFrame(result)<span class="hljs-comment">###其中第三列是目标值</span><br>X_train,X_test,y_train,y_test=train_test_split(result_pca.loc[:,<span class="hljs-number">0</span>:<span class="hljs-number">1</span>],result_pca.loc[:,<span class="hljs-number">2</span>],random_state=<span class="hljs-number">1</span>)<br>logistic=LogisticRegression(penalty=<span class="hljs-string">&#x27;l2&#x27;</span>,C=<span class="hljs-number">1</span>,solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,max_iter=<span class="hljs-number">1000</span>)<br>logistic.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;经过主成分分析的Logistic模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,logistic.predict(X_train))))<br><span class="hljs-comment">###经过主成分分析（pca）的模型评分</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617150138257.png" alt="在这里插入图片描述"></p><p>从上面的结果对比便可以看出pca的优点，那就是在最大限度地保留了数据集的信息状态的条件下，它将原本复杂的数据集，转变为了更容易训练的低维度数据级，提高了模型的训练精度。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;在之前的文章中，我讲了很多监督学习的算法（线性模型，SVM，决策树，神经网络等），那么接下来，我们要开始接触无监督学习了。首先，我们先说下相关概念。</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="主成分分析" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
    
    <category term="python" scheme="https://yb705.github.io/tags/python/"/>
    
    <category term="数据处理" scheme="https://yb705.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>决策树集成-随机森林</title>
    <link href="https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/"/>
    <id>https://yb705.github.io/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/</id>
    <published>2021-06-15T12:22:28.000Z</published>
    <updated>2021-08-15T04:03:02.951Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>集成</strong></p><p>集成是合并多个机器学习模型来构建更强大模型的方法。在机器学习算法中有许多模型属于这一类，但已证明有两种集成模型对大量分类和回归的数据集都是有效的，二者都以决策树为基础，分别是<strong>随机森林（random forest）</strong>和<strong>梯度提升决策树（gradiet boosted decision tree）</strong>。</p><p>之前已经讲解过了随机森林(<a href="https://blog.csdn.net/weixin_43580339/article/details/116231286">决策树集成-随机森林之分类实操</a>),这次讲解<strong>梯度提升决策树</strong>。在了解梯度提升决策树之前,建议先去看一下我的另外两篇讲解决策树的文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/115939923">决策树算法之讲解实操（下）</a>，重复的东西，我这里就不在赘述了。</p><h2 id="思想简介"><a href="#思想简介" class="headerlink" title="思想简介"></a>思想简介</h2><p>在之前的一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中我们提到过，决策树的一个主要缺点在于经常对训练数据过拟合。那么除了随机森林之外,梯度提升回归树就是解决这个问题的另一种方法。</p><p>梯度提升回归树是通过合并多个决策树来构建一个更为强大的模型。虽然名字中含有”回归”，但是这个模型既可以用于回归也可以用于分类。与随机森林的方法不同,<strong>梯度提升采用连续的方式构造树,每颗树都试图纠正前一棵树的错误</strong>.默认情况下,梯度提升回归树中没有随机化,而是用到了强预剪枝。<strong>梯度提升树通常使用深度很小(1到5之间)的树</strong>,这样模型占用的内存更少,预测速度也更快.</p><p><strong>梯度提升树背后的主要思想是合并许多简单的模型</strong>(在这个语境中叫做弱学习器),比如深度较小的树.每棵树只能对部分数据作出好的预测,因此,添加的树越来越多,可以不断迭代,提高性能.</p><p>梯度提升树经常是机器学习竞赛的优胜者,并广泛应用于业界.与随机森林相比,它通常对参数的设置更为敏感,但如果参数设置正确的话,模型精度会更高.</p><h2 id="实操建模"><a href="#实操建模" class="headerlink" title="实操建模"></a>实操建模</h2><p>数据是一份<a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒质量分类</a>的数据集，通过各个维度来判断红酒质量，之前在<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中已经讲解使用过了，这里就不多在赘述了，我们直接建模，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-comment">#梯度提升回归树</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\winequality-red.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</span><br>red_wine=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br>train=red_wine.drop([<span class="hljs-string">&quot;quality&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,red_wine[<span class="hljs-string">&quot;quality&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>gbcf=GradientBoostingClassifier(random_state=<span class="hljs-number">1</span>)<br>gbcf.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,gbcf.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,gbcf.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508121801300.png" alt="在这里插入图片描述"></p><p>下面是之前的文章中单棵决策树建立的模型结果：</p><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210428134820111.jpg" alt="在这里插入图片描述"></p><p>二者相比可以看出，梯度提升树的模型精度要比单棵树的要好一点，过拟合现象也比之前要减轻很多。<br>接下来我们了解一下梯度提升树的主要模型参数。</p><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>在梯度提升回归树中，我们主要会用到三个模型参数n_estimators（树的个数），max_depth（树的深度）,learning_rate（学习率）,至于其它的参数，一般情况下直接默认就好。</p><p><strong>max_depth</strong>：用于降低每棵树的复杂度.一般来说,梯度提升模型的max_depth通常设置得很小,一般不超过5.<br>接下来我们来调节这个参数，提高模型精度，代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_1=pd.DataFrame(columns=[<span class="hljs-string">&quot;决策树深度(max_depth)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>):<br>    gbcf=GradientBoostingClassifier(max_depth=i,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_1=result_1.append([&#123;<span class="hljs-string">&quot;决策树深度(max_depth)&quot;</span>:i,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_1[result_1[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_1[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508122322592.jpg" alt="在这里插入图片描述"></p><p>可以看到当我们设定参数max_depth为4的时候，模型精度可以达到66.5%左右，较之前的结果提高了一些。</p><p>梯度提升树模型的另外两个主要参数包括<strong>树的数量n_estimators</strong>和<strong>学习率learn_rate</strong>,后者<strong>用于控制每棵树对前一棵树的错误纠正程度</strong>.这两个参数高度相关,因为learning_rate越低,就需要更多的树来构建具有相似复杂度的模型.随机森林的n_estimators值总是越大越好,但是梯度提升不同,增大n_estimators会导致模型更加复杂,进而可能导致过拟合.<strong>通常的做法是根据时间和内存的预算选择合适的n_estimators,然后对不同的learning_rate进行遍历.</strong></p><p>这两个参数的调节代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_2=pd.DataFrame(columns=[<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">500</span>,<span class="hljs-number">10</span>):<br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=i,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_2=result_2.append([&#123;<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>:i,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_2[result_2[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_2[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/2021050812303492.jpg" alt="在这里插入图片描述"></p><p>n_estimators的调节结果如上图所示,那么接下来我们在上面的参数基础上继续调节学习率:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">result_3=pd.DataFrame(columns=[<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>):<br>    m=i/<span class="hljs-number">10</span><br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=<span class="hljs-number">161</span>,learning_rate=m,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_3=result_3.append([&#123;<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>:m,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_3[result_3[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_3[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123248624.png" alt="在这里插入图片描述"></p><p>接下来,我们还可以对学习率的参数调节进行进一步的区间划分,代码及结果如下所示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">result_4=pd.DataFrame(columns=[<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>):<br>    m=i/<span class="hljs-number">100</span><br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=<span class="hljs-number">161</span>,learning_rate=m,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_4=result_4.append([&#123;<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>:m,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_4[result_4[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_4[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123518243.png" alt="在这里插入图片描述"></p><p>至此,这个模型的参数就调节完毕了.<br>(ps:为了提高模型精度,参数是可以进行更近一步的调节,不过剩下的就需要朋友们自行探索了)</p><h2 id="分析特征重要性"><a href="#分析特征重要性" class="headerlink" title="分析特征重要性"></a>分析特征重要性</h2><p>与随机森林类似，梯度提升树也可以给出特征重要性,代码及结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>%matplotlib inline<br>gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###梯度提升回归树</span><br>forest=RandomForestClassifier(max_depth=<span class="hljs-number">4</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###随机森林分类器</span><br>gbcf_prediction=gbcf.fit(X_train,y_train)<br>forest_prediction=forest.fit(X_train,y_train)<br>fig= plt.subplots(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">15</span>))<br>fig1 = plt.subplot(<span class="hljs-number">211</span>)<br>plt.title(<span class="hljs-string">&#x27;梯度提升回归树特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,gbcf_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.legend()<br>fig2=plt.subplot(<span class="hljs-number">212</span>)<br>plt.title(<span class="hljs-string">&#x27;随机森林特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,forest_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">13</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123904327.jpg" alt="在这里插入图片描述"></p><p>如上图所示，在保证树的深度参数（max_depth）相同的情况下，梯度提升树的特征重要性与随机森林的特征重要性有些相似,实际上在某些数据集中,梯度提升树可能会完全忽略某些特征.</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>梯度提升决策树是监督学习中最强大也是最常用的模型之一.其主要缺点是需要仔细调参,而且训练的时间可能会比较长.与其他基于树的模型类似,这一算法不需要对数据进行缩放就可以表现得很好,而且也<strong>适用于二元特征与连续特征同时存在的数据集</strong>.但它也通常不适用于高维稀疏数据.</p><p>由于梯度提升和随机森林两种方法在类似的数据上表现的都很好,因此一种常用的方法就是先尝试随机森林,它的鲁棒性很好.如果随机森林效果很好,但预测时间太长,或者机器学习模型精度小数点后第二位的提高也很重要,那么切换成梯度提升通常会有用.</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基础概念&quot;&gt;&lt;a href=&quot;#基础概念&quot; class=&quot;headerlink&quot; title=&quot;基础概念&quot;&gt;&lt;/a&gt;基础概念&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;集成&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;集成是合并多个机器学习模型来构建更强大模型的方法。在机器学习算法中有许</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="决策树集成" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90/"/>
    
    
    <category term="python" scheme="https://yb705.github.io/tags/python/"/>
    
    <category term="梯度提升回归树" scheme="https://yb705.github.io/tags/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理与缩放</title>
    <link href="https://yb705.github.io/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/"/>
    <id>https://yb705.github.io/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/</id>
    <published>2021-06-15T12:20:02.000Z</published>
    <updated>2021-08-15T03:56:39.136Z</updated>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们在接触监督学习时了解到，有一些算法（譬如<a href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>）对于数据的缩放非常敏感。因此，通常的做法是对数据集进行调节，使得数据表示更适合于这些算法。通常来说，这是对数据特征的一种简单的缩放和移动。</p><p>机器学习的理论实际上是起源于概率论与数理统计，接下来，我们来简单提几个相关概念，来帮助大家更好地理解接下来的要说的几种处理方法。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>中位数</strong>——对于一组数字来说，中位数指的是这样的数值x：有一半的数值小于x，另一半的数值大于x。如果数据集的数据个数是偶数，就取中间两个数值的平均数。</p><p><strong>四分位数</strong>——按照四分之一的数据个数来划分数据集。较小四分位数指的是这样的数值x：有四分之一的数值小于x。较大四分位数指的是这样的数值x：有四分之一的数值大于x。</p><p><strong>方差</strong>——衡量随机变量或一组数据的离散程度的度量。</p><p><strong>异常值</strong>——在数据集之中的那些与众不同的数据点。</p><p>需要注意的是<strong>异常值并不一定是误差值或者错误值</strong>。譬如说想要统计某辆公交车上20名乘客的平均年龄，有一种情况就是其中19名乘客是处于20岁到30岁之间，但是有一名乘客的年龄是70岁，那么这名乘客的年龄就是属于与众不同的数据点，是异常值，而不是误差值。并且这个数值会影响最终的统计计算结果——拉高公交车上乘客的平均年龄。</p><h2 id="预处理的四种类型"><a href="#预处理的四种类型" class="headerlink" title="预处理的四种类型"></a>预处理的四种类型</h2><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610115711512.jpg" alt="在这里插入图片描述"><br>scikit-learn中一共提供了4种预处理方法，变换效果分别如上图所示，接下来，结合图像，我们来详细说一下这四种变换。<br><del>PS：如果大家不喜欢下面的枯燥理论就直接跳过吧，说实话，我觉得只要会使用，知道用于什么地方就够了。</del> </p><p><strong>StandardScaler（标准化）</strong>：确保每个特征的<strong>平均值</strong>为0，<strong>方差</strong>为1，使特征值都位于同一量级。但这种缩放不能保证特征任何特定的最大值和最小值。（我曾经在讲解<a href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>的时候进行过标准化的人工处理，有源代码，感兴趣的朋友可以去看下，可以帮助大家更好地理解。）</p><p><strong>RobustScaler（剔除异常值）</strong>：RobustScaler也是一种标准化，工作原理与StandardScaler类似，确保每个特征的统计属性都处于同一范围。但是RobustScaler使用的是<strong>中位数</strong>和<strong>四分位数</strong>，而不是平均值和方差。这样RobustScaler会忽略与其它点有很大不同的数据点（<strong>异常值</strong>），减少异常值造成的麻烦。</p><p><strong>MinMaxScaler（归一化）</strong>：MinMaxScaler移动数据，使得所有特征都刚好位于0到1之间。对于二维数据集来说，所有的数据都包含在x轴0到1与y轴0到1组成的矩形中。（同样，我在讲解<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>的时候进行过归一化的人工处理，也有相关源代码。）</p><p><strong>Normalizer（正则化，有些地方也叫做归一化）</strong>：Normalizer用到的是一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于1。通过上面的第四幅小图可以看出：它<strong>将一个数据点投射到半径为1的圆上（对于更高维度的情况是球面）</strong>。这意味着每个数据点的缩放比例都不相同。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。</p><h2 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h2><p>接下来我们用数据集<a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">SVM</a>算法来实际使用下，看看效果。（数据以及相关算法我们都在之前讲解的SVM中详细地讲过了，感兴趣的朋友可以点击超链接去看下，这里就不在赘述了。）</p><p>首先是导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>划分训练集和测试集，并进行建模，精度评分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>svm=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140210590.jpg" alt="在这里插入图片描述"></p><p>接下来我们对数据进行缩放再进行建模评分，看看模型精度有什么变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>standard=StandardScaler()<br>standard.fit(X_train)<span class="hljs-comment">###使用fit方法拟合缩放器，并将其应用于训练数据，其实就和之前学过的算法一样，先用fit去训练数据，适应数据</span><br>X_train_scaled=standard.transform(X_train)<span class="hljs-comment">####对训练数据进行实际缩放，也是类似之前学习的训练过程，在，fit适用数据之后，再用transfrom去同等变换X_train,X_test</span><br>X_test_scaled=standard.transform(X_test)<span class="hljs-comment">###注意测试集相对训练集来说移动必须是一致的，因为变换后数量级是不同的，但是要保证数据的分布形状要完全相同</span><br>svm.fit(X_train_scaled,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;标准化后SVM模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test_scaled))))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140415448.jpg" alt="在这里插入图片描述"></p><p>可以看到模型精度较之前有了提升。</p><p>整个处理过程与之前算法训练模型类似。首先使用<strong>fit</strong>方法拟合缩放器（scaler），并将其应用于训练数据。然后为了应用刚刚学习的数据（即对训练数据进行实际缩放），我们使用缩放器的<strong>transform</strong>方法。最后为了将SVM应用到缩放后的数据上，还需要对测试集进行变换。</p><p>需要注意的是，为了让监督模型能够在测试集上运行，<strong>对训练集和测试集应用完全相同的变换是很重要的</strong>。因为<strong>刻度数值可以不一样，但是必须要保证测试集与训练集的数据分布是一样的（可以结合上面的散点图来看一下）</strong>。</p><h2 id="替代方法"><a href="#替代方法" class="headerlink" title="替代方法"></a>替代方法</h2><p>通常来说，想要在某个数据集上fit一个模型，然后再将其transform，是一个非常常见的过程。但是可以用比先调fit再调transform更高效的方法来计算。对于这种使用场景，所有具有transform方法的模型也都有一个fit_transform方法，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br>standard=StandardScaler()<br>X_train_scaled=standard.fit(X_train).transform(X_train)<span class="hljs-comment">###原方法先fit后transform</span><br>X_train_scaled=standard.fit_transform(X_train)<span class="hljs-comment">###结果相同，但计算更加高效</span><br></code></pre></td></tr></table></figure><p>虽然fit_transform不一定对所有模型都更加高效，但在尝试变换训练集时，使用这一方法仍然是很少的。<br><del>PS：最主要是看起来上了点档次</del></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>对于任何一种类型的数据集或者是一个算法来说，没有绝对正确的预处理算法。<strong>预处理方法，数据集与建模算法这三者之间永远都不会存在绑定关系</strong>。 譬如我们分别用SVM和神经网络来测试上述四种预处理算法，代码及结果如下所示：</p><p><strong>SVM：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:<br>    scaled=i<br>    i.fit(X_train)<br>    X_train_scaled=i.transform(X_train)<br>    X_test_scaled=i.transform(X_test)<br>    svm.fit(X_train_scaled,y_train)<br>    score=accuracy_score(y_test,svm.predict(X_test_scaled))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(i)+<span class="hljs-string">&quot;处理后得分：&quot;</span>+<span class="hljs-built_in">str</span>(score))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014311582.png" alt="在这里插入图片描述"></p><p><strong>神经网络（MLP)：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<span class="hljs-comment">#多层感知机-MLP/神经网络</span><br>mlp=MLPClassifier(solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,random_state=<span class="hljs-number">1</span>,max_iter=<span class="hljs-number">100000</span>).fit(X_train,y_train)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:<br>    scaled=i<br>    i.fit(X_train)<br>    X_train_scaled=i.transform(X_train)<br>    X_test_scaled=i.transform(X_test)<br>    mlp.fit(X_train_scaled,y_train)<br>    score=accuracy_score(y_test,mlp.predict(X_test_scaled))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(i)+<span class="hljs-string">&quot;处理后得分：&quot;</span>+<span class="hljs-built_in">str</span>(score))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014321875.png" alt="在这里插入图片描述"></p><p>可以看出，对于同一个数据集应用不同的算法，选择的处理方法是不一样的。对于SVM来说，用<strong>标准化</strong>处理精度会高一些，而对于神经网络来说，用<strong>归一化</strong>处理效果会更好。而如果我们对其它数据集进行机器学习的话，那么就会存在其它的选择。所以对于一份没有接触过的数据集来说，如果时间允许的话，可以尝试各种各样的组合，来去搭建精度最高的模型。</p><p>最后，虽然数据缩放不涉及任何复杂的数学，但良好的做法仍是使用scikit_learn提供的缩放机制，而不是自己人工实现它们，因为即使在这些简单的计算中也容易犯错。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h2&gt;&lt;p&gt;之前我们在接触监督学习时了解到，有一些算法（譬如&lt;a href=&quot;https://blog.csdn.net/weixin_43580339/art</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="无监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据预处理" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
    
    <category term="python" scheme="https://yb705.github.io/tags/python/"/>
    
    <category term="SVM" scheme="https://yb705.github.io/tags/SVM/"/>
    
    <category term="数据缩放" scheme="https://yb705.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BC%A9%E6%94%BE/"/>
    
  </entry>
  
  <entry>
    <title>最小二乘法</title>
    <link href="https://yb705.github.io/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"/>
    <id>https://yb705.github.io/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</id>
    <published>2021-06-15T12:17:20.000Z</published>
    <updated>2021-08-15T03:51:01.007Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>线性回归模型</strong></p><p>对于不同的数据集，数据挖掘或者说机器学习的过程，就是建立数据模型的过程。对于回归类问题，线性模型预测的一般公式如下：</p><blockquote><p><em><strong>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+……+w[p]x[p]+b</strong></em></p></blockquote><p>这里x[0]到x[p]表示耽搁数据点的特征(本例中特征个数为p+1),w和b是学习模型的参数，y是预测结果，对于单一特征的数据集，公式如下：</p><blockquote><p>*<em>y=w[0]<em>x[0]+b</em></em></p></blockquote><p>大家可以看出来，这个很像高中数学里的直线方程。其中w[0]就是斜率，对于有更多特征的数据集，w包含沿每个特征坐标元素的斜率。或者，你也可以将预测的响应值看作输入特征的加权求和，权重由w的元素给出。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>最小二乘算法</strong></p><p>普通最小二乘算法，或者说线性回归，是回归问题最简单也是最经典的线性方法，线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的<strong>均方误差</strong>最小。<br><strong>均方误差</strong>是预测值与真实值之差的平方和除以样本数。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/sp1nalcord/mycsgo-data">第一人称fps游戏csgo的等分数据：https://www.kaggle.com/sp1nalcord/mycsgo-data</a></p><p><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106144238394.png" alt="在这里插入图片描述"></p><p>csgo是一款第一人称的射击游戏，该数据包括每位玩家的网络延迟（ping），击杀数量，死亡数量，得分情况等等。<br><del>嘿嘿，游戏这方面我还是比较了解的，这也是博主唯一一个不用看原版数据的英文介绍，就能看懂各个维度的数据集了。</del> </p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<span class="hljs-comment">#导入线性回归算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块</p><p><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\avocado.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>glass=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/neuromusic/avocado-prices</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p><p><strong>3.清洗数据</strong></p><p><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106144941434.png" alt="在这里插入图片描述"></p><p>可以看到这个数据并不包括缺失值，而且各个特征值之间也没有属性重叠的状况，所以暂时不需要任何处理。</p><p><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train,X_test,y_train,y_test=train_test_split(csgo[[<span class="hljs-string">&quot;Ping&quot;</span>,<span class="hljs-string">&quot;Kills&quot;</span>,<span class="hljs-string">&quot;Assists&quot;</span>,<span class="hljs-string">&quot;Deaths&quot;</span>,<span class="hljs-string">&quot;MVP&quot;</span>,<span class="hljs-string">&quot;HSP&quot;</span>]],csgo[<span class="hljs-string">&quot;Score&quot;</span>],random_state=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>将score划分为预测值，其它的属性划分为特征值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">LR=LinearRegression()<br>LR.fit(X_train,y_train)<br>prediction=LR.predict(X_test)<br>r2_score(y_test,prediction)<br></code></pre></td></tr></table></figure><p>引入knn算法，进行建模后，对测试集进行精度评分，得到的结果如下：</p><p><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106145315552.png" alt="在这里插入图片描述"></p><p>可以看到，该模型的精度为94%左右。<br>至此，这个数据集的将建模就算是完成了。</p><p><strong>5.总结</strong></p><p>1.可以感受到最小二乘法算法并没有什么难点，但是它确是最经典，最重要的算法之一。因为有很多的其它线性回归算法都是基于它的模型公式推导出来的，所以对于这个模型的公式一定要理解。<br><del>对算法的原理一定要有自己的理解，或许并不会专门从事算法研发的工作，但是对于算法如何去使用，用在什么场景是一定要知道的。</del> </p><p>2.大家可以看到这个算法并不需要调参，因为这个算法并没有参数可言，这是一个优点，但是这也反映了一件事，就是没有办法控制模型的复杂度，也没有办法通过调整算法本身，来去提高模型精度。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;线性回归模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于不同的数据集，数据挖掘或者说机器学习的过程，就是建立数据模</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性模型" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="python" scheme="https://yb705.github.io/tags/python/"/>
    
    <category term="回归模型" scheme="https://yb705.github.io/tags/%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机（SVM）算法-实际应用</title>
    <link href="https://yb705.github.io/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <id>https://yb705.github.io/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</id>
    <published>2021-06-15T12:14:42.000Z</published>
    <updated>2021-08-15T03:53:13.301Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><em><strong>SVM</strong></em></p><p>之前我们用了很多线性算法来做预测模型，像是<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">逻辑算法（LogisticRegression)</a>,<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192">lasso</a>,<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">岭回归</a>。但现实生活中，很多事情不是线性可分的（即画一条直线就能分类的），而SVM就是专治线性不可分，把分类问题转化为平面分类问题。这个算法中，我们将每一个数据项作为一个点，而在n维空间中(其中n是你拥有的特征数)作为一个点，每一个特征值都是一个特定坐标的值。然后，我们通过查找区分这两个类的超平面来进行分类。</p><p>我们用一张图形来说明这一点：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331144958674.png" alt="在这里插入图片描述"></p><p>在实际使用过程中，我们并不需要了解确定最佳分类平面的原理，只需要记住一个经验法则来确定正确的超平面:“选择能更好地隔离两个类的超平面”。在这个场景中，蓝色超平面“A”出色地完成了这项工作。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类：https://www.kaggle.com/andrewmvd/fetal-health-classification</a></p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150227435.jpg" alt="在这里插入图片描述"></p><p>该数据包含胎儿心电图，胎动，子宫收缩等特征值，而我们所需要做的就是通过这些特征值来对胎儿的健康状况(fetal_health)进行分类。</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150323351.jpg" alt="在这里插入图片描述"></p><p>数据集包含从心电图检查中提取的2126条特征记录，然后由三名产科专家将其分为3类，并用数字来代表：1-普通的，2-疑似病理，3-确定病理。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#划分数据集与测试集</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<span class="hljs-comment">#导入算法模块</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<span class="hljs-comment">#导入评分模块</span><br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150702818.jpg" alt="在这里插入图片描述"></p><p>从上面的结果来看，数据中没有缺失值。</p><p><strong>3.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">svm_linear=svm.SVC(C=<span class="hljs-number">10</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<span class="hljs-comment">#参数部分会在下面进行讲解</span><br>svm_linear.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,svm_linear.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm_linear.predict(X_test))))<br></code></pre></td></tr></table></figure><p>引入SVM算法，并将算法中的参数依次设立好，进行建模后，对测试集进行精度评分，得到的结果如下：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331145934753.png" alt="在这里插入图片描述"></p><p>可以看到，该模型的精度为89%左右。</p><p><strong>4.参数</strong></p><p>在这里我们只讲解几个重要的参数，对于其它的参数，朋友们可以自行探究。</p><blockquote><p>sklearn.svm.SVC(C,kernel,degree,gamma,coef0,shrinking,probability,tol,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state)</p></blockquote><p>1.<strong>C</strong> ：惩罚参数，通常默认为1。 C越大，表明越不允许分类出错，但是C越大可能会造成过拟合，泛化效果太低。C越小，正则化越强，分类将不会关注分类是否正确的问题，只要求间隔越大越好，此时分类也没有意义。所以，这其中需要朋友们做一些调整。</p><p>2.<strong>kernel（核函数）</strong> ：核函数的引入是为了解决线性不可分的问题，将分类点映射到高维空间中以后，转化为可线性分割的问题。</p><p>我们用一张表来说明核函数的几个参数：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331152357255.jpg" alt="在这里插入图片描述"></p><p>这其中最常用的核函数是<strong>Linear核</strong>与<strong>RBF核</strong>：</p><p>（1）<strong>Linear核</strong>：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。</p><p>（2）<strong>RBF核</strong>：主要用于线性不可分的情形，相比其它线性算法来说这也是一个非常突出的一个优点了。无论是小样本还是大样本，高维还是低维，RBF核函数均适用。</p><p>接下来用一段分类边界函数，就能很明显地展现出这两者的区别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br>X, y = make_moons(<span class="hljs-number">200</span>, noise=<span class="hljs-number">0.20</span>)<br>plt.scatter(X[:,<span class="hljs-number">0</span>], X[:,<span class="hljs-number">1</span>], s=<span class="hljs-number">40</span>, c=y, cmap=plt.cm.Spectral)<br>plt.show()<span class="hljs-comment"># 手动生成一个随机的平面点分布，并画出来</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_boundary</span>(<span class="hljs-params">pred_func</span>):</span><br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.5</span><br>    y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.5</span><br>    h = <span class="hljs-number">0.01</span><br>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br>    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])<br>    Z = Z.reshape(xx.shape)<br>    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)<br>    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Spectral)<br></code></pre></td></tr></table></figure><p>手动设定一个随机的平面分布点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br>clf = svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovo&quot;</span>)<br>clf.fit(X, y)<br>plot_decision_boundary(<span class="hljs-keyword">lambda</span> x: clf.predict(x))<br>plt.title(<span class="hljs-string">&quot;SVM-Linearly&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>咱们先来瞄一眼线性核分类对于它的分类效果：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331153225903.png" alt="在这里插入图片描述"></p><p>接下来看看rbf核对它进行非线性分类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">clf = svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovo&quot;</span>)<br>clf.fit(X, y)<br>plot_decision_boundary(<span class="hljs-keyword">lambda</span> x: clf.predict(x))<br>plt.title(<span class="hljs-string">&quot;SVM-Nonlinearity&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331153412296.png" alt="在这里插入图片描述"></p><p>从上面两个图可以很轻松的看出来linear核与rbf核的区别，在上面的两个结果中，很明显rbf核函数的模型精度更好一些。但是在运行过程中，却可以感觉到选用rbf核函数比linear核函数运行速度要稍慢一些，并且随着数据量的增大，运行时间是会一直增加的。</p><p>3.<strong>decision_function_shape</strong>:原始的svm只是用于二分类的问题，如果将其扩展到多分类问题，就要采取一定的融合策略，<strong>‘ovo’一对一</strong>，就是两两之间进行划分，<strong>‘ovr’一对多</strong>就是一类与其他类别进行划分。<br>这里重点讲一下<strong>一对多</strong>:对每个类别都学习一个二分类模型，将这个类别与其它类别都尽量分开，这样就生成了与类别个数一样多的二分类模型。在测试点上运行所有二分类分类器来进行预测。在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>SVM是一种二分类模型，处理的数据可以分为三类：</p><p>1.线性可分，通过硬间隔最大化，学习线性分类器，在平面上对应直线<br>2.近似线性可分，通过软间隔最大化，学习线性分类器<br>3.线性不可分，通过核函数以及软间隔最大化，学习非线性分类器，在平面上对应曲线</p><p>而svm比线性模型要优秀的一个点就是可以处理非线性分类数据，甚至是更高维的数据，例如这样的数据分类：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/2021033116142616.jpg" alt="在这里插入图片描述"></p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;算法简介&quot;&gt;&lt;a href=&quot;#算法简介&quot; class=&quot;headerlink&quot; title=&quot;算法简介&quot;&gt;&lt;/a&gt;算法简介&lt;/h2&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;SVM&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;之前我们用了很多线性算法来做预测模型，像是&lt;a h</summary>
      
    
    
    
    <category term="机器学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="监督学习" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="核向量机" scheme="https://yb705.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E6%A0%B8%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
    <category term="python" scheme="https://yb705.github.io/tags/python/"/>
    
    <category term="SVM" scheme="https://yb705.github.io/tags/SVM/"/>
    
    <category term="实际应用" scheme="https://yb705.github.io/tags/%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    
  </entry>
  
</feed>
