<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>金字塔-内部结构</title>
    <link href="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/"/>
    <url>/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我们在表达或者是写作的时候,往往会根据一个想法联想出许多其他的想法.这时我们不能将所有的想法一股脑地都”丢”出去,如果这样做的话,受众就会被我们”砸懵”了.所以我们在表达之前一定要按照某种逻辑顺序,梳理好自己要表达的东西.</p><h2 id="纵向"><a href="#纵向" class="headerlink" title="纵向"></a>纵向</h2><p>“维度”这个词想必大家都不会陌生.通常,我们所写在纸上或者是键盘敲打出来的文字只有一种方向,也就是从头到尾.所以,我们就可以说文字的维度是一维的.或者说我们所写的文章就是纵向的.</p><p>那么知道了文章的方向,我们便可以用”疑问-回答”的模式去编排我们的文章.接下来我简单说明一下这种模式:</p><p>通常人们会对未知的事物更感兴趣,所以在阅读的时候,往往也会偏好于之前没有了解过的文章.因此,当读者在阅读到一段之前从未接触过的思想/段落时,心里便会有疑问:”为什么”,”怎么回事”……接下来,文章的下一部分便会顺其自然地对给予解释,并提出作者的观点.但在回答了读者的疑问的同时,往往又会引出新的问题,继而引起读者新的疑问:”为什么这么说?”,”接下来还会发生什么?”……就这样周而复始,依次迭代,一个清晰明了的纵向结构就构建了出来,如下图所示:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84/image-20210724100115729.png" alt="image-20210724100115729"></p><p>那么”疑问-回答”这种结构有什么好处呢?</p><p>我觉得最大的作用就是将作者的思想清晰准确地表达了出来.也就是说,读者可以不认同作者的某个观点,或者是对某个疑问的解答,但是却可以清楚地知道作者思考的整体过程和逻辑顺序.读者的理解与作者的表达不会产生歧义.</p><p>在搭建”疑问-回答”模式中,还有两点是要注意的:</p><ol><li>在做好回答问题的准备之前,先不要提出问题</li><li>答案要放在问题后面</li></ol><h2 id="横向"><a href="#横向" class="headerlink" title="横向"></a>横向</h2><p>上面我们说了文章的内部结构搭建,接下来我们再来说一下文章的逻辑顺序.通常,一段文字的表达有两种逻辑顺序,一个是<strong>演绎</strong>,一个是<strong>归纳</strong>.这两个是互不兼容的,也就是说一个意思的表达不能既有演绎,又有归纳.接下来,我简单说明一下这两种表达逻辑.</p><h3 id="演绎"><a href="#演绎" class="headerlink" title="演绎"></a>演绎</h3><p>演绎是一种由一般情况推论出特殊情况的逻辑表达,举个例子:</p><blockquote><p>人都会死.</p><p>苏格拉底是人.</p><p>所以苏格拉底会死.</p></blockquote><p>总结下来,就是第一个思想是一个耳熟能详的大众观点或者现象.而第二个思想是针对第一个思想的主语/谓语提出的个体现象.第三个思想就是前两个思想”名正言顺”地推论总结.</p><h3 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h3><p>归纳是一种根据多个观点的某种共性作出推论的逻辑表达,举个例子:</p><blockquote><p>美军进驻伊拉克.</p><p>巴勒斯坦军进驻伊拉克.</p><p>以色列军进驻伊拉克.</p></blockquote><p>可以看到,上面的三句话有某种共性,就是某军进驻伊拉克.那么将共性归纳起来,我们就可以推断出伊拉克<strong>可能</strong>将会发生战争.需要注意的是,推断只是针对于一般情况,不包含特殊情况,也就是说推断出来的事情不会100%发生.</p><h2 id="序言的结构"><a href="#序言的结构" class="headerlink" title="序言的结构"></a>序言的结构</h2><p>上面已经说了,金字塔机构可以使作者与读者不断地进行疑问-回答式对话.但是,除非引发这种对话的话题与读者有相关性,否则很难吸引读者的注意力.保证产生相关性的唯一办法,就是确保对话直接回答了已经存在于读者头脑中的疑问.</p><p>而文章的序言可以通过追溯问题的起源与发展来给出这一问题.需要注意的是问题的起源与发展必然以叙述的形式出现,应当按照典型的叙述模式展开.</p><p>这种典型的讲故事的呈现方式——–<strong>背景,冲突,疑问,回答</strong>———能够保证在引导读者了解你的思维过程之前,确保与作者站在同一个位置上.</p><p>总之,序言以讲故事的形式告诉读者,关于作者正在讨论的主题他已经了解或将要了解的相关信息,从而引起读者的疑问,这个疑问也是整篇文章将要回答的问题.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>了解纵向关系,就可以确定某一层次上的思想必须包含哪些信息(即必须回答读者针对上一层次的思想提出的新疑问).</p><p>了解横向关系,就可以判断你组织在一起的思想是否用符合逻辑的方式表达信息(即时候采用了正确的归纳/演绎论述).</p><p>更重要的是,了解读者最初提出的疑问,将确保你组织和呈现的思想与读者的有关性(即文章中的思想有助于回答读者的问题).</p>]]></content>
    
    
    <categories>
      
      <category>金字塔思维</category>
      
      <category>表达的逻辑</category>
      
      <category>第二章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内部笔记</tag>
      
      <tag>读书笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>金字塔-思维结构</title>
    <link href="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/"/>
    <url>/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h3 id="归类分组"><a href="#归类分组" class="headerlink" title="归类分组"></a>归类分组</h3><h4 id="共性"><a href="#共性" class="headerlink" title="共性"></a>共性</h4><p>在日常生活中,我们经常会或主动或被动地接受信息.而对于这些信息,大脑便会自发的按照某种关系将其归类划分.譬如古希腊人便将夜晚看到的星星,按照彼此之间的距离长短,并参考生活中的图像,划分成12个星座.</p><p>再譬如下面的图形:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713134006230.png" alt="image-20210713134006230"></p><p>大部分人第一眼看到上面图像的时候,便会依据散点之间的距离,自动将散点划分成左右两个图像,而我却并没有设置任何前提条件.而这便是刚才所说的:<strong>人们在接受信息的时候,会自动依据信息的共性,将其归类分组</strong>.</p><h4 id="奇妙数字”7”"><a href="#奇妙数字”7”" class="headerlink" title="奇妙数字”7”"></a>奇妙数字”7”</h4><p>曾经有位科学家通过实验得出来一个结论:在一般情况下,人类大脑能够同时接收到的信息数量是有限的,通常不会超过7条.</p><p>举个例子:我去便利店购物,要买”<strong>橘子,香蕉,苹果,纸巾,牙刷,毛巾,薯片,虾条,干脆面,筷子,碗,勺</strong>“.</p><p>就上面提到的12样东西来说,朋友们第一次看下来可以记住几个?</p><p>如果记住了5-7个,那说明你的记忆力已经达到平均水平了;如果都记住了,那我只能说你很强!!!</p><p>那通过上面的例子我们可以得出一个结论:那就是人类的短期记忆力是有限的.</p><p>但是假如我们将它用金字塔的形式进行归类分组呢?</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141003449.png" alt="image-20210713141003449" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141111222.png" alt="image-20210713141111222" style="zoom:50%;"></p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141308015.png" alt="image-20210713141308015" style="zoom:50%;"><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210713141350913.png" alt="image-20210713141350913" style="zoom:50%;"></p><p>好的,那么现在你能记住的多少?<del>反正我都能记住.</del></p><h4 id="逻辑关系"><a href="#逻辑关系" class="headerlink" title="逻辑关系"></a>逻辑关系</h4><p>其实上面的分组过程并不复杂,就是通过物品的共性来寻找一个名词,然后再去划分每样物品的类别,譬如香蕉,橘子和苹果都属于水果.这样一来,我们便将需要记住的12个物品划分成了4类,而我们只需要记住这4类就可以了.然后每一类名字下面有三个从属物品,我们再去以此记忆每类从属.</p><p>这种方法之所以会加强我们的记忆量,主要有三个原因:</p><ol><li>我们的大脑是有联想功能的.依据总结出来的抽象名词,我们自发的联想到与这个名词有关系的共性事物,譬如说看到”水果”这个词,脑海里便会自觉的浮现出经常吃的水果,像是香蕉啊,苹果什么的.这种联想减轻了我们的记忆负担.高层次的思想总会提示低层次的思想.</li><li>需要记忆的东西数量减少了,需要大脑处理的东西由原来的12样变成了4样.</li><li>参考每样事物之间的抽象的逻辑关系,将无序便成了有序.而我们的大脑总是很容易记住有某种顺序的东西(逻辑顺序也是顺序的一种).譬如数字12345比32541要好记的多.</li></ol><h3 id="自上而下-结论先行"><a href="#自上而下-结论先行" class="headerlink" title="自上而下,结论先行"></a>自上而下,结论先行</h3><p>其实人们在表述的时候,一般会先说最重要的东西.而对大多数情况下,结论比过程更重要.所以<strong>理清表达思想的顺序,先总结后具体的表达顺序是十分重要的</strong>.</p><p>鉴于每个人的生活,工作,环境等不同,对于同一样事,每个人的认知是会出现差异的.实际上,人们在理解一段话的时候,一部分的精力用来识别文字本身,另一部分精力便是用来构造每句话之间的逻辑关系.而如果不考虑每个人的文化水平的差异,那么这种认知偏差便是来源于每句话之间的逻辑关系.既然如此,我们为什么不在一开始就将结论或者说逻辑架构提前说清楚呢?</p><p>有的时候,与其说读者容易接收理解作者整体的思维顺序,倒不如说是读者在跟着作者的顺序走,继而去理解作者的思维.</p><h3 id="自下而上-思考分析"><a href="#自下而上-思考分析" class="headerlink" title="自下而上,思考分析"></a>自下而上,思考分析</h3><p>平常,我们在阅读或者是写作的时候,发现文章会按照表达思想的不同划分成不同的段落,而每个段落所表达的思想最后汇总成全篇文章的中心思想,这就是作者要告诉我们的东西,同时整篇文章遵循金字塔结构:</p><p><img src="/2021/07/26/%E9%87%91%E5%AD%97%E5%A1%94-%E6%80%9D%E7%BB%B4%E7%BB%93%E6%9E%84/image-20210719133157281.png" alt="image-20210719133157281"></p><p>所谓的思考分析就是这样:</p><p>将表达同一思想的语句放在一起,组成一个段落,生成一个观点.在这个段落里,每句话都为这个观点的成立提供了论述,或者说是证明;</p><p>而每个段落的观点最后汇总在一起,推导/证明全篇文章的中心思想.</p><p>简单来说就是:<strong>自上而下,每一层都是下一层的抽象总结;自下而上,每一层都对上一层的思想提供了支持.</strong></p><p>但是在我们搭建金字塔的过程中要注意三点:</p><blockquote><p>纵向:每一层的思想均是下一层思想的概括总结</p><p>横向:每个组别都要按照正确的分类放在一起</p><p>横向:每个组别/段落之间要有正确的逻辑顺序</p></blockquote><p>接下来,我们分别简单地解释一下.</p><h4 id="1-每一层的思想均是下一层思想的概括总结"><a href="#1-每一层的思想均是下一层思想的概括总结" class="headerlink" title="1.每一层的思想均是下一层思想的概括总结"></a>1.每一层的思想均是下一层思想的概括总结</h4><p>这一点实际上并不需要过多的解释.每个段落依照共性总结出一个思想,不然的话就会南辕北辙,逻辑混乱.</p><p>郭德纲之前说过一个包袱:</p><blockquote><p>某人去水果店买水果,指着苹果问售货员:”这个苹果甜吗?”</p><p>售货员说:”甜.”</p><p>问:”这种苹果产地在哪里?”</p><p>售货员说:”这个苹果是国外进口的,您看上面还有英文呢.”</p><p>问:”那这个苹果多少钱一斤?”</p><p>售货员说:”国外的就贵一点,20块钱一斤.”</p><p>某人说:”我还真没吃过国外的苹果.行吧,那给我拿二斤葡萄.”</p></blockquote><p>很有意思,这就是典型的逻辑混乱.全篇在说苹果的事,但是最后却买了葡萄.</p><h4 id="2-每个组别都要按照正确的分类放在一起"><a href="#2-每个组别都要按照正确的分类放在一起" class="headerlink" title="2.每个组别都要按照正确的分类放在一起"></a>2.每个组别都要按照正确的分类放在一起</h4><p>在阅读/写作的时候,我们为什么要将这两句话放在一个段落里面呢?就是因为这两句话有共性,或者说是表达了同一个意思.譬如说,椅子和桌子都属于家具,牙膏和毛巾都属于洗漱用品,汉堡和薯条都属于肯德基(PS:我想应该没有人去肯德基买皮鞋吧…..).</p><p>那么如何比较两个事物之间的共性呢?就看他们的共性名词大不大.譬如说:牙刷和沐浴露都属于生活用品,但是牙刷属于洗漱用品,沐浴露属于洗浴用品,而生活用品包含洗漱和洗浴用品.这样一来,就可以比较牙刷和沐浴露之间的共性相关了.</p><h4 id="3-每个组别-段落之间要有正确的逻辑顺序"><a href="#3-每个组别-段落之间要有正确的逻辑顺序" class="headerlink" title="3.每个组别/段落之间要有正确的逻辑顺序"></a>3.每个组别/段落之间要有正确的逻辑顺序</h4><p>每个段落或者说段落中的每句话都要按照一定的逻辑顺序去排列.而我们通常所用到的逻辑顺序通常有以下几种:</p><blockquote><p>顺序逻辑:首先,然后,最后,</p><p>时间逻辑:第一天,第二天,昨天,今天,明天</p><p>空间逻辑:在水下,在水上,在沙滩上,在山底,在半山腰,在山顶</p><p>结构逻辑:最重要的,重要的,其次,最差</p><p>因果逻辑:因为,所以,虽然,但是</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>金字塔思维</category>
      
      <category>表达的逻辑</category>
      
      <category>第一章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>读书笔记</tag>
      
      <tag>内化笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无监督学习——流形学习（t-SNE）</title>
    <link href="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/"/>
    <url>/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经说过<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>通常是用于数据变换的首选方法，使人能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。而有一类可用于可视化的算法叫做<strong>流形学习算法</strong>，它允许进行更复杂的映射，通常也可以给出更好的可视化。其中一个特别有用的算法就是<strong>t-SNE算法</strong>。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p><strong>流形学习算法主要用于可视化，因此很少用来生成两个以上的新特征</strong>。其中一些算法（包括t-SNE）计算训练数据的一种新表示，但不允许变换新数据。这意味着这些算法不能用于测试集：<strong>准确地说，它们只能用于训练数据</strong>。流形学习对探索性数据分析是很有用的，但如果最终目的是监督学习的话，则很少使用。</p><p>t-SNE背后的思想是找到数据的一个二维表示，尽可能地保持数据点之间的距离。t-SNE首先给出每个数据点的随机二维表示，然后尝试让原始特征空间中距离较近的点更加靠近，原始特征空间中相距较远的点更加远离。t-SNE重点关注距离较近的点，而不是保持距离较远的点之间的距离。换句话说，它试图保存那些保存表示哪些点比较靠近的信息。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>数据是来自于scikit-learn包含的一个手写数字数据集，在这个数据集中，每个数据点都是0到9之间手写数字的一张8x8的灰度图像，图像如下：<br><del>PS：最近博主很忙，实在是没有时间找新数据来做了。。。</del> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_digits<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>digits=load_digits()<br>fig,axes=plt.subplots(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>),subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br><span class="hljs-keyword">for</span> ax,img <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(axes.ravel(),digits.images):<br>    ax.imshow(img)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>为了与PCA进行比较，我们先用PCA将降到二维的数据可视化。对前两个主成分作图，并按照类别对数据点着色：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">2</span>)<span class="hljs-comment">###构建一个PCA模型</span><br>pca.fit(digits.data)<span class="hljs-comment">###将digits数据变换到前两个主成分上</span><br>digits_pca=pca.transform(digits.data)<br>colors=[<span class="hljs-string">&quot;#476A2A&quot;</span>,<span class="hljs-string">&quot;#7851B8&quot;</span>,<span class="hljs-string">&quot;#BD3430&quot;</span>,<span class="hljs-string">&quot;#4A2D4E&quot;</span>,<span class="hljs-string">&quot;#875525&quot;</span>,<span class="hljs-string">&quot;#A83683&quot;</span>,<span class="hljs-string">&quot;#4E656E&quot;</span>,<span class="hljs-string">&quot;#853541&quot;</span>,<span class="hljs-string">&quot;#3A3120&quot;</span>,<span class="hljs-string">&quot;#535D8E&quot;</span>]<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>())<br>plt.ylim(digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_pca[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>())<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_pca[i,<span class="hljs-number">0</span>],digits_pca[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一主成分&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二主成分&quot;</span>)<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756071762" alt="在这里插入图片描述"></p><p>这里我们将每个类别对应的数字作为符号来显示每个类别的位置。从上图可以看出，除了0，4，6以外，大部分数字都是重叠在一起的。</p><p>接下来我们将t-SNE应用于同一数据集，并对结果进行比较。由于t-SNE不支持变换新数据，所以TSNE类没有transfrom方法。我们可以调用fit_transform方法来代替，它会构建模型并立刻返回变换后的数据，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE<br>tsne=TSNE(random_state=<span class="hljs-number">42</span>)<span class="hljs-comment">###使用fit_transform而不是fit,因为TSNE没有transform方法</span><br>digits_tsne=tsne.fit_transform(digits.data)<span class="hljs-comment">###运行时间较久</span><br></code></pre></td></tr></table></figure><p>接下来我们也将处理过的数据可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))<br>plt.xlim(digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br>plt.ylim(digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(),digits_tsne[:,<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">1</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(digits.data)):<span class="hljs-comment">###将数据绘制成文本散点</span><br>    plt.text(digits_tsne[i,<span class="hljs-number">0</span>],digits_tsne[i,<span class="hljs-number">1</span>],<span class="hljs-built_in">str</span>(digits.target[i]),color=colors[digits.target[i]],fontdict=&#123;<span class="hljs-string">&quot;weight&quot;</span>:<span class="hljs-string">&quot;bold&quot;</span>,<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">9</span>&#125;)<br>plt.xlabel(<span class="hljs-string">&quot;第一分量&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;第二分量&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88t-SNE%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272756111304" alt="在这里插入图片描述"></p><p>结果自不必多说，与PCA相比，t-SNE的结果非常棒。所有类型都被明确分开。数字1到9被分成几块，但大多数类别都形成一个密集的组。<strong>要记住，这种方法并不知道类别标签：它完全是无监督的。但它能够找到数据的一种二维表示，仅根据原始空间中数据点之间的靠近程度就能够将各个类别明确分开</strong>。</p><p>t-SNE算法有一些调节参数，不过默认参数的效果通常就很好。感兴趣的朋友可以尝试修改perplexity和early_exaggeration，但作用一般很小。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实博主在了解这个算法的时候就在思考，这个算法有什么作用。毕竟说的再好听，也要在现实中有用才行。</p><p>实际上，t-SNE直接用于降维，并后接分类器比较少见。</p><p>当我们意识到需要降维时，一般是发现了特征间的高度线性相关，而t-SNE主打的是非线性降维。如果我们发现了线性相关，可能用PCA处理就可以了。即使发现了“非线性相关性”，我们也不会尝试用t-SNE降维再搭配一个线性分类模型，而会直接选择非线性的分类模型去处理。复杂的非线性关系不适合强行降维再做分类，而应该用非线性模型直接处理。如果是高度稀疏的矩阵，也有适合的分类器直接用，也没必要降维。</p><p>所以想了想，觉得t-SNE应该比较适合可视化，就像上面的图像一样，了解和验证数据或者模型。至于降维的话，还有很多局限性有待解决。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>流形学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据可视化</tag>
      
      <tag>图片分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据拟合实际应用</title>
    <link href="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <url>/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们已经学习了很多关于监督学习的算法，但是最近博主在看有关于数据分析的书籍的时候，忽然觉得在实际应用中，我们很少会用得到机器学习，数据挖掘方面的东西。我们所需要做的就是得到实际生活中的数据，并找出数据之间的关系，然后再根据这个关系去做一些运营，决策等行为，仅此而已。所以这篇我要说一下关于数据拟合的一些东西。（其实与监督学习的那些算法相比，数据拟合可以说是非常简单了。）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>数据拟合又称曲线拟合</strong>，俗称<strong>拉曲线</strong>，是一种把现有数据透过数学方法来代入一条数式的表示方式。科学和工程问题可以通过诸如采样、实验等方法获得若干离散的数据，根据这些数据，我们往往希望得到一个连续的函数（也就是曲线）或者更加密集的离散方程与已知数据相吻合，这过程就叫做**拟合(fitting)**。</p><p><del>PS：上面的解释是从百度上抄的。</del></p><p>用通俗一点的话来说，就是我们通过某种方法得到了一些未知关系的数据，然后找到一条函数曲线，来表达数据之间的关系，并保证大部分的数据点可以落在曲线上。</p><p>这里我说一下用python来实现多项式拟合数据的方法。网上还有用最小二乘法和高斯算法来进行数据拟合的方法，但是实际工作中并不需要这么复杂的应用，所以这里就不再赘述了，感兴趣的朋友可以自行学习。</p><h2 id="数据拟合"><a href="#数据拟合" class="headerlink" title="数据拟合"></a>数据拟合</h2><p>而我之所以要说多项式拟合数据，最重要的原因便是<strong>数学上可以证明，任意函数都可以表示为多项式形式</strong>。</p><p>接下来，我们将数据拟合成2次多项式，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>x=[<span class="hljs-number">10</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">40</span>,<span class="hljs-number">50</span>,<span class="hljs-number">60</span>,<span class="hljs-number">70</span>,<span class="hljs-number">80</span>]<span class="hljs-comment">#定义x、y散点坐标</span><br>x=np.array(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;自变量x :\n&#x27;</span>,x)<br>y=np.sin(x)+np.tan(x)<span class="hljs-comment">#三角函数任意加减</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;因变量y :\n&#x27;</span>,y)<br>f=np.polyfit(x,y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用2次多项式拟合，可改变多项式阶数</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式系数:\n&#x27;</span>,f)<br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;多项式表达式:\n&#x27;</span>,p)<br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;函数拟合出来的数值:\n&#x27;</span>,yvals)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>我们将拟合出来的函数用图像表示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760940342" alt="在这里插入图片描述"></p><p>之前我有提到，任意函数都可以表示为多项式形式。而<strong>如果拟合效果不理想，那只能说明多项式的次数不够</strong>，接下来我们试试拟合成7次多项式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">f=np.polyfit(x,y,<span class="hljs-number">7</span>)<br>p=np.poly1d(f)<br>yvals = p(x)<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;数据拟合&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272760973004" alt="在这里插入图片描述"></p><p>可以看出结果较之前好了不少。</p><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>其实我之所以写下这篇文章便是因为最近看书的时候，看到了一个星巴克关于咖啡市场的A/B实验。这个实验是想要验证某富人区，咖啡价格对于新客户转化率的影响，并根据影响执行相应的运营决策。在这个实验中，我们只知道咖啡价格以及该区域门店的客户转化率：</p><blockquote><p>咖啡单价—10,20,30,40,50,60,70,80</p><p>客户转化率—2.30258509, 2.99573227, 3.40119738, 3.68887945, 3.91202301,4.09434456, 4.24849524, 4.38202663</p></blockquote><p>看到这个实验，我的第一反应就是用监督学习里面的逻辑回归去进行拟合建模。可是转念一想，貌似并没有必要搞的这么复杂，我只需要将这两个数据之间的关系用一个多项式函数表示出来就可以了。</p><p>PS：或许这个函数的次数会非常高，系数也会非常复杂，但是那有怎么样，反正也不是人工手算，有什么头痛难解的计算统统丢给电脑就可以了，不然的话我学编程干什么。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">log_y=np.array([<span class="hljs-number">2.30258509</span>, <span class="hljs-number">2.99573227</span>, <span class="hljs-number">3.40119738</span>, <span class="hljs-number">3.68887945</span>, <span class="hljs-number">3.91202301</span>,<span class="hljs-number">4.09434456</span>, <span class="hljs-number">4.24849524</span>, <span class="hljs-number">4.38202663</span>])<br>f=np.polyfit(x,log_y,<span class="hljs-number">2</span>)<span class="hljs-comment"># 用6次多项式拟合，可改变多项式阶数</span><br>p=np.poly1d(f)<span class="hljs-comment">#得到多项式系数，按照阶数从高到低排列</span><br><span class="hljs-comment">#也可使用yvals=np.polyval(f1, x)</span><br>yvals = p(x) <span class="hljs-comment">#求对应x的各项拟合函数值</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">8</span>))<br>plot1=plt.plot(x,log_y,<span class="hljs-string">&#x27;s&#x27;</span>,label=<span class="hljs-string">&#x27;原对应值&#x27;</span>)<br>plot2=plt.plot(x,yvals,<span class="hljs-string">&#x27;r&#x27;</span>,label=<span class="hljs-string">&#x27;拟合函数&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;价格提高&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;客户转化率&#x27;</span>)<br>plt.legend(loc=<span class="hljs-number">4</span>) <span class="hljs-comment">#指定legend的位置右下角</span><br>plt.title(<span class="hljs-string">&#x27;A/B测试&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70-16272761009236" alt="在这里插入图片描述"></p><p>结果如上所示，可以看出<strong>随着价格的提高，新客户转化率是逐渐提高的（该说真不愧是有钱人），但是转化率的增长速度是逐渐降低的</strong>。</p><p>也就是说，<strong>当咖啡价格提高到一定程度的时候，富人们便不会再倾心于昂贵的咖啡。而星巴克所需要做的便是要找出这个价格的临界值，并在该区域根据这个价格展开一些运营手段。这样做既可以最高限度地提高新客户转化率，同时也能将销售利润最大化。</strong></p><p><del>PS：这就是数据分析的魅力所在。</del></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>数据拟合是用来观察某一自变量归对因变量造成的影响，或者是二者之间的关系，适用于x与y的这种一对一关系:</p><blockquote><p>y=a<em>x^n+bx^(n-1)+…+m</em>x^2+n*x+d</p><p>x-自变量     y-因变量</p></blockquote><p>而在数据挖掘/机器学习中，我们往往探讨多个特征值对目标值的共同作用，单独的去研究每个特征值的影响是非常麻烦的，而且用数据拟合去处理是非常片面的，因为有些特征值之间也会相互作用:</p><blockquote><p>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+…+w[p]x[p]+b </p><p>这里x[0]到x[p]表示耽搁数据点的特征(本例中特征个数为p+1),w和b是学习模型的参数，y是预测结果</p></blockquote><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>数据分析</category>
      
      <category>数据拟合</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多项式拟合</tag>
      
      <tag>A/B测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无监督学习——非负矩阵分解（NMF）</title>
    <link href="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/"/>
    <url>/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>非负矩阵分解（NMF）是一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">PCA</a>，也可以用于降维。与PCA相同，我们试图将每个数据点写成一些分量的加权求和。但<strong>在PCA中，我们想要的是正负分量，并且能够解释尽可能多的数据方差；而在NMF中，我们希望分量和系数均为负，也就是说，我们希望分量和系数都大于或等于0</strong>。因此，<strong>NMF只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。</strong></p><p>将数据分解成非负加权求和的这个过程，对由多个独立源相加（或叠加）创建而成的数据特别有用，比如多人说话的音轨或包含很多乐器的音乐。在这种情况下，NMF可以识别出组合成数据的原始分量。总的来说，与PCA相比，NMF得到的分量更容易解释，因为负的分量和系数可能会导致难以解释的抵消效应。</p><p>PCA原理传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>接下来，我们将NMF应用于人脸识别。</p><h2 id="NMF实际应用"><a href="#NMF实际应用" class="headerlink" title="NMF实际应用"></a>NMF实际应用</h2><p><strong>1.数据源</strong></p><p>数据是之前我们已经处理好的人脸图像数据，一共有15个人物，每个人有10张头像。想了解具体处理过程的可以去看一下<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>。</p><p>提数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<span class="hljs-comment">###https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</span><br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]   <br>img_10=numbers_img[numbers_img[<span class="hljs-string">&quot;图片数量&quot;</span>]==<span class="hljs-number">10</span>].reset_index()<span class="hljs-comment">#####为了降低数据偏斜，选取图片数量为10的文件（否则，特征提取会被图片数量过多的数据影响）</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p><strong>2.建模</strong></p><p>提取完数据集之后，我们划分数据集为训练集和测试集，并用核向量算法SVM来进行建模和评估。</p><p>SVM算法讲解传送门：<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">支持向量机（SVM）算法之补充说明</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144125570.png" alt="在这里插入图片描述"></p><p>这样，我们就得到了一个精度为23.6%的模型。</p><p><strong>3.NMF处理</strong></p><p>NMF的主要参数是我们想要提取的分量个数——<strong>n_components</strong>。通常来说，这个数字要小于输入特征的个数（否则的话，将每个像素作为单独的分量就可以对数据进行解释）。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> NMF<br>nmf=NMF(n_components=<span class="hljs-number">20</span>,random_state=<span class="hljs-number">0</span>,max_iter=<span class="hljs-number">10000</span>).fit(X_train)<span class="hljs-comment">###增加最大迭代次数，不然会预警</span><br>X_train_nmf=nmf.transform(X_train)<br>X_test_nmf=nmf.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_nmf,y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_nmf)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p>需要注意的是，如果特征值过多的话，NMF默认的迭代次数便会限制模型的精度，并且预警。所以我们还需要设立下NMF的最大迭代次数。最后得到的结果如下：</p><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702144752949.png" alt="在这里插入图片描述"></p><p>通常来说，提取的特征越多，即n_components越大，模型的精度就越高，但是模型的训练时间也就越长。这里我就不再继续尝试了，感兴趣的朋友们可以试试改变n_components的值来提高模型精度。</p><h2 id="与PCA的比较"><a href="#与PCA的比较" class="headerlink" title="与PCA的比较"></a>与PCA的比较</h2><p>PCA对于数据特征的处理是找到特征重建的最佳方向。而NMF通常并不用于对数据进行重建或者编码。而是寻找用于数据中的有趣的模式。正如我们之前提到的一样，<strong>NMF最适合于具有叠加结构的数据，包括音频，基因表达和文本数据</strong>。接下来我们通过一段模拟信号来与PCA比较一下。</p><p>假设有一段信号，它是由三个不同的信号源组成的，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mglearn<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>S=mglearn.datasets.make_signals()<br>plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">1</span>))<br>plt.plot(S,<span class="hljs-string">&quot;_&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;Time&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;Signal&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/20210702145638480.png" alt="在这里插入图片描述"></p><p>不幸的是，我们无法观测到原始信号，只能观测到三个信号的叠加混合。而我们的目的便是<strong>将混合信号分解成原信号</strong>。假设我们有100台测量装置，每个测量装置都为我们提供了一系列测量结果。所以接下来我们将数据混合成100维的状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">A=np.random.RandomState(<span class="hljs-number">0</span>).uniform(size=(<span class="hljs-number">100</span>,<span class="hljs-number">3</span>))<span class="hljs-comment">###假设有100台装置测量混合信号</span><br>X=np.dot(S,A.T)<span class="hljs-comment">###将数据混合成100维的状态</span><br></code></pre></td></tr></table></figure><p>接下来，我们分别用NMF和PCA来还原这三个信号：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">3</span>)<br>H=pca.fit_transform(X)<br>nmf=NMF(n_components=<span class="hljs-number">3</span>,max_iter=<span class="hljs-number">10000</span>,random_state=<span class="hljs-number">0</span>)<br>S_=nmf.fit_transform(X)<br></code></pre></td></tr></table></figure><p>最后，我们将结果画出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">models=[X,S,S_,H]<br>names=[<span class="hljs-string">&quot;观测信号&quot;</span>,<span class="hljs-string">&quot;真实信号&quot;</span>,<span class="hljs-string">&quot;非负矩阵（NMF）还原信号&quot;</span>,<span class="hljs-string">&quot;主成分分析（PCA）还原信号&quot;</span>]<br>fig,axes=plt.subplots(<span class="hljs-number">4</span>,figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),gridspec_kw=&#123;<span class="hljs-string">&quot;hspace&quot;</span>:<span class="hljs-number">1</span>&#125;,subplot_kw=&#123;<span class="hljs-string">&quot;xticks&quot;</span>:(),<span class="hljs-string">&quot;yticks&quot;</span>:()&#125;)<br>plt.figure(figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> model,name,ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(models,names,axes):<br>    ax.set_title(name)<br>    ax.plot(model[:,:<span class="hljs-number">3</span>],<span class="hljs-string">&quot;_&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/07/26/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%EF%BC%88NMF%EF%BC%89/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzU4MDMzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>可以看到，NMF在发现原始信号源是得到了不错的成果，而PCA的表现却很差，仅使用第一个成分来解释数据中的大部分变化。</p><p>这里需要注意的是，<strong>NMF生成的分量是没有顺序的</strong>。在上面的例子中，NMF分量的顺序与原始信号完全相同（可以从三条线的颜色看出来），但这纯属偶然。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>除了PCA和NMF之外，还有许多算法可用于将每个数据点分解为一系列固定分量的加权求和。通常描述对分量和系数的约定会涉及到概率论。如果朋友们对这种类型的模式感兴趣，可以去看下scikit-learn中关于独立成分分析（ICA），因子分析（FA）和稀疏编码（字典学习）等。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>非负矩阵分解</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分解混合信号</tag>
      
      <tag>特征重建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>主成分分析（PCA）应用——特征提取_人脸识别（下）</title>
    <link href="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <url>/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在上一篇文章中，我简单说了下利用python对图像进行操作的基础知识，不了解这方面的小伙伴可以去查看下。（传送门——<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>）</p><p>接下来我们来看一下关于人脸识别的模型训练，以及PCA对机器学习流程的优化。</p><p>数据集就是我们在<a href="https://blog.csdn.net/weixin_43580339/article/details/118222281">主成分分析（PCA）应用——特征提取_人脸识别（上）</a>中已经处理完的图像数据，这里就不再赘述了。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>人脸识别的一个常见任务就是看某个前所未见的人脸是否属于数据库中的某个已知人物。这在照片收集，社交媒体和安全应用中都有应用。解决这个问题的方法之一就是构建一个分类器，每个人都是单独的一个类别。但人脸数据库中通常有许多不同的人，而同一个人的图像很少（也就是说，每个类别的训练样例很少）。这使得大多数分类器的训练都很困难。另外，通常你还想要能够轻松添加新的人物，不需要重新训练一个大型模型。</p><p>一个简单的解决方法是使用单一最近临分类器，寻找与你要分类的人脸最为相似的人脸。由于上面我们设定的数据集中，每个人物都有10张图片，所以这个分类器原则上可以处理每个类别只有10个训练样例的情况。</p><p>PS：由于之前有讲过k近邻算法，所以这里就不在赘述了，感兴趣的朋友可以自行查看——<a href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法-分类实操</a>。</p><p>接下来，我们看下<strong>KNeighborsClassifier</strong>的表现如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=faces_dict[<span class="hljs-string">&quot;data&quot;</span>]/<span class="hljs-number">255</span><br>X_train,X_test,y_train,y_test=train_test_split(train,faces_dict[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">0</span>)<span class="hljs-comment">###划分训练集和测试集</span><br>knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>knn.fit(X_train,y_train)<span class="hljs-comment">###训练模型</span><br>prediction=knn.predict(X_test)<span class="hljs-comment">###对测试集进行预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test,prediction))<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625152141759.png" alt="在这里插入图片描述"></p><p>我们得到的精度为18.4%。对于包含15个类别的分类问题来说，这实际上不算太差（随机猜测的精度约为1/15=6%)，但也不算太好。</p><p>那么接下来我们换成更为复杂的<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM核向量</a>算法，来试一试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>clf = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train, y_train)<span class="hljs-comment">#训练</span><br>y_predict = clf.predict(X_test)<span class="hljs-comment">#预测</span><br><span class="hljs-built_in">print</span>(accuracy_score(y_test, y_predict))<span class="hljs-comment">#评分</span><br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160013600.png" alt="在这里插入图片描述"></p><p>可以看出，模型精度较knn近邻分类算法要有所提高，但是训练速度却有所下降。</p><h2 id="PCA特征提取"><a href="#PCA特征提取" class="headerlink" title="PCA特征提取"></a>PCA特征提取</h2><p>这里我们可以用到PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大的不同，使用这种原始表示很难获得面部特征。例如，如果利用像素距离，那么将人脸向右移动一个像素将会发生很大变化，得到一个完全不同的表示。我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用PCA的<strong>白化</strong>选项，它将主成分缩放到相同的尺度。变换后的结果与使用标准化（StandarScaler）相同。</p><p>PS：关于PCA的原理我有在之前讲过，感兴趣的小伙伴可以去看下——<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">无监督学习与主成分分析（PCA）</a></p><p>那么接下来，我们对训练数据拟合PCA对象，并提取前50个主成分。然后对训练数据和测试数据进行变换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">50</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<br>X_train_pca=pca.transform(X_train)<br>X_test_pca=pca.transform(X_test)<br>clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>clf_predict=clf.predict(X_test_pca)<br><span class="hljs-built_in">print</span>(accuracy_score(y_test,clf_predict))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625160907371.png" alt="在这里插入图片描述"></p><p>可以看到模型精度反而有所下降，这是因为我们对主成分数量的选择有问题，没有达到最优选取。通常主成分数量不会超过训练集的数据样本数，所以接下来我们通过遍历数字1-112，来挑选最合适的主成分数量:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">components_pca=pd.DataFrame(columns=[<span class="hljs-string">&quot;n_components&quot;</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>,<span class="hljs-string">&quot;knn_score&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">112</span>):<br>    pca=PCA(n_components=i+<span class="hljs-number">1</span>,whiten=<span class="hljs-literal">True</span>,random_state=<span class="hljs-number">0</span>).fit(X_train)<span class="hljs-comment">###拟合训练集</span><br>    X_train_pca=pca.transform(X_train)<span class="hljs-comment">###对训练集进行数据变换</span><br>    X_test_pca=pca.transform(X_test)<span class="hljs-comment">######对测试集进行数据变换</span><br>    clf=SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>,random_state=<span class="hljs-number">0</span>)<br>    clf.fit(X_train_pca, y_train)<span class="hljs-comment">#训练</span><br>    clf_predict=clf.predict(X_test_pca)<span class="hljs-comment">#预测</span><br>    knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">10</span>)<span class="hljs-comment">###构建邻居值为1的knn分类器</span><br>    knn.fit(X_train_pca,y_train)<br>    knn_predict=knn.predict(X_test_pca)<span class="hljs-comment">###对测试集进行预测</span><br>    components_pca=components_pca.append([&#123;<span class="hljs-string">&quot;n_components&quot;</span>:i+<span class="hljs-number">1</span>,<span class="hljs-string">&quot;SVC_score&quot;</span>:accuracy_score(y_test,clf_predict),<span class="hljs-string">&quot;knn_score&quot;</span>:accuracy_score(y_test,knn_predict)&#125;], ignore_index=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161509683.png" alt="在这里插入图片描述"></p><p>那么接下来，我们再选择能够使得上述模型达到最大精度的主成分数量：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8B%EF%BC%89/20210625161605292.png" alt="在这里插入图片描述"></p><p>自此，有关于人脸识别的基础流程便演示完了。</p><p>PS：说实话，在之前的监督学习里，完成了那么多次建模，就没有一个模型精度是低于80%的，像上面这么低的模型精度还真有点让人不适应。想要继续提高模型精度的朋友可以进行调参，也可以利用其它算法来建模，或者尝试其它处理数据的方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实，在要求人们评价人脸的相似度时，他们更可能会使用年龄，性别，面部表情和发型等属性，而这些属性很难从像素强度中推断出来。重要的是要记住，算法对数据（特别是视觉数据，比如人们非常熟悉的图像）的解释通常与人类的解释方式大不相同。</p><p>让我们回到PCA的具体案例。我们对PCA变换的介绍是：先旋转数据，然后删除方差较少的成分。另一种有用的解释是尝试找到一些数字（PCA旋转后的新特征值），使我们可以将测试点表示为主成分的加权求和。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>主成分分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征提取</tag>
      
      <tag>人脸识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>主成分分析（PCA）应用——特征提取_人脸识别（上）</title>
    <link href="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <url>/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我在另一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/117960112">《无监督学习与主成分分析（PCA）》</a>中已经讲过关于PCA的原理，以及它的其中一个应用——<strong>降维</strong>。那么本篇文章我来说一下PCA的另一个应用——<strong>特征提取</strong>。</p><p>特征提取背后的思想是，可以找到一种数据表示，比给定的原始表示更适合分析。特征提取很有用，它的一个很好的应用实例就是最近几年很火的<strong>人脸（图像）识别</strong>。</p><p>考虑到有很多小伙伴不了解图像的处理，所以我们分成上下两篇来进行讲解。</p><p><strong>本篇先讲解图像的基础以及python通常是如何处理图像的。</strong></p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt">LFW - People (Face Recognition)：https://www.kaggle.com/atulanandjha/lfwpeople?select=pairs.txt</a></p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624154112480.png" alt="在这里插入图片描述"></p><p>这是kaggle网站上一个专门用来做人脸识别的数据集，收录了网站上超过13000张人脸图片。好的，那么接下来把这份图片数据集下载下来并解压。</p><p>PS：下载下来的图片保存在lfw-funneled.tgz文件里，”.tgz”是一种压缩文件的格式，所以我们只要解压缩就可以了。</p><p>解压完毕后，我们就可以看见图片存储在以每人的名字所命名的文件里，每个文件夹包含数量不同的照片，而每个照片又分别以名字+数字的名字命名，方便我们使用。</p><h2 id="数据整理"><a href="#数据整理" class="headerlink" title="数据整理"></a>数据整理</h2><p>我们每拿到一份新数据，一定要对数据进行整理，<strong>了解数据的基本信息</strong>，譬如数据量，如何命名，数据维度等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>all_folds = os.listdir(<span class="hljs-string">r&#x27;C:\Users\Administrator\Desktop\源数据-分析\lfw_funneled&#x27;</span>)<br>all_folds = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> all_folds <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> x]<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(all_folds))<br>n=os.listdir(<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\Richard_Gere\\&#x27;</span>)<br><span class="hljs-built_in">print</span>(n[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><p><strong>os模块</strong>是一个python中专门用来遍历文件的第三方模块，具体原理就不在这里赘述了，感兴趣的朋友可以自己搜一下。那么运行上述代码后，我们就可以得知在lfw_funneled文件夹中，一共有5749文件，也就是说一共有5749个人的人脸图像，并且每个人的图像均是以名字+数字的方式来命名的.jpg图像文件。</p><p>那么接下来我们再看下每个人都有多少张人脸图像，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br>numbers_img=pd.DataFrame(columns=[<span class="hljs-string">&quot;文件名称&quot;</span>,<span class="hljs-string">&quot;图片数量&quot;</span>])<span class="hljs-comment">####统计各个文件夹里面的图片数量</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(all_folds)):<br>    path = <span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+all_folds[i]<br>    all_files = os.listdir(path)<br>    numbers_img.loc[i]=[all_folds[i],<span class="hljs-built_in">len</span>(all_files)]  <br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624155722308.png" alt="在这里插入图片描述"></p><p>这样一来，我们就知道了每个人有多少张人脸图像，也方便我们接下来进行数据集的选取和划分。</p><p>可以看出数据非常庞大，我们不可能对所有数据进行机器学习（电脑硬件达不到）。同时我们还要降低<strong>数据倾斜</strong>对模型精度的影响，那么我们这里只选取图片数量为10的人脸来当作数据集。</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624160729765.png" alt="在这里插入图片描述"></p><p>PS：如果某人的人脸出现次数过多的话，会造成数据倾斜，大大影响特征提取。</p><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><h3 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a><strong>基础介绍</strong></h3><p>这里先简单说一下什么是图像。图像由<strong>像素</strong>组成，通常存储为<strong>红绿蓝（RGB）强度（三维维度）</strong>。图像中的对象通常由上千个像素组成，它们只有放在一起才有意义。而我们所需要做的便是读取图像，将图像的像素转化为numpy数组，然后再通过操作numpy数组来去处理图像，最后再还原。</p><p>python里面有一个<strong>PIL</strong>的第三方模块，是专门用来处理数据的，如下图所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210624162232736.png" alt="在这里插入图片描述"></p><p>一般的像素值是以三维的形式存储的，其中有一个维度是专门用来存储像素颜色的。考虑到接下来的数据处理速度及提高模型精度，我们便剔除颜色维度，用图像的<strong>灰度值</strong>版本来进行处理，代码如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/2021062416343973.png" alt="在这里插入图片描述"></p><h3 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a><strong>图像操作</strong></h3><h4 id="1-图片转换成灰度值"><a href="#1-图片转换成灰度值" class="headerlink" title="1.图片转换成灰度值"></a><strong>1.图片转换成灰度值</strong></h4><p>好的，图像的基础处理方法讲解完了，接下来我们便对选出来的包含150张图片的数据集依次进行处理，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>image_arr_list=[]<span class="hljs-comment">###存放灰度值numpy数组</span><br>flat_arr_list=[]<span class="hljs-comment">###存放灰度值一维数组</span><br>target_list=[]<span class="hljs-comment">###存放目标值</span><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_10[<span class="hljs-string">&quot;文件名称&quot;</span>])):<br>    file_address=<span class="hljs-string">&#x27;C:\\Users\\Administrator\\Desktop\\源数据-分析\\lfw_funneled\\&#x27;</span>+img_10[<span class="hljs-string">&quot;文件名称&quot;</span>][m]+<span class="hljs-string">&quot;\\&quot;</span><span class="hljs-comment">####指定特定的文件地址</span><br>    image_name=os.listdir(file_address)<span class="hljs-comment">###获得指定文件夹下的左右文件名称</span><br>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> image_name:<br>        image=Image.<span class="hljs-built_in">open</span>(file_address+n)<br>        image=image.convert(<span class="hljs-string">&#x27;L&#x27;</span>)<span class="hljs-comment">###RGB（红绿蓝）像素值转换成灰度值</span><br>        image_arr=np.array(image,<span class="hljs-string">&quot;f&quot;</span>)<span class="hljs-comment">###灰度值转化成numpy数组（二维）</span><br>        flat_arr=image_arr.ravel()<span class="hljs-comment">###将数组扁平化处理，返回的是一个一维数组的非副本视图，就是将几行的数据强行拉成一行</span><br>        image_arr_list.append(image_arr)<br>        flat_arr_list.append(flat_arr)<br>        target_list.append(m)<span class="hljs-comment">###这里的m设定是数字，如果是文本的话后面的算法会报错</span><br>faces_dict=&#123;<span class="hljs-string">&quot;images&quot;</span>:np.array(image_arr_list),<span class="hljs-string">&quot;data&quot;</span>:np.array(flat_arr_list),<span class="hljs-string">&quot;target&quot;</span>:np.array(target_list)&#125;<br></code></pre></td></tr></table></figure><p>将读取的像素信息转化为numpy数组后，分别存储在各自对应的列表里面，并组合成一个字典，方便接下来的使用。接下来简单讲解一下：</p><ol><li>读取的RGB像素值如果直接转化为numpy数组的话会是三维数组，转换为一维数组后是可以用作接下来的机器学习的，但会大大降低训练速度。</li><li>灰度值转化成的numpy数组是一个二维数组，如果直接用于机器学习的话是没有办法读取使用的，所以需要用.ravel()来将二维数组转化为一维数组，也就是将两行的数据强行拉成一行数据。</li><li>如果数据集的标签值（目标值）”target“是文本的话，在接下来的训练部分中，机器便会无法识别，并报错，所以需要转换为数字。并且<strong>为了对应前面特征值的维度（数据维度是150行），这里需要将target也转换成numpy数组。</strong></li><li>这里之所以存储成字典，而不是DataFrame格式，也是因为存储维度的问题。如果存储成DataFrame的话，便需要62500（每张图片的像素数量是250x250）列来存储每个像素，这样的DataFrame太大了，不利于后面的处理，所以这里就以numpy数组的形式存储成字典。</li></ol><p>这里再说一下字典中的“images”，“data”的维度，如下所示：</p><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/20210625150016818.png" alt="在这里插入图片描述"></p><p>图片的像素是以250x250的二维numpy数组的形式存储在”images”中，而为了接下来的机器学习，便将二维数组转换为一维numpy数组存储在“data”中（250x250=62500)。</p><p>PS：可以通过矩阵变换，将原有的一维数组还原成二维灰度值，具体原理就不多说了，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">shape=image_arr.shape<span class="hljs-comment">###获得二维数组的维度</span><br>vector=np.matrix(flat_arr)<span class="hljs-comment">####将一维数组转换成矩阵</span><br>arr2=np.asarray(vector).reshape(shape)<span class="hljs-comment">###可以通过这个矩阵将一维数组转换为原灰度值numpy数组，即arr2=image_arr</span><br></code></pre></td></tr></table></figure><h4 id="2-灰度值还原成图片"><a href="#2-灰度值还原成图片" class="headerlink" title="2.灰度值还原成图片"></a><strong>2.灰度值还原成图片</strong></h4><p>接下来，我们可以把灰度值再还原成图片，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br>i = <span class="hljs-number">0</span><br>plt.figure(figsize=(<span class="hljs-number">45</span>, <span class="hljs-number">30</span>))<br><span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> faces_dict[<span class="hljs-string">&quot;images&quot;</span>]:<br>    <span class="hljs-comment">#总共150张图，把图像分割成15X10</span><br>    plt.subplot(<span class="hljs-number">15</span>,<span class="hljs-number">10</span>,i+<span class="hljs-number">1</span>)<br>    plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<span class="hljs-comment">###通过灰度值还原图像</span><br>    <span class="hljs-comment">#关闭x，y轴显示</span><br>    plt.xticks([])<br>    plt.yticks([])<br>    plt.xlabel(faces_dict[<span class="hljs-string">&quot;target&quot;</span>][i])<br>    i=i+<span class="hljs-number">1</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/06/27/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%88%E4%B8%8A%EF%BC%89/2021062515065469.png" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>好的，关于python处理图像方面的基础便先说道这里。总的来说，便是利用numpy函数来存储组成图像的像素信息，之后通过操作numpy数组来去达到变换图像的目的。</p><p>下一篇，我会讲解关于人脸识别的模型训练，以及PCA对训练过程的优化。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>主成分分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征提取</tag>
      
      <tag>人脸识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>天龙八部-段誉篇</title>
    <link href="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/"/>
    <url>/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<blockquote><p>“如果在我的小说中选一个角色让我做，我愿做天龙八部中的段誉，他身上没有以势压人的霸道，总给人留有余地。”——金庸</p></blockquote><span id="more"></span><h3 id="生平简介"><a href="#生平简介" class="headerlink" title="生平简介"></a>生平简介</h3><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=874374245,3319416477&fm=26&fmt=auto&gp=0.jpg" alt="img" style="zoom: 45%;">    </div>     <div class="column" style="float:left;width:75%">         在《天龙八部》里面，段誉就属于酱油型男主。虽然书中从头到尾都有他，但他一般都是在旁边看戏。从第一回青衫磊落险峰行，一直到第十回剑气碧烟横，这个内容大概就是说他泡了两个妹子，一个叫钟灵，一个叫木婉清，后来就发现这两个人竟然都是自己同父异母的妹妹。之后,他便被鸠摩智抓到苏州去了，在那里，他碰到了他的一生之敌王语嫣。之后段誉就一直跟着慕容复这些人，在这一路上就是盯着这个王姑娘。虽然人家没怎么理他，但是他还是没有放弃。就这么一直从第十三回追到了第四十五回，终于等到了转机。书中写到西夏公主要招亲，而慕容复为了得到西夏的势力，便也去报名参加。可这样一来，王语嫣就没办法嫁给自己心爱的表哥了。于是一气之下，她就选择自杀，而段誉正好趁虚而入，一下子就翻盘了。    </div></div><h3 id="人物原型"><a href="#人物原型" class="headerlink" title="人物原型"></a>人物原型</h3><p>有人说萧峰是武松的复制，其实段誉也是有原型的。读过《红楼梦》的朋友们应该一眼就能看出来，段誉的原型就是《红楼梦》里面的贾宝玉。这两个人都是属于那种比较叛逆的富家子弟，一个是不想读书当大官，一个是不想学武功当皇帝。但是两个人的喜好却是一样的，那就是都喜欢“妹妹”。</p><p>在《天龙八部》的第二回玉壁月华明中，写到段誉掉进了无量山的琅环福地之后，在里面见到了神仙姐姐的玉像，然后又学会了逍遥派的北冥神功和凌波微步。而在《红楼梦》的第五回里面，就写的是贾宝玉神游太虚幻境，他也是在里面见到了美女，并且也是学到了功夫，只不过一个是打人的功夫，而一个是床上的功夫。在小说的结局中，贾宝玉当了和尚，而段誉最后也是当了和尚。</p><p>（PS：段誉最后传位给了段正兴，也就是《射雕英雄传》里面“南帝”段智兴的父亲。后来段智兴也出家成为了一灯大师，所以大理的皇家好像一直都有这种出家的传统。）</p><h3 id="身世"><a href="#身世" class="headerlink" title="身世"></a>身世</h3><p>其实书中关于段誉身世的描写算是高潮部分了。说是段誉一家都被慕容复跟“恶贯满盈”段延庆给抓了。因为段誉是大理皇帝的继承人，所以慕容复为了实现皇帝梦，就必须先把段誉给杀了，然后再逼段正醇传位于自己。正当他要下手之时，只听得段誉的母亲刀白凤说道：“天龙寺外，菩提树下，花子邋遢，观音长发。”这十六个字说来极轻，但在段延庆听来却如晴天霹雳一般。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=2170625573,283933444&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>当年段延庆在天龙寺外一心求死之时，突然来了一个白衣女子主动与他交合，他觉得这肯定是观音菩萨前来点化他，于是又重拾做人的信心。此时见刀白凤解开发际，万缕青丝披将下来，竟与当年那位天龙寺外菩提树下的观音菩萨别无二样。原来这是刀白凤当年为了报复段正醇的滥情，而作出的一件糊涂事。然后刀白凤又把段誉的生辰八字告诉了他，正好与那天差了十个月。这时段延庆终于反应过来，原来自己才是段誉的亲生父亲。</p><p>段延庆这一生从未有过男女之情、阖家之乐，此时突然冒出来一个亲生儿子，刹时间惊喜交集，心神激荡，只觉世上什么名利富贵，什么帝王之业，都万万比不上这个儿子。虽然后来段誉死活不肯认他，但是他觉得既然段正醇死了，那皇位肯定是传给段誉了。当年段正明抢了自己的皇位，现在皇位又回到了自己的儿子身上，只能说是天道好轮回。于是心花怒放，飘然而去了。</p><p>之后刀白凤就告诉段誉：“你的亲生父亲并不是段正醇，而是那个段延庆，你之前的那些妹妹跟你也没有什么血缘关系，你想娶哪个就娶哪个，最好一块都娶了。”而段誉听后却也不知道是该高兴还是该难受。书中最后段誉便带着他的妹妹们回到大理，继承了皇位。</p><h3 id="感情"><a href="#感情" class="headerlink" title="感情"></a>感情</h3><p>其实我们看到古龙小说中描写的陆小凤，楚留香这样的人，不管他们走到哪里都有女人跟着。这并不说他们缺少女人，只是因为他们对女性有一种欣赏和爱惜的感情。而段誉对王语嫣的感情，其实也是类似于这种感情，或者说是有点幻想性质的感情。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622193647.png" alt="微信截图_20210622193647"></p><p>书里说段誉在看到神仙姐姐的玉像后，瞬间就被迷住了，并且玉像的主人让他磕了一千个头，还要他学武功，他也照做了。要知道这可是连他爹都叫不动的事儿啊，结果却被一个“<strong>等身手办</strong>”给说服了。后来段誉在苏州见到了王语嫣，自此之后他就被迷的神魂颠倒，便是因为王语嫣跟之前的雕像长得实在太像了。</p><p>在老版里面段誉最后就跟王语嫣在一起了。但是在新版里面，金庸先生又在后面加了一大段，说是王语嫣并不知道段誉的身世，她以为段誉也是自己的哥哥。于是王语嫣就给段誉发脾气，说要当尼姑。而段誉经过王语嫣的一顿闹，突然觉得这个王姑娘并不怎么爱自己，只是在无可奈何之下才对自己妥协的。而自己对王姑娘的感情，也可能只是因为自己把她当成了那座玉像而已。</p><p><img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=3839996475,2478899382&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>其实很多人看小说，都觉得里面的王语嫣肯定特别美丽。小说翻拍的电视剧也是请了<strong>刘亦菲</strong>来饰演这个角色。其实这里我们犯的错误跟段誉一样，也是误把王语嫣当成了神仙姐姐。我们都知道金庸先生笔下那些女主都很漂亮，书里面也是喜欢用一大堆的华丽的辞藻来形容她们。比如《神雕侠侣》里面的小龙女，书中说杨过第一次见到小龙女：<u>只觉这少女清丽秀雅，莫可逼视，神色间却冰冷淡漠，当真洁若冰雪，却也是冷若冰雪，实不知她是喜是怒，是愁是乐，竟不自禁的感到恐怖：「这姑娘是水晶做的，还是个雪人儿？到底是人是鬼，还是菩萨仙女？」</u>。还有《书剑恩仇录》里面的香香公主，书里面说是两边的人要打仗了，可看到她之后竟然都放下了兵器。大家要是无法想象的话，可以看一看《还珠格格》里面的香妃，大概就是那个意思。还有就是《鹿鼎记》和《碧血剑》里面写的陈圆圆，那可是历史上的有名的大美女，李闯王为她放弃了江山，吴三桂更是为了她放清兵入关，当了“汉奸”。《碧血剑》中描写说陈圆圆一出现，全场的人都被她迷住了，就连练了混元功的袁承志都差点把持不住。</p><p>但奇怪的是，同样作为女神的王语嫣，书里面对她的描写却都是围绕着那个玉像，总是说她怎么像那个雕像，而不是说她到底有多美。而书中仅有的几句夸赞的话也都是出于段誉之口，旁人见了她无非就是说什么美貌可爱啊、什么娇滴滴的小姑娘啊。但是在段誉眼里，她却是各种仙气，各种女神范。在新修版里面，金庸先生更是明确强调了这一点，就是无论王语嫣长什么样子，都不及段誉自己心中构成的形象。实际上，他真正爱的就是那个想象出来的完美女性，也就是所谓的心魔。</p><p>后来金庸先生为了点醒那些把自己带入成段誉的读者，就又在后面下了重手。书中写到王语嫣想要永葆青春，于是就跟段誉一起去了一个叫做不老长春谷的地方，结果什么也没找着。失望之余，段誉又带她去了当年的琅环玉洞。玉洞里，王语嫣看到那个神仙姐姐的玉像之后，就觉得自己再怎么好看也总有老去的一天，不能像这玉像一般永葆青春。一气之下，她就把玉像给砸了，然后就愤愤而去了。</p><p>可以说上面写的这段，一下子就把王语嫣从神坛上拉了下来，从一个高冷的女生彻底变成了一个神经质的泼妇。所以直到此刻，段誉才真正地看清了这位神仙姐姐的本相。但是很多读者依然沉浸在以前的那个童话故事里，他们还是不肯接受。甚至有些人更是破口大骂，说金庸老糊涂了，把好好的小说改成这样。</p><p>其实看一部作品啊，有时候可以结合一下作者的生平事迹，这样就可以理解书里面的一些奇怪的地方。</p><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/9345d688d43f879436f34c7cdf1b0ef41bd53a57" alt="img" style="zoom: 60%;">    </div>     <div class="column" style="float:left;width:75%">           金庸先生年轻的时候追过一个演员，叫做夏梦，她当时被称为是东方的奥黛丽赫本。当初在金庸追夏梦的时候，夏梦已经结婚了。她就告诉金庸说：虽然你特别有才华，我也是非常欣赏你，但可惜我已经结婚了，所以咱们就算了。但是金庸还是不死心，他就一直追，就跟书里面的段誉是一模一样。而到了最后，他还是没有成功。于是他就把这段恋情写到了他的作品里。不光是《天龙八部》，可以说金庸先生所有作品里的女主都或多或少的有一点夏梦的影子。在2000年的时候，他修改了《天龙八部》的结局，搞了一个新修版。这个时候，他已经七十多岁了。作为一个古稀之年的老人，他对以前的这些事情肯定已经看开了，所以他就把自己的人生感悟写到了书里面。金庸先生觉得自己对夏梦的感情是心魔，那段誉对王语嫣的感情又何尝不是？    </div></div><p>《天龙八部》虽说有点悲剧的色彩，但是在最后，所有人都解脱了。旧版小说里写段誉最后和王语嫣在一起，看起来是有情人终成眷属。其实这两个人并没有真正的解脱，反而更像是一个王子和公主的童话，作者对于两人结局的处理也是显得过于简单和草率。而在新版里面，这两个人才真正地看清了自己，段誉也破了神仙姐姐的心魔，而王语嫣也明白自己爱的还是原来的那个慕容复表哥。所以书里最后写到阿碧和王语嫣一起陪着已经疯了的慕容复，继续做他的皇帝梦。段誉看到这个景象之后就觉得各有各的缘法，我觉得他们可怜，其实他们心中焉知不是心满意足。</p><h3 id="心魔"><a href="#心魔" class="headerlink" title="心魔"></a>心魔</h3><p>我们前面说金庸先生在追夏梦的时候，跟段誉是一样的，也是被心魔所困。其实很多人都会有这种经历，就是有些人追了好几年的所谓的男神女神，等追到手后就发现这个人并没有想象中的那么好，就像是淘宝的买家秀和卖家秀一样。那就是因为在追求的过程中，自己会不自觉地把对方的形象进行魔改，所以最后就造成了一个特别大的心理落差。</p><p>不仅是爱情方面，其实很多问题都是由于这个心魔导致的。比较典型的就是<strong>中国式家长</strong>，父母觉得只要自己本着为孩子好的这个出发点，那自己做的任何荒唐的事情都是对孩子有好处的。最近刚高考完，学生们都在填志愿，很多父母都觉得：哎呀，这个好啊、那个有前途啊，这个工作稳定啊。他们完全不考虑孩子的天赋跟兴趣。还有一些女生的家长，一直就不让孩子在大学里谈恋爱，说这个学校里的男孩子都不靠谱，谈恋爱耽误学习什么的。但是一到毕业，父母又马上要求孩子带个男朋友回家。说实话，挺难为人的。因为这些家长只愿意对自己意淫出来的这个“为孩子好”的心魔去负责。<strong>那么孩子到底好不好呢？</strong></p><h3 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h3><div class="row">       <div class="column" style="float:left;width:25%">            <img src="/2021/06/22/%E5%A4%A9%E9%BE%99%E5%85%AB%E9%83%A8-%E6%AE%B5%E8%AA%89%E7%AF%87/u=211747070,732967533&fm=26&fmt=auto&gp=0.jpg" alt="img" style="zoom: 85%;">    </div>     <div class="column" style="float:left;width:75%">           最后再说一说鸠摩智。其实这个鸠摩智对应的就是天龙八部里面的迦楼罗。迦楼罗是印度神话里的一种半人半鸟的神，以龙为食。而段誉对应的正是天龙八部里面的龙。这个龙不是我们现在说的那个龙，而是一种水中的大毒蛇。古印度人就对他特别的尊敬，因为他是负责降雨的神。而在中国，古人也是经常把皇帝比作真龙天子。而上面的这些特点，基本上都能跟段誉对上。书里说段誉虽然老是被鸠摩智欺负，但最后却又不小心用北冥神功把鸠摩智的内力全部吸走了。传说迦楼罗就是因为吃了很多毒蛇，体内积蓄毒气极多，最后中毒而死，死后留下一颗纯青琉璃心。而书里面也说鸠摩智在被段誉吸干内力后就醒悟了，最后一心投入到佛学工作中去。    </div></div><p>其实《天龙八部》里的萧峰，段誉，虚竹还分别对应了佛教里面的贪、嗔、痴三毒。段誉的问题就跟鸠摩智一样，都是贪。只不过鸠摩智是贪于武功，而段誉则是贪于美色。那为什么说段誉是贪于美色呢？</p><p>最近博主看到的一个故事或许可以形容段誉的行为。说是有一对青年男女搞对象，女生发现男生经常会看一些美女的视频，图片，甚至在大街上走路的时候，眼睛也会不自觉的往其他女生身上瞟。于是女生就很生气，质问男生：“你明明已经有女朋友了，为什么还会看其他的美女？难道是我长得不够好看吗？”男生回答说：“没有，你长得很好看。但是你见过哪个男孩子一辈子只有一件玩具的？”</p><p><strong>我觉得把女生比作玩具是很王八蛋的想法。</strong>但上面的故事倒也从另一方面反映了段誉的感情经历，从一开始的钟灵，木婉清，再到后来的王语嫣，甚至是那种对于王语嫣的“求而不得”的感情。最后，幻想破碎，才发现只有共患难的人才是适合自己的。</p><p><strong>所以说啊, 喜欢和合适不是一回事, 恋爱和结婚也不是一回事。</strong></p><p>小的时候,博主觉得”有情人终成眷属”之所以很难做到,都是现实中的物质原因所导致的,或是金钱,或是工作等等.但现在,我却觉得两个相互喜欢的人能够走到最后真的是太不容易了,因为他们不仅要拥有中彩票的运气来遇见对方,还要<strong>同时</strong>经受住物质与感情的双重考验.</p><p><strong>或许,这就是感情珍贵的地方之一吧.</strong></p>]]></content>
    
    
    <categories>
      
      <category>武侠小说</category>
      
      <category>天龙八部</category>
      
    </categories>
    
    
    <tags>
      
      <tag>金庸先生</tag>
      
      <tag>段誉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>无监督学习与主成分分析</title>
    <link href="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <url>/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在之前的文章中，我讲了很多监督学习的算法（线性模型，SVM，决策树，神经网络等），那么接下来，我们要开始接触无监督学习了。首先，我们先说下相关概念。</p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>与监督学习不同，<strong>在无监督学习中，学习算法只有输入数据，并且从数据中提取需要的知识</strong>。而其中有两种常用类型：<strong>数据集变换</strong>和<strong>聚类</strong>。</p><p><strong>无监督变换</strong>是创建数据新的表示的算法，与数据的原始表示相比，新的表示可能更容易被人或其它机器学习算法所理解。而<strong>无监督变换的一个常见应用就是降维</strong>，它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新的方法，<strong>用较少的特征就可以概括数据信息的重要特性</strong>。降维的一个常见应用是将数据降为二维之后进行可视化。（PS：这里的降维和“降维打击”里面的降维是两回事。。。）</p><p><strong>无监督变换的另一个应用是找到”构成“数据的各个部分</strong>。这方面的一个例子就是对文本文档集合进行主题提取。</p><p>而与之相反，<strong>聚类</strong>算法就是将数据划分成不同的组，每组包含相似的物项。譬如说人脸识别，可以将相同的某个人的照片分在一组。</p><p>实际上，<strong>无监督学习的一个主要挑战就是评估算法是否学习到了有用的东西</strong>。因为无监督学习一般用于不包含任何标签信息的数据，所以我们不知道正确的输出应该是什么。因此很难判断一个模型是否”表现良好“。<strong>通常来说，评估无监督算法结果的唯一方法就是人工检查</strong>。</p><p>PS：博主之前在58同城做过微聊审核相关的工作。整体的审核流程大致就是用机器学习建立一个模型，去评估用户有没有说违规的话，之后随机抽取模型的审核结果，再进行人工复审。当复审的错误率达到某个阈值的时候，就需要向技术部门阐明情况，提出修改模型的要求了。</p><p>因此，如果数据科学家想要更好地理解数据，那么无监督算法通常可用于探索性的目的，而不是作为大型自动化系统的一部分（这点与监督学习是不同的）。因此<strong>无监督算法的另一个常见应用就是作为监督算法的预处理步骤</strong>。</p><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p>前面说过，利用无监督学习进行数据变换可能有很多种目的。最常见的就是可视化，压缩数据（降维），以及寻找信息量更大的数据表示以用于进一步的处理。为了实现这些目的，最简单也是最常用的一种算法就是<strong>主成分分析（PCA）</strong>。</p><p>PS：接下来要说理论的东西了，很枯燥，但是希望各位朋友可以耐心的看完，下面的内容对于算法的理解很有帮助。</p><p>主成分分析（PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。</p><p>接下来，我用通俗一点的话来解释下：</p><p>首先，<strong>模型就是数据集所表现出来的信息的集合体或者说构成体</strong>。通常，在机器学习的过程中，特征的个数过多会增加模型的复杂度。而我们所希望的<strong>理想状态就是用最少的特征表示数据集最多的信息</strong>。</p><p>在许多情形下，特征之间是有一定的相关关系的（如线性相关：一个特征可以用另一个特征线性表示）。而当两个特征之间有一定的相关关系时，可以理解为两个特征所反映的此数据集的信息有一定的重叠。（譬如特征x和特征y，其中y=a*x+b）。</p><p>而主成分分析就是对于原先数据集的所有特征进行处理。删去多余的重复的特征，建立尽可能少的特征，使得这些新特征两两不相关。并且这些新特征在反映数据集的信息方面尽可能保持原有信息。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>来自于kaggle的一份关于心脏病患者分类的数据集：<a href="https://www.kaggle.com/ronitf/heart-disease-uci">https://www.kaggle.com/ronitf/heart-disease-uci</a><br><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617134807955.jpg" alt="在这里插入图片描述"><br>这份数据集并不大，只包括303份数据样本（该数据不包含空值）。其中数据特征包括患者年龄，性别，心率，血糖量，血压等13个维度以及分类目标-target。而我们所需要做的，就是依照这些特征值来进行建模，从而依照模型的某种趋势来判断患者心脏是否健康。</p><h2 id="PCA实际应用-降维"><a href="#PCA实际应用-降维" class="headerlink" title="PCA实际应用-降维"></a>PCA实际应用-降维</h2><p><strong>1.数据导入</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\heart.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/ronitf/heart-disease-uci</span><br>heart=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.标准化</strong></p><p>由于PCA的数学原理是依照方差最大的方向来去标记主成分，所以我们先对数据进行标准化，使得各个维度的方差均为1.所以我们先用StandardScaler来对数据进行缩放，代码如下：</p><p>（PS：具体实现原理及过程就不再赘述了，不了解的朋友可以看下<a href="https://blog.csdn.net/weixin_43580339/article/details/117774014">python机器学习之数据预处理与缩放</a>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>train=heart.drop([<span class="hljs-string">&quot;target&quot;</span>],axis=<span class="hljs-number">1</span>)<br>standard=StandardScaler()<br>standard.fit(train)<br>X_scaled=standard.transform(train)<br></code></pre></td></tr></table></figure><p>注意别忘了把数据集中的分类目标提取出来（<code>heart.drop([&quot;target&quot;],axis=1)</code>），分类目标是我们的分类目的或者说结果，而这里的标准化是不针对分类目标的。</p><p><strong>3.PCA拟合转换</strong></p><p>学习并应用PCA变换与应用预处理变换一样简单，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>pca=PCA(n_components=<span class="hljs-number">2</span>)<span class="hljs-comment">###为了接下来做图方便一些，就只保留数据的前两个部分，实际上这里也可以多保留些主成分</span><br>pca.fit(X_scaled)<span class="hljs-comment">###对数据进行拟合pca模型</span><br>X_pca=pca.transform(X_scaled)<span class="hljs-comment">#将数据变换到前两个主成分上</span><br></code></pre></td></tr></table></figure><p>为了调整降低数据的维度，我们需要在创建PCA对象时指定想要保留的主成分个数（<strong>n_components=2</strong>）。之后将PCA对象实例化，调用<strong>fit</strong>方法找到主成分，再调用<strong>transform</strong>来旋转并降维。默认情况下，PCA仅变换数据，但保留所有的主成分。</p><p>我们来看下变换前后的数据规模：<br><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/2021061714251032.jpg" alt="在这里插入图片描述"><br>可以看出数据由之前的303x13，变成了303x2，样本数量没有变化，特征维度由之前的13个变成了2个。</p><p>在拟合过程中，主成分被保存在components_属性中，其中每一行对应于一个主成分，它们按照重要性排序（第一主成分排在首位，以此类推）。列对应于PCA的原始特征属性。<br><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617144959959.png" alt="在这里插入图片描述"></p><h2 id="PCA实际应用-高维数据可视化"><a href="#PCA实际应用-高维数据可视化" class="headerlink" title="PCA实际应用-高维数据可视化"></a>PCA实际应用-高维数据可视化</h2><p>正如我们之前所说的，PCA的另一个应用就是将高维数据可视化。要知道，对于有两个以上特征的数据，我们是很难绘制散点图的，譬如说上面包含13个维度的心脏患病数据。但是，我们可以用散点图画出PCA变换之后的两个主成分，并着色分类，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>result=np.c_[X_pca,heart[<span class="hljs-string">&quot;target&quot;</span>]]<span class="hljs-comment">###将处理结果和数据集的目标值结合起来，这样就是一个新的数据集了。</span><br><span class="hljs-comment">###新数据集与原数据集的信息相差不大，甚至剔除了部分重叠数据造成的影响</span><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<br>plt.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;SimHei&#x27;</span>]<span class="hljs-comment">###防止中文显示不出来</span><br>plt.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="hljs-literal">False</span><span class="hljs-comment">###防止坐标轴符号显示不出来</span><br>result_1=result[result[:,<span class="hljs-number">2</span>]==<span class="hljs-number">1</span>]<span class="hljs-comment">###target类别1</span><br>result_0=result[result[:,<span class="hljs-number">2</span>]==<span class="hljs-number">0</span>]<span class="hljs-comment">###target类别0</span><br>plt.scatter(result_1[:,<span class="hljs-number">0</span>],result_1[:,<span class="hljs-number">1</span>])<span class="hljs-comment">###画类别1的散点图</span><br>plt.scatter(result_0[:,<span class="hljs-number">0</span>],result_0[:,<span class="hljs-number">1</span>])<span class="hljs-comment">###画类别0的散点图</span><br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&#x27;第一主成分&#x27;</span>)<span class="hljs-comment">###横坐标轴标题</span><br>plt.ylabel(<span class="hljs-string">&#x27;第二主成分&#x27;</span>)<span class="hljs-comment">###纵坐标轴标题</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617143301881.png" alt="在这里插入图片描述"><br>上面的散点图绘制了第一主成分和第二主成分的关系，然后利用类别信息对数据点进行着色。这让我们相信，即使是线性分类器（在这个空间中学习一条直线）也可以在区分这两个类别时表现的相当不错。</p><p>在这里提醒大家一点，要注意pca是一种无监督的方法，在旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>缺点</strong></p><p>pca有一个缺点，就是通常不容易对图中的两个轴进行解释。虽然主成分是原始特征的组合，但这些组合往往非常复杂，我们可以用热力图表现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">6</span>))<br>plt.matshow(pca.components_)<br>plt.yticks([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-string">&quot;第一主成分&quot;</span>,<span class="hljs-string">&quot;第二主成分&quot;</span>])<span class="hljs-comment">###横坐标轴刻度</span><br>plt.colorbar()<br>plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(train.columns))),train.columns)<span class="hljs-comment">###纵坐标轴刻度</span><br>plt.xlabel(<span class="hljs-string">&#x27;特征值&#x27;</span>)<span class="hljs-comment">###横坐标轴标题</span><br>plt.ylabel(<span class="hljs-string">&#x27;主成分&#x27;</span>)<span class="hljs-comment">###纵坐标轴标题</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617144107125.jpg" alt="在这里插入图片描述"><br>PS：横坐标轴是13个特征维度</p><p>上图可以看出每个主成分均是由13个特征值组成的，还展现出了各个特征对于成分构成的重要性（颜色越深越重要），但是我们却无法明确的指出这些特征如何构成主成分的。</p><p><strong>优点</strong></p><p>至于优点，我们从之前的散点图便可以看出来，那就是用简单的模型便可以对数据进行分类。通常来说，SVM核向量算法要比普通的线性模型算法Logistic更为复杂，且模型精度要更高一些。（<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">SVM</a>与<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">Logistic</a>的原理之前有在其他的文章中讲过，感兴趣的朋友可以点击链接去看一下，这里就不多做解释了。）</p><p>那么为了突出刚才提到的优点，接下来我们用svm对没有经过pca变换的数据进行建模：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>X_train,X_test,y_train,y_test=train_test_split(train,heart[<span class="hljs-string">&quot;target&quot;</span>],random_state=<span class="hljs-number">1</span>)<br>svm=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;没有经过主成分分析的SVM模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test))))<br><span class="hljs-comment">###没有经过主成分分析（pca）的模型评分</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617145818809.png" alt="在这里插入图片描述"><br>再利用Logistic算法对变换后的数据进行建模：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>result_pca=pd.DataFrame(result)<span class="hljs-comment">###其中第三列是目标值</span><br>X_train,X_test,y_train,y_test=train_test_split(result_pca.loc[:,<span class="hljs-number">0</span>:<span class="hljs-number">1</span>],result_pca.loc[:,<span class="hljs-number">2</span>],random_state=<span class="hljs-number">1</span>)<br>logistic=LogisticRegression(penalty=<span class="hljs-string">&#x27;l2&#x27;</span>,C=<span class="hljs-number">1</span>,solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,max_iter=<span class="hljs-number">1000</span>)<br>logistic.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;经过主成分分析的Logistic模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,logistic.predict(X_train))))<br><span class="hljs-comment">###经过主成分分析（pca）的模型评分</span><br></code></pre></td></tr></table></figure><p><img src="/2021/06/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/20210617150138257.png" alt="在这里插入图片描述"><br>从上面的结果对比便可以看出pca的优点，那就是在最大限度地保留了数据集的信息状态的条件下，它将原本复杂的数据集，转变为了更容易训练的低维度数据级，提高了模型的训练精度。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>主成分分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>数据处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>决策树集成-随机森林随机森林</title>
    <link href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/"/>
    <url>/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>集成</strong></p><p>集成是合并多个机器学习模型来构建更强大模型的方法。在机器学习算法中有许多模型属于这一类，但已证明有两种集成模型对大量分类和回归的数据集都是有效的，二者都以决策树为基础，分别是<strong>随机森林（random forest）</strong>和<strong>梯度提升决策树（gradiet boosted decision tree）</strong>。</p><p>之前已经讲解过了随机森林(<a href="https://blog.csdn.net/weixin_43580339/article/details/116231286">决策树集成-随机森林之分类实操</a>),这次讲解<strong>梯度提升决策树</strong>。在了解梯度提升决策树之前,建议先去看一下我的另外两篇讲解决策树的文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/115939923">决策树算法之讲解实操（下）</a>，重复的东西，我这里就不在赘述了。</p><h2 id="思想简介"><a href="#思想简介" class="headerlink" title="思想简介"></a>思想简介</h2><p>在之前的一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中我们提到过，决策树的一个主要缺点在于经常对训练数据过拟合。那么除了随机森林之外,梯度提升回归树就是解决这个问题的另一种方法。</p><p>梯度提升回归树是通过合并多个决策树来构建一个更为强大的模型。虽然名字中含有”回归”，但是这个模型既可以用于回归也可以用于分类。与随机森林的方法不同,<strong>梯度提升采用连续的方式构造树,每颗树都试图纠正前一棵树的错误</strong>.默认情况下,梯度提升回归树中没有随机化,而是用到了强预剪枝。<strong>梯度提升树通常使用深度很小(1到5之间)的树</strong>,这样模型占用的内存更少,预测速度也更快.</p><p><strong>梯度提升树背后的主要思想是合并许多简单的模型</strong>(在这个语境中叫做弱学习器),比如深度较小的树.每棵树只能对部分数据作出好的预测,因此,添加的树越来越多,可以不断迭代,提高性能.</p><p>梯度提升树经常是机器学习竞赛的优胜者,并广泛应用于业界.与随机森林相比,它通常对参数的设置更为敏感,但如果参数设置正确的话,模型精度会更高.</p><h2 id="实操建模"><a href="#实操建模" class="headerlink" title="实操建模"></a>实操建模</h2><p>数据是一份<a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒质量分类</a>的数据集，通过各个维度来判断红酒质量，之前在<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中已经讲解使用过了，这里就不多在赘述了，我们直接建模，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-comment">#梯度提升回归树</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\winequality-red.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</span><br>red_wine=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br>train=red_wine.drop([<span class="hljs-string">&quot;quality&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,red_wine[<span class="hljs-string">&quot;quality&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>gbcf=GradientBoostingClassifier(random_state=<span class="hljs-number">1</span>)<br>gbcf.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,gbcf.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,gbcf.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508121801300.png" alt="在这里插入图片描述"></p><p>下面是之前的文章中单棵决策树建立的模型结果：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210428134820111.jpg" alt="在这里插入图片描述"><br>二者相比可以看出，梯度提升树的模型精度要比单棵树的要好一点，过拟合现象也比之前要减轻很多。<br>接下来我们了解一下梯度提升树的主要模型参数。</p><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>在梯度提升回归树中，我们主要会用到三个模型参数n_estimators（树的个数），max_depth（树的深度）,learning_rate（学习率）,至于其它的参数，一般情况下直接默认就好。</p><p><strong>max_depth</strong>：用于降低每棵树的复杂度.一般来说,梯度提升模型的max_depth通常设置得很小,一般不超过5.<br>接下来我们来调节这个参数，提高模型精度，代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_1=pd.DataFrame(columns=[<span class="hljs-string">&quot;决策树深度(max_depth)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">5</span>):<br>    gbcf=GradientBoostingClassifier(max_depth=i,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_1=result_1.append([&#123;<span class="hljs-string">&quot;决策树深度(max_depth)&quot;</span>:i,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_1[result_1[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_1[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508122322592.jpg" alt="在这里插入图片描述"></p><p>可以看到当我们设定参数max_depth为4的时候，模型精度可以达到66.5%左右，较之前的结果提高了一些。</p><p>梯度提升树模型的另外两个主要参数包括<strong>树的数量n_estimators</strong>和<strong>学习率learn_rate</strong>,后者<strong>用于控制每棵树对前一棵树的错误纠正程度</strong>.这两个参数高度相关,因为learning_rate越低,就需要更多的树来构建具有相似复杂度的模型.随机森林的n_estimators值总是越大越好,但是梯度提升不同,增大n_estimators会导致模型更加复杂,进而可能导致过拟合.<strong>通常的做法是根据时间和内存的预算选择合适的n_estimators,然后对不同的learning_rate进行遍历.</strong></p><p>这两个参数的调节代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_2=pd.DataFrame(columns=[<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">500</span>,<span class="hljs-number">10</span>):<br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=i,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_2=result_2.append([&#123;<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>:i,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_2[result_2[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_2[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/2021050812303492.jpg" alt="在这里插入图片描述"><br>n_estimators的调节结果如上图所示,那么接下来我们在上面的参数基础上继续调节学习率:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">result_3=pd.DataFrame(columns=[<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>):<br>    m=i/<span class="hljs-number">10</span><br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=<span class="hljs-number">161</span>,learning_rate=m,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_3=result_3.append([&#123;<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>:m,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_3[result_3[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_3[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123248624.png" alt="在这里插入图片描述"><br>接下来,我们还可以对学习率的参数调节进行进一步的区间划分,代码及结果如下所示:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">result_4=pd.DataFrame(columns=[<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>):<br>    m=i/<span class="hljs-number">100</span><br>    gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,n_estimators=<span class="hljs-number">161</span>,learning_rate=m,random_state=<span class="hljs-number">1</span>)<br>    gbcf.fit(X_train,y_train)<br>    result_4=result_4.append([&#123;<span class="hljs-string">&quot;学习率(learning_rate)&quot;</span>:m,<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>:accuracy_score(y_test,gbcf.predict(X_test))&#125;])<br>result_4[result_4[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>]==result_4[<span class="hljs-string">&quot;梯度提升回归树待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123518243.png" alt="在这里插入图片描述"><br>至此,这个模型的参数就调节完毕了.<br>(ps:为了提高模型精度,参数是可以进行更近一步的调节,不过剩下的就需要朋友们自行探索了)</p><h2 id="分析特征重要性"><a href="#分析特征重要性" class="headerlink" title="分析特征重要性"></a>分析特征重要性</h2><p>与随机森林类似，梯度提升树也可以给出特征重要性,代码及结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>%matplotlib inline<br>gbcf=GradientBoostingClassifier(max_depth=<span class="hljs-number">4</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###梯度提升回归树</span><br>forest=RandomForestClassifier(max_depth=<span class="hljs-number">4</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###随机森林分类器</span><br>gbcf_prediction=gbcf.fit(X_train,y_train)<br>forest_prediction=forest.fit(X_train,y_train)<br>fig= plt.subplots(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">15</span>))<br>fig1 = plt.subplot(<span class="hljs-number">211</span>)<br>plt.title(<span class="hljs-string">&#x27;梯度提升回归树特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,gbcf_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.legend()<br>fig2=plt.subplot(<span class="hljs-number">212</span>)<br>plt.title(<span class="hljs-string">&#x27;随机森林特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,forest_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">13</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91/20210508123904327.jpg" alt="在这里插入图片描述"></p><p>如上图所示，在保证树的深度参数（max_depth）相同的情况下，梯度提升树的特征重要性与随机森林的特征重要性有些相似,实际上在某些数据集中,梯度提升树可能会完全忽略某些特征.</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>梯度提升决策树是监督学习中最强大也是最常用的模型之一.其主要缺点是需要仔细调参,而且训练的时间可能会比较长.与其他基于树的模型类似,这一算法不需要对数据进行缩放就可以表现得很好,而且也<strong>适用于二元特征与连续特征同时存在的数据集</strong>.但它也通常不适用于高维稀疏数据.</p><p>由于梯度提升和随机森林两种方法在类似的数据上表现的都很好,因此一种常用的方法就是先尝试随机森林,它的鲁棒性很好.如果随机森林效果很好,但预测时间太长,或者机器学习模型精度小数点后第二位的提高也很重要,那么切换成梯度提升通常会有用.</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>决策树集成</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>梯度提升回归树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据预处理与缩放</title>
    <link href="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/"/>
    <url>/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我们在接触监督学习时了解到，有一些算法（譬如<a href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>）对于数据的缩放非常敏感。因此，通常的做法是对数据集进行调节，使得数据表示更适合于这些算法。通常来说，这是对数据特征的一种简单的缩放和移动。</p><p>机器学习的理论实际上是起源于概率论与数理统计，接下来，我们来简单提几个相关概念，来帮助大家更好地理解接下来的要说的几种处理方法。</p><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>中位数</strong>——对于一组数字来说，中位数指的是这样的数值x：有一半的数值小于x，另一半的数值大于x。如果数据集的数据个数是偶数，就取中间两个数值的平均数。</p><p><strong>四分位数</strong>——按照四分之一的数据个数来划分数据集。较小四分位数指的是这样的数值x：有四分之一的数值小于x。较大四分位数指的是这样的数值x：有四分之一的数值大于x。</p><p><strong>方差</strong>——衡量随机变量或一组数据的离散程度的度量。</p><p><strong>异常值</strong>——在数据集之中的那些与众不同的数据点。</p><p>需要注意的是<strong>异常值并不一定是误差值或者错误值</strong>。譬如说想要统计某辆公交车上20名乘客的平均年龄，有一种情况就是其中19名乘客是处于20岁到30岁之间，但是有一名乘客的年龄是70岁，那么这名乘客的年龄就是属于与众不同的数据点，是异常值，而不是误差值。并且这个数值会影响最终的统计计算结果——拉高公交车上乘客的平均年龄。</p><h2 id="预处理的四种类型"><a href="#预处理的四种类型" class="headerlink" title="预处理的四种类型"></a>预处理的四种类型</h2><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610115711512.jpg" alt="在这里插入图片描述"><br>scikit-learn中一共提供了4种预处理方法，变换效果分别如上图所示，接下来，结合图像，我们来详细说一下这四种变换。<br><del>PS：如果大家不喜欢下面的枯燥理论就直接跳过吧，说实话，我觉得只要会使用，知道用于什么地方就够了。</del> </p><p><strong>StandardScaler（标准化）</strong>：确保每个特征的<strong>平均值</strong>为0，<strong>方差</strong>为1，使特征值都位于同一量级。但这种缩放不能保证特征任何特定的最大值和最小值。（我曾经在讲解<a href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络</a>的时候进行过标准化的人工处理，有源代码，感兴趣的朋友可以去看下，可以帮助大家更好地理解。）</p><p><strong>RobustScaler（剔除异常值）</strong>：RobustScaler也是一种标准化，工作原理与StandardScaler类似，确保每个特征的统计属性都处于同一范围。但是RobustScaler使用的是<strong>中位数</strong>和<strong>四分位数</strong>，而不是平均值和方差。这样RobustScaler会忽略与其它点有很大不同的数据点（<strong>异常值</strong>），减少异常值造成的麻烦。</p><p><strong>MinMaxScaler（归一化）</strong>：MinMaxScaler移动数据，使得所有特征都刚好位于0到1之间。对于二维数据集来说，所有的数据都包含在x轴0到1与y轴0到1组成的矩形中。（同样，我在讲解<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">SVM</a>的时候进行过归一化的人工处理，也有相关源代码。）</p><p><strong>Normalizer（正则化，有些地方也叫做归一化）</strong>：Normalizer用到的是一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于1。通过上面的第四幅小图可以看出：它<strong>将一个数据点投射到半径为1的圆上（对于更高维度的情况是球面）</strong>。这意味着每个数据点的缩放比例都不相同。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。</p><h2 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h2><p>接下来我们用数据集<a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">SVM</a>算法来实际使用下，看看效果。（数据以及相关算法我们都在之前讲解的SVM中详细地讲过了，感兴趣的朋友可以点击超链接去看下，这里就不在赘述了。）</p><p>首先是导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>划分训练集和测试集，并进行建模，精度评分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br>train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>svm=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140210590.jpg" alt="在这里插入图片描述"></p><p>接下来我们对数据进行缩放再进行建模评分，看看模型精度有什么变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<span class="hljs-comment">###标准化</span><br>standard=StandardScaler()<br>standard.fit(X_train)<span class="hljs-comment">###使用fit方法拟合缩放器，并将其应用于训练数据，其实就和之前学过的算法一样，先用fit去训练数据，适应数据</span><br>X_train_scaled=standard.transform(X_train)<span class="hljs-comment">####对训练数据进行实际缩放，也是类似之前学习的训练过程，在，fit适用数据之后，再用transfrom去同等变换X_train,X_test</span><br>X_test_scaled=standard.transform(X_test)<span class="hljs-comment">###注意测试集相对训练集来说移动必须是一致的，因为变换后数量级是不同的，但是要保证数据的分布形状要完全相同</span><br>svm.fit(X_train_scaled,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;标准化后SVM模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm.predict(X_test_scaled))))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/20210610140415448.jpg" alt="在这里插入图片描述"><br>可以看到模型精度较之前有了提升。</p><p>整个处理过程与之前算法训练模型类似。首先使用<strong>fit</strong>方法拟合缩放器（scaler），并将其应用于训练数据。然后为了应用刚刚学习的数据（即对训练数据进行实际缩放），我们使用缩放器的<strong>transform</strong>方法。最后为了将SVM应用到缩放后的数据上，还需要对测试集进行变换。</p><p>需要注意的是，为了让监督模型能够在测试集上运行，<strong>对训练集和测试集应用完全相同的变换是很重要的</strong>。因为<strong>刻度数值可以不一样，但是必须要保证测试集与训练集的数据分布是一样的（可以结合上面的散点图来看一下）</strong>。</p><h2 id="替代方法"><a href="#替代方法" class="headerlink" title="替代方法"></a>替代方法</h2><p>通常来说，想要在某个数据集上fit一个模型，然后再将其transform，是一个非常常见的过程。但是可以用比先调fit再调transform更高效的方法来计算。对于这种使用场景，所有具有transform方法的模型也都有一个fit_transform方法，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br>standard=StandardScaler()<br>X_train_scaled=standard.fit(X_train).transform(X_train)<span class="hljs-comment">###原方法先fit后transform</span><br>X_train_scaled=standard.fit_transform(X_train)<span class="hljs-comment">###结果相同，但计算更加高效</span><br></code></pre></td></tr></table></figure><p>虽然fit_transform不一定对所有模型都更加高效，但在尝试变换训练集时，使用这一方法仍然是很少的。<br><del>PS：最主要是看起来上了点档次</del></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>对于任何一种类型的数据集或者是一个算法来说，没有绝对正确的预处理算法。<strong>预处理方法，数据集与建模算法这三者之间永远都不会存在绑定关系</strong>。 譬如我们分别用SVM和神经网络来测试上述四种预处理算法，代码及结果如下所示：</p><p><strong>SVM：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:<br>    scaled=i<br>    i.fit(X_train)<br>    X_train_scaled=i.transform(X_train)<br>    X_test_scaled=i.transform(X_test)<br>    svm.fit(X_train_scaled,y_train)<br>    score=accuracy_score(y_test,svm.predict(X_test_scaled))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(i)+<span class="hljs-string">&quot;处理后得分：&quot;</span>+<span class="hljs-built_in">str</span>(score))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014311582.png" alt="在这里插入图片描述"><br><strong>神经网络（MLP）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<span class="hljs-comment">#多层感知机-MLP/神经网络</span><br>mlp=MLPClassifier(solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,random_state=<span class="hljs-number">1</span>,max_iter=<span class="hljs-number">100000</span>).fit(X_train,y_train)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [StandardScaler(),RobustScaler(),MinMaxScaler(),Normalizer()]:<br>    scaled=i<br>    i.fit(X_train)<br>    X_train_scaled=i.transform(X_train)<br>    X_test_scaled=i.transform(X_test)<br>    mlp.fit(X_train_scaled,y_train)<br>    score=accuracy_score(y_test,mlp.predict(X_test_scaled))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(i)+<span class="hljs-string">&quot;处理后得分：&quot;</span>+<span class="hljs-built_in">str</span>(score))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE/2021061014321875.png" alt="在这里插入图片描述"><br>可以看出，对于同一个数据集应用不同的算法，选择的处理方法是不一样的。对于SVM来说，用<strong>标准化</strong>处理精度会高一些，而对于神经网络来说，用<strong>归一化</strong>处理效果会更好。而如果我们对其它数据集进行机器学习的话，那么就会存在其它的选择。所以对于一份没有接触过的数据集来说，如果时间允许的话，可以尝试各种各样的组合，来去搭建精度最高的模型。</p><p>最后，虽然数据缩放不涉及任何复杂的数学，但良好的做法仍是使用scikit_learn提供的缩放机制，而不是自己人工实现它们，因为即使在这些简单的计算中也容易犯错。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>无监督学习</category>
      
      <category>数据预处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>SVM</tag>
      
      <tag>数据缩放</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>最小二乘法</title>
    <link href="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"/>
    <url>/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>线性回归模型</strong></p><p>对于不同的数据集，数据挖掘或者说机器学习的过程，就是建立数据模型的过程。对于回归类问题，线性模型预测的一般公式如下：</p><blockquote><p><em><strong>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+……+w[p]x[p]+b</strong></em></p></blockquote><p>这里x[0]到x[p]表示耽搁数据点的特征(本例中特征个数为p+1),w和b是学习模型的参数，y是预测结果，对于单一特征的数据集，公式如下：</p><blockquote><p>*<em>y=w[0]<em>x[0]+b</em></em></p></blockquote><p>大家可以看出来，这个很像高中数学里的直线方程。其中w[0]就是斜率，对于有更多特征的数据集，w包含沿每个特征坐标元素的斜率。或者，你也可以将预测的响应值看作输入特征的加权求和，权重由w的元素给出。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>最小二乘算法</strong></p><p>普通最小二乘算法，或者说线性回归，是回归问题最简单也是最经典的线性方法，线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的<strong>均方误差</strong>最小。<br><strong>均方误差</strong>是预测值与真实值之差的平方和除以样本数。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/sp1nalcord/mycsgo-data">第一人称fps游戏csgo的等分数据：https://www.kaggle.com/sp1nalcord/mycsgo-data</a><br><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106144238394.png" alt="在这里插入图片描述"></p><p>csgo是一款第一人称的射击游戏，该数据包括每位玩家的网络延迟（ping），击杀数量，死亡数量，得分情况等等。<br><del>嘿嘿，游戏这方面我还是比较了解的，这也是博主唯一一个不用看原版数据的英文介绍，就能看懂各个维度的数据集了。</del> </p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<span class="hljs-comment">#导入线性回归算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块<br><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\avocado.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>glass=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/neuromusic/avocado-prices</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p><p><strong>3.清洗数据</strong><br><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106144941434.png" alt="在这里插入图片描述"><br>可以看到这个数据并不包括缺失值，而且各个特征值之间也没有属性重叠的状况，所以暂时不需要任何处理。</p><p><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train,X_test,y_train,y_test=train_test_split(csgo[[<span class="hljs-string">&quot;Ping&quot;</span>,<span class="hljs-string">&quot;Kills&quot;</span>,<span class="hljs-string">&quot;Assists&quot;</span>,<span class="hljs-string">&quot;Deaths&quot;</span>,<span class="hljs-string">&quot;MVP&quot;</span>,<span class="hljs-string">&quot;HSP&quot;</span>]],csgo[<span class="hljs-string">&quot;Score&quot;</span>],random_state=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>将score划分为预测值，其它的属性划分为特征值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">LR=LinearRegression()<br>LR.fit(X_train,y_train)<br>prediction=LR.predict(X_test)<br>r2_score(y_test,prediction)<br></code></pre></td></tr></table></figure><p>引入knn算法，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/20210106145315552.png" alt="在这里插入图片描述"></p><p>可以看到，该模型的精度为94%左右。<br>至此，这个数据集的将建模就算是完成了。</p><p><strong>5.总结</strong></p><p>1.可以感受到最小二乘法算法并没有什么难点，但是它确是最经典，最重要的算法之一。因为有很多的其它线性回归算法都是基于它的模型公式推导出来的，所以对于这个模型的公式一定要理解。<br><del>对算法的原理一定要有自己的理解，或许并不会专门从事算法研发的工作，但是对于算法如何去使用，用在什么场景是一定要知道的。</del> </p><p>2.大家可以看到这个算法并不需要调参，因为这个算法并没有参数可言，这是一个优点，但是这也反映了一件事，就是没有办法控制模型的复杂度，也没有办法通过调整算法本身，来去提高模型精度。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>回归模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>支持向量机（SVM）算法-实际应用</title>
    <link href="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/"/>
    <url>/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><em><strong>SVM</strong></em></p><p>之前我们用了很多线性算法来做预测模型，像是<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">逻辑算法（LogisticRegression)</a>,<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192">lasso</a>,<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">岭回归</a>。但现实生活中，很多事情不是线性可分的（即画一条直线就能分类的），而SVM就是专治线性不可分，把分类问题转化为平面分类问题。这个算法中，我们将每一个数据项作为一个点，而在n维空间中(其中n是你拥有的特征数)作为一个点，每一个特征值都是一个特定坐标的值。然后，我们通过查找区分这两个类的超平面来进行分类。</p><p>我们用一张图形来说明这一点：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331144958674.png" alt="在这里插入图片描述"><br>在实际使用过程中，我们并不需要了解确定最佳分类平面的原理，只需要记住一个经验法则来确定正确的超平面:“选择能更好地隔离两个类的超平面”。在这个场景中，蓝色超平面“A”出色地完成了这项工作。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类：https://www.kaggle.com/andrewmvd/fetal-health-classification</a></p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150227435.jpg" alt="在这里插入图片描述"></p><p>该数据包含胎儿心电图，胎动，子宫收缩等特征值，而我们所需要做的就是通过这些特征值来对胎儿的健康状况(fetal_health)进行分类。</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150323351.jpg" alt="在这里插入图片描述"></p><p>数据集包含从心电图检查中提取的2126条特征记录，然后由三名产科专家将其分为3类，并用数字来代表：1-普通的，2-疑似病理，3-确定病理。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#划分数据集与测试集</span><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<span class="hljs-comment">#导入算法模块</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<span class="hljs-comment">#导入评分模块</span><br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210326150702818.jpg" alt="在这里插入图片描述"><br>从上面的结果来看，数据中没有缺失值。</p><p><strong>3.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">svm_linear=svm.SVC(C=<span class="hljs-number">10</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<span class="hljs-comment">#参数部分会在下面进行讲解</span><br>svm_linear.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,svm_linear.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm_linear.predict(X_test))))<br></code></pre></td></tr></table></figure><p>引入SVM算法，并将算法中的参数依次设立好，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331145934753.png" alt="在这里插入图片描述"><br>可以看到，该模型的精度为89%左右。</p><p><strong>4.参数</strong></p><p>在这里我们只讲解几个重要的参数，对于其它的参数，朋友们可以自行探究。</p><blockquote><p>sklearn.svm.SVC(C,kernel,degree,gamma,coef0,shrinking,probability,tol,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state)</p></blockquote><p>1.<strong>C</strong> ：惩罚参数，通常默认为1。 C越大，表明越不允许分类出错，但是C越大可能会造成过拟合，泛化效果太低。C越小，正则化越强，分类将不会关注分类是否正确的问题，只要求间隔越大越好，此时分类也没有意义。所以，这其中需要朋友们做一些调整。</p><p>2.<strong>kernel（核函数）</strong> ：核函数的引入是为了解决线性不可分的问题，将分类点映射到高维空间中以后，转化为可线性分割的问题。</p><p>我们用一张表来说明核函数的几个参数：</p><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331152357255.jpg" alt="在这里插入图片描述"><br>这其中最常用的核函数是<strong>Linear核</strong>与<strong>RBF核</strong>：</p><p>（1）<strong>Linear核</strong>：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。</p><p>（2）<strong>RBF核</strong>：主要用于线性不可分的情形，相比其它线性算法来说这也是一个非常突出的一个优点了。无论是小样本还是大样本，高维还是低维，RBF核函数均适用。</p><p>接下来用一段分类边界函数，就能很明显地展现出这两者的区别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br>X, y = make_moons(<span class="hljs-number">200</span>, noise=<span class="hljs-number">0.20</span>)<br>plt.scatter(X[:,<span class="hljs-number">0</span>], X[:,<span class="hljs-number">1</span>], s=<span class="hljs-number">40</span>, c=y, cmap=plt.cm.Spectral)<br>plt.show()<span class="hljs-comment"># 手动生成一个随机的平面点分布，并画出来</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_boundary</span>(<span class="hljs-params">pred_func</span>):</span><br>x_min, x_max = X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.5</span><br>    y_min, y_max = X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>() - <span class="hljs-number">.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>() + <span class="hljs-number">.5</span><br>    h = <span class="hljs-number">0.01</span><br>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br>    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])<br>    Z = Z.reshape(xx.shape)<br>    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)<br>    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y, cmap=plt.cm.Spectral)<br></code></pre></td></tr></table></figure><p>手动设定一个随机的平面分布点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br>clf = svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;linear&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovo&quot;</span>)<br>clf.fit(X, y)<br>plot_decision_boundary(<span class="hljs-keyword">lambda</span> x: clf.predict(x))<br>plt.title(<span class="hljs-string">&quot;SVM-Linearly&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>咱们先来瞄一眼线性核分类对于它的分类效果：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331153225903.png" alt="在这里插入图片描述"><br>接下来看看rbf核对它进行非线性分类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">clf = svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovo&quot;</span>)<br>clf.fit(X, y)<br>plot_decision_boundary(<span class="hljs-keyword">lambda</span> x: clf.predict(x))<br>plt.title(<span class="hljs-string">&quot;SVM-Nonlinearity&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/20210331153412296.png" alt="在这里插入图片描述"><br>从上面两个图可以很轻松的看出来linear核与rbf核的区别，在上面的两个结果中，很明显rbf核函数的模型精度更好一些。但是在运行过程中，却可以感觉到选用rbf核函数比linear核函数运行速度要稍慢一些，并且随着数据量的增大，运行时间是会一直增加的。</p><p>3.<strong>decision_function_shape</strong>:原始的svm只是用于二分类的问题，如果将其扩展到多分类问题，就要采取一定的融合策略，<strong>‘ovo’一对一</strong>，就是两两之间进行划分，<strong>‘ovr’一对多</strong>就是一类与其他类别进行划分。<br>这里重点讲一下<strong>一对多</strong>:对每个类别都学习一个二分类模型，将这个类别与其它类别都尽量分开，这样就生成了与类别个数一样多的二分类模型。在测试点上运行所有二分类分类器来进行预测。在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>SVM是一种二分类模型，处理的数据可以分为三类：</p><p>1.线性可分，通过硬间隔最大化，学习线性分类器，在平面上对应直线<br>2.近似线性可分，通过软间隔最大化，学习线性分类器<br>3.线性不可分，通过核函数以及软间隔最大化，学习非线性分类器，在平面上对应曲线</p><p>而svm比线性模型要优秀的一个点就是可以处理非线性分类数据，甚至是更高维的数据，例如这样的数据分类：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8/2021033116142616.jpg" alt="在这里插入图片描述"></p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>核向量机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>SVM</tag>
      
      <tag>实际应用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>支持向量机（SVM）算法-补充说明</title>
    <link href="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/"/>
    <url>/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>之前我有写过一篇关于svm的使用流程和基本概念讲解——<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>。不过最近又接触了一些关于svm的基础概念和预处理数据的使用，所以在这里做一下简单地补充。在接触本篇文章之前，建议先去看完<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>，一些我之前讲过的东西，这里就不在赘述了。</p><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><p>首先需要声明的一点是，向数据表示中添加非线性特征，可以让线性模型变得更强大。但是，通常来说我们并不知道要添加哪些特征，而添加许多特征（比如100维特征空间所有可能的交互项）的计算开销可能会很大。幸运的是，有一种巧妙的数学技巧，让我们可以在更高维空间中学习分类器，而不用实际计算可能非常大的新的数据表示。这种技巧叫做<strong>核技巧</strong>，它的原理是直接计算特征表示中数据点之间的距离（更准确地说是内积），而不用实际对扩展进行计算。</p><p>对于支持向量机，将数据映射到更高维空间中有两种常用的办法，也就是在<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>中提到过的核参数：一种是<strong>多项式核</strong>，在一定阶数内计算原始特征所有可能的多项式（比如feature1＊<em>2+feature2＊</em>2）；另一种是<strong>径向基函数（RBF）核</strong>，也叫高斯核。高斯核有点难以解释，因为它对应无限维的特征空间。一种对高斯核的解释是它考虑所有阶数的所有可能的多项式，但阶数越高，特征的重要性越小。</p><p>当然在实际中，核svm背后的数学细节并不是很重要。</p><h2 id="理解SVM"><a href="#理解SVM" class="headerlink" title="理解SVM"></a>理解SVM</h2><p>与线性模型相比，核支持向量机（通常简称为SVM）是可以推广到更复杂模型的扩展，这些模型无法被输入线性函数进行定义，譬如下面的数据：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/2021051213473747.png" alt="在这里插入图片描述"><br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512140025880.png" alt="在这里插入图片描述"><br>如上图所示，在训练过程中，SVM学习每个数据点对于表示两个类别之间的决策边界的重要性。通常只有一部分训练数据点对于定义决策边界来说很重要：位于类别之间边界上的那些点。这些点叫做<strong>支持向量</strong>，支持向量机正是由此得名。</p><p>想要对新样本点进行预测，需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性来做出的。</p><p>数据点之间的距离有<strong>高斯核</strong>给出:</p><blockquote><p><strong>k(x1,x2)=exp(-y||x1-x2||^2)</strong><br>x1,x2——数据点；||x1-x2||——表示欧式距离；y（gamma）——控制高斯核宽度的参数</p></blockquote><h2 id="预处理数据-数据缩放"><a href="#预处理数据-数据缩放" class="headerlink" title="预处理数据-数据缩放"></a>预处理数据-数据缩放</h2><p>首先我们先对一个数据集<a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类</a>进行简单的建模，这个数据之前有讲到过，在这里就不在赘述了，具体代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\fetal_health.csv&quot;</span><br>health=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br>train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>svm_Notlinear=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm_Notlinear.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,svm_Notlinear.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm_Notlinear.predict(X_test))))<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512143227271.png" alt="在这里插入图片描述"><br>虽然SVM的表现通常都很好，但它对参数的设定和数据的缩放非常敏感。特别的，它要求所有特征有相似的变化范围。解决这个问题的一种方法就是对每个特征进行缩放，使其大致都位于一个范围。核SVM常用的缩放方法就是将所有特征缩放到0到1之间，实际上MinMaxScaler预处理方法就可以做到这一点，但是为了大家更好地理解，这里我们手动缩放一下，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">range_train=(X_train-X_train.<span class="hljs-built_in">min</span>()).<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">0</span>)<span class="hljs-comment">###计算各个特征值的最大距离</span><br>scaled_train=(X_train-X_train.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>))/range_train<span class="hljs-comment">###看看各个特征维度里面的个体数值距离与最大距离的比，这样所有的特征个体数据都在0到1之间</span><br>scaled_test=(X_test-X_train.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>))/range_train<span class="hljs-comment">###测试集进行同样的缩放，注意是利用训练集缩放测试集</span><br><span class="hljs-comment">##############################################</span><br>svm_Notlinear=svm.SVC(C=<span class="hljs-number">1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>svm_Notlinear.fit(scaled_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,svm_Notlinear.predict(scaled_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;SVM待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,svm_Notlinear.predict(scaled_test))))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512143756876.png" alt="在这里插入图片描述"><br>从上面的结果可以看出，模型精度较之前有了提高。<strong>在实际应用过程中，我们会遇到各种各样的数据，如果我们不能保证数据特征的统一性，那么数据缩放就会取到非常大的作用！</strong></p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>之前我们在<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>中已经讲过各种核函数的适用范围，这里就不在多说了，如下图所示：<br><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512160537252.png" alt="在这里插入图片描述"><br>下面说一下另外两个参数C和gamma。</p><p>gamma是核技巧中给出的公式中的参数，用于控制高斯核的宽度。它决定了点与点之间“靠近”是指多大的距离。C是正则化参数，与线性模型中用到的类似。它限制每个点的重要性。</p><p>gamma较小，说明高斯核的半径较大，许多点都被看作比较接近。小的gamma值表示决策边界变化很慢，生成的是复杂度较低的模型，而大的gamma值则会生成更为复杂的模型。</p><p>至于参数C，与线性模型相同，如果C值很小，说迷你跟模型非常受限，每个数据点的影响范围都有限。</p><p>一般情况下，SVM中默认C=1，gamma=1/n_features。</p><p>接下来，我们来分别调节这两个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_gamma=pd.DataFrame(columns=[<span class="hljs-string">&quot;gamma&quot;</span>,<span class="hljs-string">&quot;SVM训练模型评分&quot;</span>,<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>):<br>    svm_Notlinear=svm.SVC(C=<span class="hljs-number">1</span>,gamma=i/<span class="hljs-number">10</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>    svm_Notlinear.fit(scaled_train,y_train)<br>    result_gamma=result_gamma.append([&#123;<span class="hljs-string">&quot;gamma&quot;</span>:i/<span class="hljs-number">10</span>,<span class="hljs-string">&quot;SVM训练模型评分&quot;</span>:accuracy_score(y_train,svm_Notlinear.predict(scaled_train)),<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>:accuracy_score(y_test,svm_Notlinear.predict(scaled_test))&#125;])<br>result_gamma[result_gamma[<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>]==result_gamma[<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512161459449.png" alt="在这里插入图片描述"><br>接下来，在保证gamma值不变的情况下，调节C参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_C=pd.DataFrame(columns=[<span class="hljs-string">&quot;C&quot;</span>,<span class="hljs-string">&quot;SVM训练模型评分&quot;</span>,<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">500</span>,<span class="hljs-number">10</span>):<br>    svm_Notlinear=svm.SVC(C=i,gamma=<span class="hljs-number">3.1</span>,kernel=<span class="hljs-string">&quot;rbf&quot;</span>,decision_function_shape=<span class="hljs-string">&quot;ovr&quot;</span>)<br>    svm_Notlinear.fit(scaled_train,y_train)<br>    result_C=result_C.append([&#123;<span class="hljs-string">&quot;C&quot;</span>:i,<span class="hljs-string">&quot;SVM训练模型评分&quot;</span>:accuracy_score(y_train,svm_Notlinear.predict(scaled_train)),<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>:accuracy_score(y_test,svm_Notlinear.predict(scaled_test))&#125;])<br>result_C[result_C[<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>]==result_C[<span class="hljs-string">&quot;SVM待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%E7%AE%97%E6%B3%95-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E/20210512162044745.png" alt="在这里插入图片描述"><br>至此，这个SVM的模型就简单地调节好了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>核支持向量机是非常强大的模型，在各种数据集上的表现都很好。SVM允许决策边界很复杂，即使数据只有几个特征。它在低维数据和高维数据（即很少特征和很多特征）上的表现都很好，但对样本个数的缩放表现不好。在有多达10000个样本的数据上运行SVM可能表现良好，但如果数据量达到1000000甚至更大，在运行时间和内存使用方面可能会面临挑战。</p><p>SVM的另一个缺点是，预处理数据和调参都需要非常小心。这也是为什么如今很多应用中用的都是基于树的模型，比如随机森林或梯度提升（需要很少的预处理，甚至不需要预处理）。此外，SVM模型很难检查，可能很难理解为什么会这么预测，而且也很难将模型向非专家进行解释。</p><p>不过SVM仍然是值得尝试的，特别是所有特征的测量单位相似（比如都是像素密度），而且范围也差不多时。</p><p>核SVM的重要参数是正则化参数C，核的选择以及与核相关的参数。虽然我们主要讲的是RBF核，但scikit-learn中还有其它选择。RBF核只有一个参数gamma，它是高斯核宽度的倒数。gamma和C控制的都是模型复杂度，较大的值都对应更为复杂的模型。因此，这两个参数的设定通常是强烈相关的，应该同时调节。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>核向量机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>SVM</tag>
      
      <tag>补充说明</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络（深度学习）算法</title>
    <link href="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/"/>
    <url>/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我想接触过机器学习的人应该都听过一个高大上，但是又非常陌生的算法，就是“<strong>神经网络</strong>”。尤其是最近两年，这类被称为神经网络的算法以“深度学习”的名字再度流行。虽然深度学习在许多机器学习应用中都有非常大的潜力，但深度学习算法往往经过精确调整，只适用于特定的使用场景。接下来，我们只讨论一些相对简单的方法，即用于分类和回归的<strong>多层感知机（MLP）</strong>，它可以作为研究更复杂的深度学习方法的起点。MLP也被称为（普通）前馈神经网络，有时也简称为神经网络。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>神经网络-MLP</strong></p><p><del>PS：接下来要说理论的东西了，很枯燥，已经了解的小伙伴可以直接跳过</del> </p><p>MLP可以被视为广义的线性模型，执行多层处理后得到结论。还记得我之前说过的<a href="https://blog.csdn.net/weixin_43580339/article/details/112271333">线性回归的预测公式</a>：</p><blockquote><p><em><strong>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+…+w[p]x[p]+b</strong></em></p></blockquote><p>简单来说，y是输入特征x[0]到x[p]的加权求和，权重为学到的系数w[0]到w[p]。我们可以将这个公式可视化，如下图所示：<br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525113728442.jpg" alt="在这里插入图片描述"><br>图中，左边的每个点代表一个输入特征，连线代表学到的系数，右边的结点代表输出，是输入的加权求和。</p><p>在MLP中，多次重复这个计算加权求和的过程，首先计算代表中间过程的<strong>隐单元</strong>，然后再计算这些<strong>隐单元</strong>的加权求和并得到最终结果，如下图所示：<br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525114044617.jpg" alt="在这里插入图片描述"><br>毫无疑问，在中间加了一层隐单元之后，模型变得更加精细了。但是这也意味着，这个模型在学习过程中需要考虑更多的系数：在每个输入与每个隐单元（隐单元组成了<strong>隐层</strong>）之间有一个系数，在每个隐单元与输出之间也有一个系数。</p><p>实际上，从数学的角度看，计算一系列加权求和与只计算一个加权求和是完全相同的，因此，为了让这个模型真正比线性模型更为强大，我们还需要一个技巧。在计算完每个隐单元的加权求和之后，对结果再应用一个非线性函数-通常是<strong>校正非线性（也叫校正线性单元或relu）</strong>或<strong>正切双曲线（也叫tanh）</strong>。然后将这个函数的结果用于加权求和，计算得到输出y。<strong>有了这两种非线性函数，神经网络可以学习比线性模型复杂得多的函数</strong>。具体的公式如下所示：</p><blockquote><p>h[0]=tanh(w[0,0]*x[0]+w[1,0]*x[1]+w[2,0]*x[2]+w[3,0]*x[3]+b[0])<br>h[1]=tanh(w[0,0]*x[0]+w[1,0]*x[1]+w[2,0]*x[2]+w[3,0]*x[3]+b[1])<br>h[2]=tanh(w[0,0]*x[0]+w[1,0]*x[1]+w[2,0]*x[2]+w[3,0]*x[3]+b[2])<br>y=v[0]*h[0]+v[1]*h[1]+v[2]*h[2]+b</p></blockquote><p>其中，w是输入x与隐层h之间的权重，v是隐层h与输出y之间的权重。权重w和v要从数据中学习得到，x是输入特征，y是计算得到的输出，h是计算的中间结果。需要用户设置的一个重要参数是隐层中的结点个数。对于非常小或非常简单的数据集，这个值可以小到10；对于非常复杂的数据，这个值可以大到10000。当然，模型中可以添加多个隐层，如下图所示：<br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525115842644.png" alt="在这里插入图片描述"><br><strong>这些由许多计算层组成的大型神经网络，正是术语“深度学习”的灵感来源。</strong></p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/sakshigoyal7/credit-card-customers">信用卡客户分类：https://www.kaggle.com/sakshigoyal7/credit-card-customers</a><br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525131658157.png" alt="在这里插入图片描述"><br>该数据包括信用卡客户的性别，年龄，收入，消费金额，学历等维度的数据，而我们所需要做的就是通过这些特征值来区分有哪些客户还在继续使用信用卡，哪些已经放弃使用，</p><p><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210326150323351.jpg" alt="在这里插入图片描述"></p><p>数据集包含23个特征维度，10127条特征记录，然后客户分为2类：Existing Customer-还在使用的客户，Attrited Customer-不使用的客户。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<span class="hljs-comment">#多层感知机-MLP</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\BankChurners.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/sakshigoyal7/credit-card-customers</span><br>Credit_Card=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/2021052513235077.jpg" alt="在这里插入图片描述"></p><p>从上面的结果来看，数据中没有缺失值。但是有三列数据需要注意，特征CLIENTNUM代表客户编号，而最后两列只是源数据作者的贝叶斯分类计算的草稿，这三列属于无用的特征，可以直接删掉，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Credit_Card=Credit_Card.drop(Credit_Card.columns[[<span class="hljs-number">0</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>]],axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>由于维度”Gender”,”Education_Level”,”Marital_Status”,”Income_Category”,”Card_Category”中的数值均是以字符文字的形式存在的，如果直接建模的话会报错，所以我们需要将其中的分类数据用数字进行替代，譬如在性别”Gender”中，用1代表F（男性），2代表M（女性），代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;Gender&quot;</span>,<span class="hljs-string">&quot;Education_Level&quot;</span>,<span class="hljs-string">&quot;Marital_Status&quot;</span>,<span class="hljs-string">&quot;Income_Category&quot;</span>,<span class="hljs-string">&quot;Card_Category&quot;</span>]:<br>    m=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(Credit_Card[i]))<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m)):<br>        Credit_Card.loc[Credit_Card[i]==m[x],i]=x<span class="hljs-comment">###loc[行，列]</span><br></code></pre></td></tr></table></figure><p><strong>3.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train=Credit_Card.drop([<span class="hljs-string">&quot;Attrition_Flag&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,Credit_Card[<span class="hljs-string">&quot;Attrition_Flag&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mlp=MLPClassifier(solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,random_state=<span class="hljs-number">1</span>).fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mlp训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,mlp.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mlp待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,mlp.predict(X_test))))<span class="hljs-comment">###警告是增大最大迭代次数，或者对数据进行标准化</span><br></code></pre></td></tr></table></figure><p>引入MLP算法，并将算法中的参数依次设立好，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525133125120.jpg" alt="在这里插入图片描述"><br>可以看到，该模型的精度为84%左右。<br>注意：红色警告是算法提示要增大最大迭代次数，或者对数据进行标准化，这些问题我们稍后解决。</p><p><strong>4.数据标准化</strong></p><p>可以看出MLP的精度还可以，但没有其它模型好。与之前说的<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">svm</a>一样，原因就在于数据需要缩放。神经网络也要求所有输入特征的变化范围相似，最理想的情况是均值为0，方差为1，也就是标准化。我们必须对数据进行缩放以满足这些要求。为了让大家更好地理解，这里我们先人工进行处理（以后可以用StandardScaler来完成），代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">mean_on_train=X_train.mean(axis=<span class="hljs-number">0</span>)<span class="hljs-comment">##计算每个特征的平均值</span><br>std_on_train=X_train.std(axis=<span class="hljs-number">0</span>)<span class="hljs-comment">##计算每个特征的标准差</span><br>X_train_scald=(X_train-mean_on_train)/std_on_train<span class="hljs-comment">##减去平均值，然后乘以标准差的倒数，之后，mean=0，std=1</span><br>X_test_scald=(X_test-mean_on_train)/std_on_train<span class="hljs-comment">##对测试集做同样的处理（使用训练集的平均值和标准差）</span><br></code></pre></td></tr></table></figure><p>之后，我们对缩放完成的数据进行建模，代码及结果如下所示：</p><p><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525135819269.jpg" alt="在这里插入图片描述"><br>可以看出模型精度较之前有了提高，达到了93%。不仅如此，我还设定了最大迭代次数参数max_iter。实际上增加迭代次数仅提高了训练集性能，而不会提高泛化性能。不过由于数据缩放，所以模型的表现较之前有了提升。由于训练性能和测试性能之间仍有一定差距，甚至是出现了<strong>过拟合</strong>的情况，所以我们可以通过调整其它参数，来提高模型的泛化性能。</p><p><strong>5.参数</strong></p><p>通常来说，对于模型MLP的定义主要需要4个参数：<strong>隐层数，每层的结点，正则化和非线性函数</strong>的选择。当然还有一个问题是，如何学习模型或用来学习参数的算法，这一点由<strong>solver</strong>参数设定。接下来我们依次讲解这些参数。</p><p><em><strong>1.solver</strong></em></p><p>solver有两个好用的选项。默认选项是“adam”，在大多数情况下效果都很好，但对数据的缩放相当敏感（因此，始终将数据缩放为均值为0，方差为1是很重要的）。另一个选项是“lbfgs”，其鲁棒性相当好，但在大型模型或者大型数据集上的时间会比较长。还有更高级的“sgd”选项，许多深度学习研究人员都会用到。“sgd”选项还有许多其它参数需要调节，以便获得最佳结果。不过一般情况下，“adma”和“lbfgs”已经足够满足我们的日常使用。</p><p><em><strong>2.activation</strong></em></p><p>该参数表示的是之前我们提到过的<strong>非线性校正函数（relu/tanh）</strong>，默认参数是relu。调参后的结果如下所示：</p><p><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525142018105.jpg" alt="在这里插入图片描述"><br>可以看到结果较之前反而有所下降，既然如此，我们依旧选择relu参数。</p><p><em><strong>3.hidden_layer_sizes</strong></em></p><p>该参数代表我们之前提到过的<strong>隐结点的个数和层数</strong>。默认情况下，MLP使用100个隐结点（hidden_layer_sizes=[100]）。这对于一个小型数据集来说已经相当多了，我们可以通过减少隐结点数量，增加隐层数，来降低模型复杂度，提高模型的泛化能力，代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mlp=MLPClassifier(solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,activation=<span class="hljs-string">&quot;relu&quot;</span>,random_state=<span class="hljs-number">1</span>,hidden_layer_sizes=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>],max_iter=<span class="hljs-number">100000</span>).fit(X_train_scald,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mlp训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,mlp.predict(X_train_scald))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mlp待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,mlp.predict(X_test_scald))))<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525142729331.jpg" alt="在这里插入图片描述"><br>hidden_layer_sizes=[10,5]的意思是该模型有两个隐层数，第一层有10个隐结点，第二层有5个隐结点，最后所得结果较之前也有了些提高。当然，感兴趣的小伙伴还可以试试设立三个隐层，以及其它数量的隐结点（如hidden_layer_sizes=[10,10,5]），看看结果较之前有没有变化。</p><p><em><strong>4.alpha</strong></em></p><p>正如我们在<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">岭回归</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">线性分类器</a>中所做的那样，MLPClassifier中正则化调节L2惩罚系数的参数是alpha（与线性回归模型中的相同），它的默认值很小（弱正则化），下面我们通过遍历来选取一个合适的alpha参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result=pd.DataFrame(columns=[<span class="hljs-string">&quot;alpha&quot;</span>,<span class="hljs-string">&quot;mlp训练模型评分&quot;</span>,<span class="hljs-string">&quot;mlp待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>):<br>    m=i/<span class="hljs-number">10</span><br>    mlp=MLPClassifier(solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,activation=<span class="hljs-string">&quot;tanh&quot;</span>,random_state=<span class="hljs-number">1</span>,hidden_layer_sizes=[<span class="hljs-number">10</span>,<span class="hljs-number">5</span>],alpha=m,max_iter=<span class="hljs-number">100000</span>).fit(X_train_scald,y_train)<br>    result=result.append([&#123;<span class="hljs-string">&quot;alpha&quot;</span>:m,<span class="hljs-string">&quot;mlp训练模型评分&quot;</span>:accuracy_score(y_train,mlp.predict(X_train_scald)),<span class="hljs-string">&quot;mlp待测模型评分&quot;</span>:accuracy_score(y_test,mlp.predict(X_test_scald))&#125;])<br>result[result[<span class="hljs-string">&quot;mlp待测模型评分&quot;</span>]==result[<span class="hljs-string">&quot;mlp待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%AE%97%E6%B3%95/20210525143758114.jpg" alt="在这里插入图片描述"><br>结果如上所示，当我们将alpha参数设立为1.9时，可以得到mlp的待测模型精度96%，较之前又有了些许提升。</p><p>现在可能大家已经意识到了，控制神经网络复杂度的方法有很多种：隐层的个数，每个隐层中的单元个数与正则化（alpha）。实际上还有很多，但这里就不在过多介绍了。</p><p>除了上述的参数之外，神经网络还有一个重要参数是<strong>random_state</strong>。在开始学习之前，神经网络的权重是随机设置的。也就是说，即使使用完全相同的参数，如果随机种子不同的话，我们也可能得到非常不一样的模型。如果网络很大，并且复杂度选择合理的话，那么这应该不会对精度有太大影响，但应该记住这一点（特别是对于较小的网络）。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在机器学习的许多应用中，神经网络再次成为最先进的模型。它的主要优点之一是能够获取大量数据中包含的信息，并且构建无比复杂的模型。给定足够的计算时间和数据，并且仔细调节参数，神经网络通常可以打败其它机器学习算法（无论是分类任务还是回归任务）。</p><p>这就引出了下面要说的缺点。神经网络——特别是功能强大的大型神经网络——通常需要很长的训练时间。它还需要仔细地预处理数据，正如我们这里所看到的。与SVM类似，神经网络在“均匀”数据上的性能最好，其中“均匀”是指所有特征都具有相似的含义。如果数据包含不同种类的特征，那么基于树的模型（<a href="https://blog.csdn.net/weixin_43580339/article/details/116231286">随机森林</a>/<a href="https://blog.csdn.net/weixin_43580339/article/details/116522395">梯度提升回归树</a>）可能表现的更好。不过神经网络调参本身也是一门艺术。</p><p>神经网络调参的常用方法是，首先创建一个大到足以过拟合的网络，确保这个网络可以对任务进行学习。在知道训练数据可以被学习之后，要么缩小网络，要么增大alpha来增强正则化，这可以提高泛化性能。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>朴素贝叶斯分类器</title>
    <link href="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>鲁棒性</strong></p><p>Huber从稳健统计的角度系统地给出了鲁棒性3个层面的概念：</p><p>1.是模型具有较高的精度或有效性，这也是对于机器学习中所有学习模型的基本要求；<br>2.是对于模型假设出现的较小偏差，只能对算法性能产生较小的影响，如噪声；<br>3.是对于模型假设出现的较大偏差，不可对算法性能产生“灾难性”的影响，如离群点。</p><p><del>PS：上面的解释是从网上抄来的</del> </p><p>定义：对于聚类（分类）算法而言，鲁棒性意味着聚类结果不应受到模型中存在的数据扰动、噪声及离群点的太大影响。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>朴素贝叶斯分类器</strong></p><p>朴素贝叶斯分类器是与之前提到的<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">逻辑算法（LogisticRegression)</a>等线性模型非常相似的一种分类器，但它的训练速度往往更快。它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。</p><p>python的第三方库scikit-learn中实现了三种朴素贝叶斯分类器：<br>GaussianNB（高斯分类器）:可应用于任意连续数据。<br>BernoulliNB（伯努利分类器）:假定输入数据为二分类数据。<br>MultinomialNB（多项式分类器）:假定输入数据为计数数据（即每个特征代表某个对象的整数计数）。</p><p>GaussianNB主要用于高维度数据，而另外两种朴素贝叶斯分类模型则广泛用于稀疏计数数据，比如文本。</p><p>MultinomialNB与GaussianNB计算的统计数据类型略有不同。MultinomialNB计算每个类别中每个特征的平均值，而GaussianNB会保存每个类别中每个特征的平均值和标准差。</p><p>对于朴素贝叶斯分类器来说，要想作出预测，需要将数据点与每个类别的统计数据做比较，并将最匹配的类别作为预测结果。有意思的是，MultinomialNB与BernoulliNB预测公式的形式都与线性模型完全相同。</p><p>本文主要介绍伯努利和多项式分类器，而高斯分类器的用法可以类比之前的其它线性算法，如<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192">lasso</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">ridge</a>等。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/uciml/mushroom-classification">蘑菇的可食用性分类：https://www.kaggle.com/uciml/mushroom-classification</a></p><p><img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409111459802.jpg" alt="在这里插入图片描述"><br>该集合以文本的形式记录了多种蘑菇的菌盖，菌柄，菌丝等部位的颜色，宽度，长度数据。<br>数据集一共有8124条特征记录，22个特征维度，并对这些蘑菇划分成两类（class）：e——可食用；p——有毒的。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> BernoulliNB<span class="hljs-comment">#伯努利贝叶斯分类器</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\mushrooms.csv&quot;</span><br>mushrooms=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409112527819.png" alt="在这里插入图片描述"></p><p>从上面的结果来看，数据中没有缺失值。</p><p><strong>3.建模</strong></p><p>首先将待测的分类特征（有毒与可食用）划分出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train=mushrooms.drop([<span class="hljs-string">&quot;class&quot;</span>],axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>由于这个数据是以文本的形式记录的，直接建模的话，算法会识别不出来，所以需要把其中的<strong>文本数据依次替换成数字</strong>，如特征维度菌盖（cap-color）包含十种颜色，将这十种颜色分别以数字0-9替换表示，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> train.columns:<br>    a=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(train[i]))<span class="hljs-comment">###set是获得去重元素值</span><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(a)):<br>        train[i].loc[train[i]==a[m]]=m<br></code></pre></td></tr></table></figure><p>替换后结果如下所示：<img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409113003550.png" alt="在这里插入图片描述"></p><p>接下来，划分列索引为特征值和预测值，并将数据划分成训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train,X_test,y_train,y_test=train_test_split(train,mushrooms[<span class="hljs-string">&quot;class&quot;</span>],random_state=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>引入伯努利算法，并将算法中的参数依次设立好，进行建模后，对测试集进行精度评分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<span class="hljs-comment">###引入时间模块，接下来的讨论会用到</span><br>start=time.time()<br>clf=BernoulliNB(alpha=<span class="hljs-number">10</span>)<br>train_prediction=clf.fit(X_train,y_train).predict(X_train)<br>test_prediction=clf.fit(X_train,y_train).predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;伯努利贝叶斯训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,train_prediction)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;伯努利贝叶斯待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,test_prediction)))<br>end=time.time()<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;运行时间：&quot;</span>+<span class="hljs-built_in">str</span>(end-start))<span class="hljs-comment">#时间单位是秒</span><br></code></pre></td></tr></table></figure><p>得到的结果如下：<br><img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409113528393.png" alt="在这里插入图片描述"><br>可以看到，该模型的精度为90%左右，运行时间是0.14s。</p><p><strong>4.参数</strong></p><p>MultinomialNB与BernoulliNB都只有一个参数alpha，用于控制模型复杂度。</p><p>alpha的工作原理：算法向数据中添加alpha这么多的虚拟数据点，这些点对所有特征都取正值。这可以将统计数据“平滑化”。alpha越大，平滑化就越强，模型复杂度就越低。</p><p>朴素贝叶斯分类器的算法性能对alpha值的鲁棒性相对较好，也就是说，alpha值对模型性能并不重要，但调用这参数通常都会使精度略有提高。<br>接下来，我们看一下模型精度与alpha之间的关系，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> MultinomialNB<span class="hljs-comment">#多项式贝叶斯分类器</span><br>result=pd.DataFrame(columns=[<span class="hljs-string">&quot;参数&quot;</span>,<span class="hljs-string">&quot;伯努利训练模型得分&quot;</span>,<span class="hljs-string">&quot;伯努利待测模型得分&quot;</span>,<span class="hljs-string">&quot;多项式训练模型得分&quot;</span>,<span class="hljs-string">&quot;多项式待测模型得分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">300</span>):<br>    Bernoulli=BernoulliNB(alpha=i).fit(X_train,y_train)<br>    Multinomial=MultinomialNB(alpha=i).fit(X_train,y_train)<br>    result=result.append([&#123;<span class="hljs-string">&quot;参数&quot;</span>:i,<span class="hljs-string">&quot;伯努利训练模型得分&quot;</span>:accuracy_score(y_train,Bernoulli.predict(X_train)),<span class="hljs-string">&quot;伯努利待测模型得分&quot;</span>:accuracy_score(y_test,Bernoulli.predict(X_test)),<span class="hljs-string">&quot;多项式训练模型得分&quot;</span>:accuracy_score(y_train,Multinomial.predict(X_train)),<span class="hljs-string">&quot;多项式待测模型得分&quot;</span>:accuracy_score(y_test,Multinomial.predict(X_test))&#125;])<br></code></pre></td></tr></table></figure><p>以折线图的形式展现出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<span class="hljs-comment">#设定汉字字体，防止出现方框</span><br>%matplotlib inline<br><span class="hljs-comment">#在jupyter notebook上直接显示图表</span><br>fig= plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">5</span>))<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;伯努利训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;伯努利训练模型得分&quot;</span>)<span class="hljs-comment">#画折线图</span><br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;伯努利待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;伯努利待测模型得分&quot;</span>)<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;多项式训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;多项式训练模型得分&quot;</span>)<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;多项式待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;多项式待测模型得分&quot;</span>)<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: <span class="hljs-number">15</span>&#125;)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的刻度字体大小</span><br>plt.yticks(fontsize=<span class="hljs-number">15</span>)<br>plt.xlabel(<span class="hljs-string">&quot;参数&quot;</span>,fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的标签内容和字体</span><br>plt.ylabel(<span class="hljs-string">&quot;得分&quot;</span>,fontsize=<span class="hljs-number">15</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409115953984.png" alt="在这里插入图片描述"><br>从图中可以看出MultinomialNB（多项式）比BernoulliNB（伯努利）的性能和稳定性要好很多，而对于BernoulliNB来说，模型性能会随着alpha的增加而逐渐下降。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>朴素贝叶斯模型的许多优点和缺点都与线性模型相同。它的训练和预测速度都很快，训练过程也很容易理解。但这种高效率所付出的代价就是，朴素贝叶斯模型的泛化能力要比线性分类器（如<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">逻辑算法（LogisticRegression)</a>）稍差。<br>接下来，用线性模型LogisticRegression算法来建模，与朴素贝叶斯做个比较，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br>start=time.time()<br>logistic=LogisticRegression(penalty=<span class="hljs-string">&#x27;l2&#x27;</span>,C=<span class="hljs-number">1</span>,solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,max_iter=<span class="hljs-number">1000</span>)<br>train_prediction=logistic.fit(X_train,y_train).predict(X_train)<br>test_prediction=logistic.fit(X_train,y_train).predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,train_prediction)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,test_prediction)))<br>end=time.time()<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;运行时间：&quot;</span>+<span class="hljs-built_in">str</span>(end-start))<br></code></pre></td></tr></table></figure><p>结果如下：<br><img src="/2021/06/15/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/20210409131522166.png" alt="在这里插入图片描述"><br>从上面结果可以看出，对于同样的数据，逻辑算法的模型精度要优于朴素贝叶斯算法，但是运行时间是0.96秒，比朴素贝叶斯的运行时间要多了7倍左右，而随着数据量达到万级，甚至继续增加，这个差距会越来越大。</p><p>总的来说，朴素贝叶斯分类器对高维稀疏数据的效果很好，对参数的鲁棒性也相对较好，是很好的基准模型，可用于非常大的数据集，毕竟在这些数据集上即使训练线性模型可能也要花费大量的时间。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>回归模型</tag>
      
      <tag>朴素贝叶斯分类器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>岭回归</title>
    <link href="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/"/>
    <url>/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>正则化</strong></p><p><strong>正则化</strong>是指对模型做显式约束，以避免过拟合。本文用到的岭回归就是L2正则化。（从数学的观点来看，岭回归惩罚了系数的L2范数或w的欧式长度）</p><p>正则化的具体原理就不在这里多叙述了，感兴趣的朋友可以看一下这篇文章：<a href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a>。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>岭回归</strong></p><p>岭回归也是一种用于回归的线性模型，因此它的模型公式与最小二乘法的相同，如下式所示：</p><blockquote><p><strong>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+……+w[p]x[p]+b</strong></p></blockquote><p>但在岭回归中，对系数w的选择不仅要在训练数据上得到很好的预测结果，而且还要拟合附加约束。换句话说，w的所有元素都应接近于0。直观上来看，这意味着每个特征对输出的影响应尽可能小（即斜率很小），同时仍给出很好的预测结果，这个约束也就是<strong>正则化</strong>。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/altavish/boston-housing-dataset">波士顿房价：https://www.kaggle.com/altavish/boston-housing-dataset</a><br><del>也是非常经典的一个数据</del><br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121135007262.jpg" alt="在这里插入图片描述"></p><p>简单解释一下这个数据的几个主要指标：<br>ZN：25,000平方英尺以上的土地划为住宅用地的比例。<br>RM：每个住宅的平均房间数。<br>AGE：1940年之前建造的自有住房的比例<br>CHAS：有没有河流经过 (如果等于1，说明有，等于0就说明没有)<br>CRIM：犯罪率<br>MEDV：住房的价格<br><del>其它指标就不用说了，都是一些住房的其它指标，感兴趣的小伙伴可以自己查一下。</del> </p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Ridge<span class="hljs-comment">###导入岭回归算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块<br><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\HousingData.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>house_price=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/altavish/boston-housing-dataset</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p><p><strong>3.清洗数据</strong><br><em>1.查找缺失值</em><br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121135832795.png" alt="在这里插入图片描述"></p><p>可以看到这个数据并包括一些缺失值，并不是很多，所以直接删掉就好了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">house_price1=house_price.dropna().reset_index()<br><span class="hljs-keyword">del</span> house_price1[<span class="hljs-string">&quot;index&quot;</span>]<br></code></pre></td></tr></table></figure><p><em>2.突变值查找</em></p><p><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121140155149.png" alt="在这里插入图片描述"><br>一般是看看特征值里面是否包含等于零的数据。其实说的直接一点就是看看数据里面是否包含不符合实际的数值，比如像是犯罪率，实际中不可能出现犯罪率等于0的片区。那么从上面的结果来看，这份数据并没有其它问题。<br><del>这份数据里面的ZN和CHAS都是利用0和1来当作一种指标，所以包含0是很正常的。</del><br><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">train=house_price1.drop([<span class="hljs-string">&quot;MEDV&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,house_price1[<span class="hljs-string">&quot;MEDV&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">#将MEDV划分为预测值，其它的属性划分为特征值，并将数据划分成训练集和测试集。</span><br>ridge=Ridge(alpha=<span class="hljs-number">10</span>)<span class="hljs-comment">#确定约束参数</span><br>ridge.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;岭回归训练模型得分：&quot;</span>+<span class="hljs-built_in">str</span>(r2_score(y_train,ridge.predict(X_train))))<span class="hljs-comment">#训练集</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;岭回归待测模型得分：&quot;</span>+<span class="hljs-built_in">str</span>(r2_score(y_test,ridge.predict(X_test))))<span class="hljs-comment">#待测集</span><br></code></pre></td></tr></table></figure><p>引入ridge算法，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121142032926.png" alt="在这里插入图片描述"></p><p>可以看到，该模型的训练精度为79%左右，对于新的数据来说，模型精度在63%左右。<br>至此，这个数据集的将建模就算是完成了。</p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p><strong>1.参数的讨论</strong></p><p>由于岭回归与线性回归（最小二乘法）的模型公式是一样的，所以这里我们与线性回归做一个比较。不了解线性回归的朋友可以看一下我的另一篇文章：<a href="https://blog.csdn.net/weixin_43580339/article/details/112271333">最小二乘算法之回归实操</a><br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121142126579.png" alt="在这里插入图片描述"></p><p>之前我们设立的约束参数是10，而上面模型参数设的是0，可以看出模型的训练精度有所提高，但泛化能力有所降低。同时与线性回归模型相比，二者的分数是完全一样的。所以，当岭回归的约束参数设为0时，失去约束的岭回归与普通最小二乘法就是同一个算法。</p><p><strong>2.与普通最小二乘法的比较</strong></p><p>我们通过变换约束参数的取值，来具体看一下岭回归与普通最小二乘法的优缺点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">result_b=pd.DataFrame(columns=[<span class="hljs-string">&quot;参数&quot;</span>,<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>,<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>,<span class="hljs-string">&quot;线性回归训练模型得分&quot;</span>,<span class="hljs-string">&quot;线性回归待测模型得分&quot;</span>])<br>train=house_price1.drop([<span class="hljs-string">&quot;MEDV&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,house_price1[<span class="hljs-string">&quot;MEDV&quot;</span>],random_state=<span class="hljs-number">23</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">21</span>):<br>    alpha=i/<span class="hljs-number">10</span><span class="hljs-comment">#约定参数可以选定为小数</span><br>    ridge=Ridge(alpha=alpha)<br>    ridge.fit(X_train,y_train)<br>    linear=LinearRegression()<br>    linear.fit(X_train,y_train)<br>    result_b=result_b.append([&#123;<span class="hljs-string">&quot;参数&quot;</span>:alpha,<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>:r2_score(y_train,ridge.predict(X_train)),<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>:r2_score(y_test,ridge.predict(X_test)),<span class="hljs-string">&quot;线性回归训练模型得分&quot;</span>:r2_score(y_train,linear.predict(X_train)),<span class="hljs-string">&quot;线性回归待测模型得分&quot;</span>:r2_score(y_test,linear.predict(X_test))&#125;])<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121143002294.png" alt="在这里插入图片描述"><br>可以看出如果只是针对训练模型的精度，最小二乘法是要优于岭回归的，但是对新的数据作出预测时，也就是考虑模型的泛化能力上，可以看出岭回归的模型得分比最小二乘法要好一点。<br>我们通过一个折线图来更直观地表现上面的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<span class="hljs-comment">#设定汉字字体，防止出现方框</span><br>%matplotlib inline<br><span class="hljs-comment">#在jupyter notebook上直接显示图表</span><br>fig= plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">5</span>))<br>plt.plot(result_b[<span class="hljs-string">&quot;参数&quot;</span>],result_b[<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>)<span class="hljs-comment">#画折线图</span><br>plt.plot(result_b[<span class="hljs-string">&quot;参数&quot;</span>],result_b[<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>)<br>plt.plot(result_b[<span class="hljs-string">&quot;参数&quot;</span>],result_b[<span class="hljs-string">&quot;线性回归训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;线性回归训练模型得分&quot;</span>)<br>plt.plot(result_b[<span class="hljs-string">&quot;参数&quot;</span>],result_b[<span class="hljs-string">&quot;线性回归待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;线性回归待测模型得分&quot;</span>)<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: <span class="hljs-number">12</span>&#125;)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的刻度字体大小</span><br>plt.yticks(fontsize=<span class="hljs-number">15</span>)<br>plt.xlabel(<span class="hljs-string">&quot;参数&quot;</span>,fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的标签内容和字体</span><br>plt.ylabel(<span class="hljs-string">&quot;得分&quot;</span>,fontsize=<span class="hljs-number">15</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E5%B2%AD%E5%9B%9E%E5%BD%92/20210121144729267.png" alt="在这里插入图片描述"><br>可以看出岭回归模型在模型的简单性（系数都接近于0）与训练集性能之间作出权衡。简单性和训练性能二者对于模型的重要程度可以由用户通过设置aplha参数来制定。增大alpha会使得系数更加趋向于0，从而降低训练集性能，但会提高泛化性能。</p><p>而且无论是岭回归还是线性回归，所有数据集大小对应的训练分数都要高于预测分数。由于岭回归是正则化的，所以它的训练分数要整体低于线性回归的训练分数。但岭回归的测试分数高，特别是对于较小的数据集。如果数据量小于一定程度的时候，线性回归将学不到任何内容，随着模型可用数据越来越多，两个模型的性能都在提升，最终线性回归的性能追上了岭回归。所以如果有足够多的训练内容，<strong>正则化</strong>变得不那么重要，并且岭回归和线性回归将具有相同的性能。</p><p>以上就是关于岭回归的实际操作与看法了，有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>回归模型</tag>
      
      <tag>L2正则化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>决策树算法-单棵树（下）</title>
    <link href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <url>/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>在上篇的文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198?spm=1001.2014.3001.5501">决策树算法之讲解实操（上）</a>当中，我们主要了解了决策树的算法原理，实际应用，以及简单介绍了下决策树的复杂度参数。而这篇我们主要讲解决策树的分析可视化，特征值重要程度，以及讨论回归决策树。</p><h2 id="决策树的分析与可视化"><a href="#决策树的分析与可视化" class="headerlink" title="决策树的分析与可视化"></a>决策树的分析与可视化</h2><p>树的可视化有助于深入理解算法是如何进行预测的，也是易于向非专家解释的机器学习算法的优秀示例。我们可以利用tree模块的export_graphviz函数来将树可视化。这个函数会生成一个.dot格式的文件，这是一种用于保存图形的文本文件格式。我们设置为结点添加颜色的选项，颜色表示每个结点中的多数类别，同时传入类别名称和特征名称，这样可以对树进行标记，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> export_graphviz<br>classifier=DecisionTreeClassifier(random_state=<span class="hljs-number">1</span>,max_depth=<span class="hljs-number">3</span>)<span class="hljs-comment">###为了图形好看一点，这里就选择深度为3</span><br>train_prediction=classifier.fit(X_train,y_train)<span class="hljs-comment">###这里要单独列出来模型，作为下面代码的输入参数，所以平时不要频繁地使用方法链，不然报错会很麻烦</span><br>tree_dot=export_graphviz(train_prediction,out_file=<span class="hljs-literal">None</span>,feature_names=train.columns,impurity=<span class="hljs-literal">False</span>,filled=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>PS：数据是来自于上一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198?spm=1001.2014.3001.5501">决策树算法之讲解实操（上）</a>当中的<a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒数据</a>，不了解的朋友可以去看一看，具体的我就不在这里多讲了。</p><p>为了让图形更容易观察，我这里将树的深度调整为了3。当然，在实际调参过程中，为了保证模型的精度，树的深度肯定不只是3。</p><p>接下来我们利用graphviz模块读取这个文件并将其可视化（当然你也可以用其它可以读取.dot文件的程序）。</p><p>与其它的第三方库的安装不同，要想使用graphviz模块，还需要再单独下载安装，并且配置环境变量，具体安装流程可以参考这篇文章：<a href="https://www.cnblogs.com/hankleo/p/9733076.html">Graphviz安装</a>（ps：安装graphviz，并且配置完环境变量后，建议重启一下python程序，不然没法马上识别出来dot文件）。</p><p>那么在安装好了之后，运行下面的代码就能可视化决策树了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> graphviz<span class="hljs-comment">###https://www.cnblogs.com/hankleo/p/9733076.html</span><br>graph=graphviz.Source(tree_dot)<br>graph.render(view=<span class="hljs-literal">True</span>, <span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;pdf&quot;</span>, filename=<span class="hljs-string">&quot;decisiontree_pdf&quot;</span>)<span class="hljs-comment">###以pdf文件展示</span><br>graphviz.Source(tree_dot)<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421113152156.jpg" alt="在这里插入图片描述"><br>实际上，即使这棵树的深度只有3层，看起来也还是有点大的。而深度更大的树（深度为10并不罕见）更加难以理解。这个时候，就有一种观察法可以提升效率了，那就是<strong>找出大部分数据的实际路径</strong>。譬如在上面的图形中，<strong>寻找samples占比最高的数据路径</strong>。像是第三层的samples=514，samples=416这两个结点，然后顺着这两个结点继续向下观察samples=400，samples=306这两个结点。几乎大部分数据都是顺着流程进入这几个结点，而其它叶结点都只包含很少的样本。</p><h2 id="树的特征重要性"><a href="#树的特征重要性" class="headerlink" title="树的特征重要性"></a>树的特征重要性</h2><p>实际上就算将整棵树可视化进行观察，也是非常费劲的。所以除此之外，我们还可以利用一些有用的属性来总结树的工作原理。其中最常用的就是特征重要性，它为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于0到1之间的数字，其中0表示“根本没用到”，1表示”完美预测目标值“，而特征重要性的求和始终为1。代码如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421114704556.png" alt="在这里插入图片描述"><br>我们可以将特征重要性可视化：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421114806519.png" alt="在这里插入图片描述"><br>我们可以看到，在这些特征当中，酒精含量（alcohol）是影响红酒质量的最重要的因素。</p><p>但是如果某个特征的feature_importance_很小，并不能说明这个特征没有提供任何信息。只能说明该特征没有被树选中，可能是因为另一个特征也包含了同样的信息。</p><p>与线性模型的系数不同，特征重要性始终为正数，也不能说明该特征对应哪个类别。<strong>特征重要性告诉我们酒精含量（alcohol）特征很重要，但并没有告诉我们含量大表示红酒质量高或者是低。事实上，在特征和类别之间可能没有这样简单的关系</strong>。</p><h2 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h2><p>虽然我们主要讨论的是用于分类的决策树，但对用于回归的决策树来说，所有内容都是类似的。在DecisionTreeRegressor中实现，回归树的用法和分析与分类树非常类似。但在将基于树的模型用于回归时，我们想要指出它的一个特殊性质：<strong>那就是DecisionTreeRegressor（以及其他所有基于树的回归模型）不能外推，也不能在训练数据范围之外进行预测</strong>。</p><p>我们通过一个简单的对数求和例子来更详细地研究这点。代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>lab=pd.DataFrame()<br>lab[<span class="hljs-string">&quot;数列&quot;</span>]=np.arange(<span class="hljs-number">1</span>,<span class="hljs-number">30</span>)<br>lab[<span class="hljs-string">&quot;数列对数&quot;</span>]=np.log(lab[<span class="hljs-string">&quot;数列&quot;</span>])<br>lab[<span class="hljs-string">&quot;求和&quot;</span>]=lab[<span class="hljs-string">&quot;数列&quot;</span>]+lab[<span class="hljs-string">&quot;数列对数&quot;</span>]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421120210592.png" alt="在这里插入图片描述"><br>实际上，上述数据集的设定就是：一列是顺序数列，一列是顺序数列的对数，还有一列就是前两列的求和，它的回归线就是：<strong>y=x+log(x)<strong>，如下图所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421120642636.png" alt="在这里插入图片描述"><br>如果是利用线性模型来去做回归预测是非常容易的，并且模型精度会很高，这里就不再赘述了。那么接下来我们用决策树的回归算法进行建模，并分别对</strong>训练数据集内和训练数据集外</strong>的数据进行预测，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<span class="hljs-comment">#决策树回归</span><br>X_train=lab.iloc[:<span class="hljs-number">15</span>,<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]<br>X_test=lab.iloc[<span class="hljs-number">15</span>:,<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]<br>Y_train=lab.loc[:<span class="hljs-number">14</span>,<span class="hljs-string">&quot;求和&quot;</span>]<br>tree_regressor=DecisionTreeRegressor().fit(X_train,Y_train)<span class="hljs-comment">###训练集模型</span><br>tree_train_prediction=tree_regressor.predict(X_train)<span class="hljs-comment">###数据范围内预测</span><br>tree_test_prediction=tree_regressor.predict(X_test)<span class="hljs-comment">###数据范围外预测</span><br></code></pre></td></tr></table></figure><p>将预测结果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<span class="hljs-comment">#设定汉字字体，防止出现方框</span><br>%matplotlib inline<br><span class="hljs-comment">#在jupyter notebook上直接显示图表</span><br>fig= plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">5</span>))<br>plt.plot(lab[<span class="hljs-string">&quot;数列&quot;</span>],lab[<span class="hljs-string">&quot;求和&quot;</span>],label=<span class="hljs-string">&quot;数据回归线&quot;</span>)<br>plt.plot(X_train[<span class="hljs-string">&quot;数列&quot;</span>],tree_train_prediction,label=<span class="hljs-string">&quot;决策树训练集结果&quot;</span>)<br>plt.plot(X_test[<span class="hljs-string">&quot;数列&quot;</span>],tree_test_prediction,label=<span class="hljs-string">&quot;外推测试集结果&quot;</span>)<br>plt.legend()<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: <span class="hljs-number">15</span>&#125;)<br>plt.xticks(fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的刻度字体大小</span><br>plt.yticks(fontsize=<span class="hljs-number">15</span>)<br>plt.xlabel(<span class="hljs-string">&quot;数列&quot;</span>,fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的标签内容和字体</span><br>plt.ylabel(<span class="hljs-string">&quot;求和&quot;</span>,fontsize=<span class="hljs-number">15</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8B%EF%BC%89/20210421121055686.jpg" alt="在这里插入图片描述"><br>可以看出树模型完美预测了训练数据集（红线与蓝线重合的部分）。由于我们没有限制树的复杂度，因此它记住了整个数据集。但是，一旦输入超出训练集之外的数据，模型就只能持续预测最后一个已知数据点（黄线）。<strong>树不能在训练数据的范围之外生成”新的“响应，所有基于树的回归模型都有这个缺点。</strong></p><p>实际上，利用基于树的模型可以做出非常好的预测。上述例子的目的并不是说明决策树是一个不好的模型，而是为了说明树在预测方式上的特殊性质。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如上篇文章所述，控制决策树模型复杂度的参数是预剪枝参数，它在树完全展开之前停止树的构造。通常来说，选择一种预剪枝策略（设置max_depth,max_leaf_nodes或min_samples_leaf）足以防止过拟合。</p><p>与前面讨论过的许多算法相比，决策树有两个优点：一是得到的模型很容易可视化，非专家也很容易理解（至少对于较小的树来说）；二是算法完全不受数据缩放影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，比如归一化或标准化。特别是特征的尺度不一样时或者二元特征和连续特征同时存在时，决策树效果很好。</p><p>决策树的主要缺点在于，即使做了预剪枝，它也会经常过拟合，泛化能力较差。因此，在大多数应用中，往往绘制用集成方法来替代单颗决策树。集成方法我们会在以后进行介绍。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>决策树</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>单棵树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>决策树算法-单棵树（上）</title>
    <link href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <url>/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>这次讲解机器学习里面非常经典的一个算法模型——<strong>分类树</strong>。由于篇幅比较长，所以特分为上下两篇讲解。本篇主要讲解决策树的原理，实际应用以及参数。</p><h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p><strong>1.分类树原理</strong></p><p>决策树是广泛应用于分类和回归任务的模型。本质上，它从一层层的if/else问题中进行学习，并得出结论。</p><p>想像一下，你想要区分下面四种动物：熊，鹰，企鹅和海豚。你的目标是通过提出尽可能少的if/else问题来得到正确答案。而这个提问过程可以表示为一棵决策树，如下图所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414153543772.png" alt="在这里插入图片描述"></p><p>在这张图中，树的每个结点代表一个问题或一个包含答案的终结点（也叫<strong>叶结点</strong>）。树的边将问题的答案与将问的下一个问题连接起来。</p><p>用机器学习的语言来说就是，为了区分四类动物（鹰，企鹅，海豚和熊），我们利用三个特征（“有没有羽毛”，“会不会飞”和“有没有鳍”）来构建一个模型。我们利用监督学习从数据中学习模型，而无需人为构造模型。</p><p><strong>2.构造分类树</strong></p><p>学习分类树，就是学习一系列if/else问题，使我们能够以最快的速度得到正确答案。在机器学习中，这些问题叫做<strong>测试</strong>（PS：不要与测试集搞混，测试集是用来测试模型泛化性能的数据）。数据通常并不像动物的例子那样具有二元特性（是/否）的形式，而是表现为连续特征。而为了构造决策树，算法会搜遍所有可能的测试，找出对目标变量来说信息量最大的那个。<br>我们以一个测试为例，如下图所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414140912176.png" alt="在这里插入图片描述"></p><p>将数据集在x[1]=0.0596处垂直划分可以得到最多信息，它在最大程度上将类别0中的点与类别1中的点进行区分。顶节点（也叫<strong>根结点</strong>）表示整个数据集，包含属于类别0的50个点和属于类别1的50个点。通过测试x[1]&lt;=0.0596的真假来对数据集进行划分。</p><p>如果测试结果为真，那么将这个点分配给左结点，左结点里面包含属于类别0的2个点和属于类别1的32个点。否则将这个点分配给右结点，右结点里面包含属于类别0的48个点和属于类别1的18个点。</p><p>尽管第一次划分已经对两个类别做了很好的区分，但底部区域仍包含属于类别0的点，顶部区域也仍包含属于类别1的点。所以我们可以在两个区域中重复寻找最佳测试的过程，从而构建出更准确的模型。如下图所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414141646424.png" alt="在这里插入图片描述"><br>开始时递归过程生成一棵二元决策树，其中的每个结点都包含一个测试。或者可以将每个测试看成沿着一条轴对当前数据进行划分。这是一种将算法看作分层划分的观点。</p><p>接下来对数据反复进行递归划分，直到划分后的每个区域（决策树的每个叶结点）只包含单一目标值（单一类别或单一回归值）。如果树中某个叶结点所包含数据点的目标值相同，那么这个叶结点就是<strong>纯的</strong>。</p><p>要想对新数据点进行预测，首先要查看这个点位于特征空间划分的哪个区域，然后将该区域的多数目标值（如果是纯的叶结点，就是单一目标值）作为预测结果。从根结点开始对树进行遍历就可以找到这一区域，每一步向左还是向右取决于是否满足相应的测试。</p><p>决策树也可以用于回归任务，使用的方法相同。预测的方法是，基于每个结点的测试对树进行遍历，最终找到新数据点所属的叶结点。这一数据的输出即为此叶结点中所有训练点的平均目标值。</p><p>接下来，我们开始用真实数据进行建模操作。</p><p><strong>scikit_learn</strong>的决策树在<strong>DecisionTreeRegressor</strong>类和<strong>DecisionTreeClassifier</strong>类中实现。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒质量分类：https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</a></p><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414143349525.png" alt="在这里插入图片描述"><br>该数据一共有1596条数据记录，包含挥发性酸度，柠檬酸，发酵后残留糖份等11个特征值，而我们所需要做的就是通过这些特征值来对红酒的质量(quality )进行分类，该数据中的质量以得分0到10之间来表示。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<span class="hljs-comment">#决策树回归</span><br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<span class="hljs-comment">#决策树分类器</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\winequality-red.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</span><br>red_wine=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/2021041414390177.png" alt="在这里插入图片描述"><br>从上面的结果来看，数据中没有缺失值。</p><p><strong>3.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">train=red_wine.drop([<span class="hljs-string">&quot;quality&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,red_wine[<span class="hljs-string">&quot;quality&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>classifier=DecisionTreeClassifier(random_state=<span class="hljs-number">1</span>)<br>train_prediction=classifier.fit(X_train,y_train).predict(X_train)<br>test_prediction=classifier.fit(X_train,y_train).predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;决策树分类器训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,train_prediction)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;决策树分类器待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,test_prediction)))<br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><p>引入决策树分类算法，并将算法中的参数设立好，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414144037342.png" alt="在这里插入图片描述"><br>可以看到，该模型的精度为63%左右。</p><p><strong>4.参数-控制决策树的复杂度</strong></p><p><strong>random_state</strong>: sklearn可以选择建不同的树，在每次分枝时不使用全部特征，随机选取一部分特征，而random_state参数决定特征选择的随机性，类似random.seed()。</p><p>通常来说，构造决策树知道所有的叶结点都是纯的叶结点，这会导致模型非常复杂，并且对训练数据高度过拟合。纯叶结点的存在说明这颗树在训练集上的精度是100%（如上面图片结果所示）。训练集中的每个数据点都位于分类正确的叶结点中。</p><p>防止过拟合有两种常见策略：一种是及早停止树的生长，也叫<strong>预剪枝</strong>；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫<strong>后剪枝</strong>或<strong>剪枝</strong>。预剪枝的限制条件可能包括限制树的最大深度，限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。</p><p>scikit-learn只实现了预剪枝，没有实现后剪枝。</p><p>我们固定一下树的<strong>random_state</strong>，限制树的深度<strong>max_depth</strong>，放在上面的数据集上重新建模，结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-%E5%8D%95%E6%A3%B5%E6%A0%91%EF%BC%88%E4%B8%8A%EF%BC%89/20210414151720918.png" alt="在这里插入图片描述"></p><p>与之前的结果相比，可以看出如果我们不限制决策树的深度，它的深度和复杂度都可以变得特别大。因此，未剪枝的树容易过拟合，对新数据的泛化性能不佳。</p><p>如果我们尝试将剪枝应用在决策树上，这可以在完美拟合训练数据之前阻止树的展开。其中一种选择是在达到一定深度之后停止树的展开。这里我们设置了max_depth=13，这意味着只可以连续问13个问题（详见之前的算法介绍图片）。限制树的深度可以减少过拟合。这会降低训练集的精度，但可以提高测试集的精度，如上图结果所示。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总的来说，决策树是一种非常经典的算法模型，它既可以用于分类问题，也可以用于回归问题。</p><p>下一篇我会讲一下如何分析决策树，以及决策树的优缺点。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p><p>PS：看过之前几篇文章的朋友应该已经发现了个现象，就是所有的机器学习算法建模的过程中， 除了算法不同，其它部分的代码都大致相同，这应该属于搬砖吧，是不是所有的机器学习或者说是数据挖掘都是这样的？ヾ(￣ー￣)</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>决策树</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>单棵树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>决策树集成-随机森林</title>
    <link href="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <url>/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    
    <content type="html"><![CDATA[<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>集成</strong></p><p>集成是合并多个机器学习模型来构建更强大模型的方法。在机器学习算法中有许多模型属于这一类，但已证明有两种集成模型对大量分类和回归的数据集都是有效的，二者都以决策树为基础，分别是<strong>随机森林（random forest）</strong>和<strong>梯度提升决策树决策（gradiet boosted decision tree）</strong>。</p><p>本片文章先讲解一下随机森林。在了解随机森林之前建议先去看一下我的另外两篇讲解<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树的文章决策树算法之讲解实操（上）</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/115939923">决策树算法之讲解实操（下）</a>，重复的东西，我这里就不在赘述了。<br>ps：接下来会花费很长的篇幅来讲解随机森林的思想和构造原理，已经有所了解的小伙伴可以直接跳过。</p><h2 id="思想简介"><a href="#思想简介" class="headerlink" title="思想简介"></a>思想简介</h2><p>在之前的一篇文章<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树的文章决策树算法之讲解实操（上）</a>中我们提到过，决策树的一个主要缺点在于经常对训练数据过拟合。那么随机森林就是解决这个问题的一种方法。随机森林本质上是许多决策树的集合，其中每棵树都和其它的树略有不同。</p><p>它的思想是：<strong>每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对树的结果取平均值来降低过拟合</strong>。这样做，既能减少过拟合又能保持树的预测能力。</p><p>为了实现上面的思想，我们需要构造很多决策树，并且每棵树都与其它的树保持不同，即树的<strong>随机化</strong>。而树的随机化方法有两种：一种是通过选择用于构造树的数据点，另一种是通过选择每次划分测试的特征。接下来，我们来更深入地讲解这一块。</p><h2 id="构造原理"><a href="#构造原理" class="headerlink" title="构造原理"></a>构造原理</h2><p>想要构造一个随机森林模型，需要确定用于构造的树的个数，即RandomForestClassifier（分类算法）或RandomForestRegressor（回归算法）的<strong>n_estimators参数</strong>。比如我们想要构造10棵树，这些树在构造时彼此完全独立，算法对每棵树进行不同的随机选择。而想要构造一棵树，首先要对数据进行<strong>自助采样</strong>。也就是说，从n个数据点中有放回地重复抽取一个样本（同一样本可以被多次抽取），共抽取n次。</p><p>举例说明，比如我们想要创建列表[a,b,c,d]的自助采样。一种可能的结果是[b,d,d,c]，另一种可能的结果是[d,a,d,a]。</p><p>接下来，基于这个新创建的数据集来构造决策树。在每个结点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试（注意并不是对每个结点都寻找最佳测试）。选择的特征个数由<strong>max_features</strong>参数来控制。每个结点中特征子集的选择都是相互独立的，这样树的每个结点可以使用特征的不同子集来作出决策。</p><p><strong>总之，由于使用了自助采样，随机森林中构造每棵决策树的数据集都是略有不同的。由于每个结点的特征选择，每棵树中的每次划分都是基于特征的不同子集这两种方法共同保证随机森林中所有树都不同</strong>。</p><p>想要利用随机森林进行预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了“软投票”策略。也就是说，每个算法作出“软预测”，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。</p><h2 id="实操建模"><a href="#实操建模" class="headerlink" title="实操建模"></a>实操建模</h2><p>数据是一份<a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">红酒质量分类</a>的数据集，通过各个维度来判断红酒质量，之前在<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>中已经讲解使用过了，这里就不多在赘述了，我们直接建模，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<span class="hljs-comment">#随机森林分类器</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\winequality-red.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</span><br>red_wine=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br>train=red_wine.drop([<span class="hljs-string">&quot;quality&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,red_wine[<span class="hljs-string">&quot;quality&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br>forest=RandomForestClassifier(n_estimators=<span class="hljs-number">50</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###n_estimators树的个数</span><br>train_prediction=forest.fit(X_train,y_train).predict(X_train)<br>test_prediction=forest.fit(X_train,y_train).predict(X_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;随机森林分类器训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,train_prediction)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;随机森林分类器待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,test_prediction)))<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428134640544.jpg" alt="在这里插入图片描述"><br>下面是之前的文章中单棵决策树建立的模型结果：<br><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428134820111.jpg" alt="在这里插入图片描述"><br>二者相比可以看出，随机森林的模型精度要比单棵树的要好上不少，过拟合现象也比之前要减轻很多。<br>接下来我们了解一下随机森林的主要模型参数。</p><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><p>在RandomForestClassifier中，我们主要会用到三个模型参数n_estimators（树的个数），max_depths（树的深度）,max_features（随机特征数），它们对模型的影响程度依次递减。至于其它的参数，一般情况下直接默认就好。</p><p><strong>n_estimators</strong>：这个参数总是越大越好，对更多的树取平均值可以降低过拟合，从而得到鲁棒性更好的集成。不过收益是递减的，而且树越多需要的内存也越多，训练的时间也越长。常用的经验法就是“<strong>在时间/内存允许的情况下尽量多</strong>”。</p><p>接下来我们来调节这个参数，提高模型精度，代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_1=pd.DataFrame(columns=[<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>,<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">500</span>,<span class="hljs-number">10</span>):<br>    forest=RandomForestClassifier(n_estimators=i,random_state=<span class="hljs-number">1</span>)<br>    forest.fit(X_train,y_train)<br>    result_1=result_1.append([&#123;<span class="hljs-string">&quot;集成树的个数(n_estimators)&quot;</span>:i,<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>:accuracy_score(y_test,forest.predict(X_test))&#125;])<br>result_1[result_1[<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>]==result_1[<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428135903273.jpg" alt="在这里插入图片描述"><br>可以看到当我们设定参数n_estimators为141或者151的时候，模型精度可以达到73%左右，较之前的结果提高了一些。</p><p><strong>max_depths</strong>：通过调节这个参数可以像单棵决策树那样进行预剪枝，当然，这个参数默认情况下就是最大深度，一般不需要调节。</p><p><strong>max_features</strong>：这个参数决定每棵树的随机性大小，较小的话可以降低过拟合，一般来说，好的经验就是使用默认值。</p><p>不过为了演示，我们依然可以调节这个参数，代码及结果如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_1=pd.DataFrame(columns=[<span class="hljs-string">&quot;最大特征数量(max_features)&quot;</span>,<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>):<br>    forest=RandomForestClassifier(n_estimators=<span class="hljs-number">151</span>,max_features=i,random_state=<span class="hljs-number">1</span>)<br>    forest.fit(X_train,y_train)<br>    result_1=result_1.append([&#123;<span class="hljs-string">&quot;最大特征数量(max_features)&quot;</span>:i,<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>:accuracy_score(y_test,forest.predict(X_test))&#125;])<br>result_1[result_1[<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>]==result_1[<span class="hljs-string">&quot;随机森林分类器待测模型评分&quot;</span>].<span class="hljs-built_in">max</span>()]<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428141704422.jpg" alt="在这里插入图片描述"><br>正如我之前提到的，这个参数主要是用来限制树的最大随机特征数量的。那么，如果我们设置max_features等于特征值的数量（在上面的红酒质量数据集中是11），接下来每次划分都要考虑数据集的所有特征，也就意味着在特征选择过程中没有添加随机性（不过自助采样依然存在随机性）。如果设置max_features等于1，那么在划分时将无法选择对哪个特征进行测试，只能对随机选择的某个特征搜索不同的阈值。因此，<strong>如果max_features较大，那么随机森林中的树将会十分相似，利用最独特的特征可以轻松地拟合数据。如果max_features较小，那么随机森林中的树将会差异很大，为了更好地拟合数据，每棵树的深度都要很大</strong>。当然，实际过程中，它的默认参数通常就已经可以给出很好的结果了。</p><p><strong>对于分类，默认值是max_features=sqrt（维度个数）；<br>对于回归，默认值是max_features=维度个数。</strong></p><h2 id="分析特征重要性"><a href="#分析特征重要性" class="headerlink" title="分析特征重要性"></a>分析特征重要性</h2><p>与决策树类似，随机森林也可以给出特征重要性，计算方法是将森林中所有树的特征重要性求和并取平均。一般来说，随机森林给出的特征重要性要比单棵树给出的更为可靠。代码及结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<br>%matplotlib inline<br>tree=DecisionTreeClassifier(max_depth=<span class="hljs-number">3</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###单棵树分类器</span><br>forest=RandomForestClassifier(max_depth=<span class="hljs-number">3</span>,random_state=<span class="hljs-number">1</span>)<span class="hljs-comment">###随机森林分类器</span><br>tree_prediction=tree.fit(X_train,y_train)<br>forest_prediction=forest.fit(X_train,y_train)<br>fig= plt.subplots(figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">15</span>))<br>fig1 = plt.subplot(<span class="hljs-number">211</span>)<br>plt.title(<span class="hljs-string">&#x27;决策树分类器特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,tree_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;blue&quot;</span>)<br>plt.legend()<br>fig2=plt.subplot(<span class="hljs-number">212</span>)<br>plt.title(<span class="hljs-string">&#x27;随机森林分类器特征重要性&#x27;</span>,fontsize=<span class="hljs-number">20</span>)<br>plt.bar(train.columns,forest_prediction.feature_importances_,<span class="hljs-number">0.4</span>,color=<span class="hljs-string">&quot;green&quot;</span>)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">13</span>)<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/20210428143909169.jpg" alt="在这里插入图片描述"></p><p>如上图所示，在保证树的深度参数（max_depth）相同的情况下，与单棵树相比，随机森林中有更多特征的重要性不为零。由于构造随机森林过程中的随机性，算法需要考虑多种可能的解释，结果就是随机森林比单棵树更能从总体把握数据的特征性。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>用于分类和回归的随机森林是目前应用最广泛的机器学习方法之一。这种方法十分强大，通常不需要反复调节参数就可以给出很好的结果，也不需要对数据进行缩放。</p><p>从本质上看，随机森林拥有决策树的所有优点，同时弥补了决策树的一些缺陷。而实际中，我们仍然需要使用决策树的一个根本原因就是需要决策过程的紧凑表示。实际过程中，基本上不可能对几十棵甚至上百棵树作出详细解释，同时随机森林中树的深度往往比决策树还要大（因为用到了特征子集）。因此，如果你需要以可视化的方式想非专家总结预测过程，那么选择单棵树可能更好。虽然在大型数据集上构建随机森林可能比较费时间，但在一台计算器上用多个内核并行计算也很容易。</p><p>随机森林本质上是随机的，设置不同的状态（或者不设置random_state）可以彻底改变构建的模型。森林中的树越多，它对随机状态选择的鲁棒性（<a href="https://blog.csdn.net/weixin_43580339/article/details/115541911">朴素贝叶斯分类器之分类实操</a>这篇文章介绍了鲁棒性的概念）也就越好。如果你希望结果可以重现，固定random_state是很重要的。</p><p>如果是分析维度非常高的稀疏数据，比如文本数据，随机森林的表现往往不是很好。对于这种数据，使用线性模型（<a href="https://blog.csdn.net/weixin_43580339/article/details/115541911">朴素贝叶斯分类器之分类实操</a>或者<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）算法之分类实操</a>）可能更合适。即使是非常大的数据集，随机森林的表现通常也很好。不过随机森林需要更大的内存，训练和预测的速度也比线性模型要慢。对于一个应用来说，如果时间和内存很重要的话，那么换用线性模型可能更为明智。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>决策树集成</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>随机森林</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分类器不确定度估计，监督学习算法小结</title>
    <link href="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/"/>
    <url>/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>我们之前用到的所有机器学习的算法均来自于scikit—learn库，但是这个接口还有另一个用处，就是能够给出分类器预测结果的不确定性估计。有的时候，我们不仅要关心一个测试数据点究竟属于哪个类别，还要考虑这个预测的置信区间。譬如，在最近新冠疫情中出现的无症状感染，如果是假阳性预测，那么可能只会让患者接受额外的测试，但是如果是假阴性感染却有可能导致患者没有得到治疗。<strong>（</strong>机器学习的大部分算法均是建立在概率统计的基础上的，而概率等于99.9%却并不意味着事件一定会发生。<strong>）</strong></p><p>scikit—learn中有两个函数可用于获取分类器的不确定估计：<strong>decision_function和predict_proba</strong>。大多数分类器都至少有其中一个函数，很多分类器两个都有。接下来我们找一个数据集来构造一个<a href="https://blog.csdn.net/weixin_43580339/article/details/116522395">梯度提升回归树</a>的分类器。（这个算法之前有讲过，不了解的小伙伴可以跳转到相应的文章去了解下。）</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/brsdincer/star-type-classification">星星分类：https://www.kaggle.com/brsdincer/star-type-classification</a></p><p><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602143118338.png" alt="在这里插入图片描述"><br>这个数据集并不大，只有240条数据记录，其中包括表面温度，颜色等特征，<strong>一共有6种星星（0-5）</strong>，而我们所需要做的就是通过这些特征值来对星星(Type)进行分类。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库并读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<span class="hljs-comment">#梯度提升回归树</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-comment">###################</span><br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\Stars.csv&quot;</span><span class="hljs-comment">###https://www.kaggle.com/brsdincer/star-type-classification</span><br>stars=pd.read_csv(file_origin)<br><span class="hljs-comment">#设立桌面绝对路径，读取源数据文件，这样将数据直接下载到桌面上就可以了，省得还要去找</span><br><span class="hljs-comment">###################</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，并读取文件。</p><p><strong>2.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/2021060214350325.png" alt="在这里插入图片描述"><br>从上面的结果来看，数据中没有缺失值。但是数据中除了数字之外，还包括字符串，如果直接用字符串建模的话会报错，所以接下来我用数字依次替代字符串，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;Color&quot;</span>,<span class="hljs-string">&quot;Spectral_Class&quot;</span>]:<br>    a=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(stars[i]))<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(a)):<br>        stars.loc[stars[i]==a[m],i]=m<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602143718488.png" alt="在这里插入图片描述"><br>好的，那么接下来我们开始构建模型。</p><p><strong>3.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train=stars.drop([<span class="hljs-string">&quot;Type&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,stars[<span class="hljs-string">&quot;Type&quot;</span>],random_state=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">gbcf=GradientBoostingClassifier(random_state=<span class="hljs-number">1</span>)<br>gbcf.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,gbcf.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;梯度提升回归树待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,gbcf.predict(X_test))))<br></code></pre></td></tr></table></figure><p>引入梯度提升决策树算法，并且让算法中的参数保持默认值就好。进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/2021060214393314.png" alt="在这里插入图片描述"></p><p>可以看到，该模型的精度为96%左右。当然，接下来我们还可以通过调参，来提高模型精度，不过本文主要讲解不确定度的问题，所以调参这部分就暂时先不讲了，有兴趣的朋友可以去<a href="https://blog.csdn.net/weixin_43580339/article/details/116522395">决策树集成-梯度提升回归树之分类实操</a>看一下</p><h2 id="不确定度估计"><a href="#不确定度估计" class="headerlink" title="不确定度估计"></a>不确定度估计</h2><p><strong>decision_function</strong></p><p>对于多分类的情况，<strong>decision_function</strong>的形状为（n_samples,n_classes）,每一列对应每个类别的“确定度分数”，分数较高的类别可能性更大，得分较低的类别可能性较小，代码及结果如下所示：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602144903332.png" alt="在这里插入图片描述"><br>这里我们只看前5个数据点，可以看到每一个用中括号包括起来的一行就是一个数据点在6个分类中的得分，分数越高，属于该类别的概率就越高。譬如，第一行：“[-10.22462681, -10.21235435, -10.24571233, -10.00954012,7.89471131, -10.32103418]”中第五个数值最高，所以第一个数据点属于类别4的星星概率最高。</p><p>当然，我们还可以利用argmax找出每个数据点的最大元素，以及这些元素所对应的索引，从而利用这些分数再现预测结果，代码及结果如下所示：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602145721262.png" alt="在这里插入图片描述"><br>这一串数字就代表每个数据点所对应的类别，可以看到前五个数据分别属于类别4，1，2，3，4。</p><p><strong>predict_proba</strong></p><p>与decision_function不同，<strong>predict_proba</strong>的输出是每个类别的概率，通常比decision_function更容易理解。同时它的输出形状也是（n_samples,n_classes）。</p><p><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/2021060215150622.png" alt="在这里插入图片描述"><br>如上图所示，每行的第一个元素是第一个类别的估计概率，第二个元素是第二个类别的估计概率，依次类推。由于predict_proba的输出是一个概率，因此总是在0到1之间，元素之和始终为1。由于概率之和为1，因此只要有一个类别的概率超过50%，那么这个类别就是模型的预测结果。（ps：由于概率是浮点数，所以二分类问题中不太可能有两个0.5000，但是如果出现了这种情况，那么预测的结果就是随机的。）</p><p>同样地，我们可以通过计算predict_proba的argmax来再现预测结果，结果如下所示：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602152013254.png" alt="在这里插入图片描述"><br>一般情况下，如果你想要对比predict的结果与predict_proba或decision_function的结果，那么我们可以考虑用分类器的classes_属性来获取真实的属性名称：<br><img src="/2021/06/15/%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1%EF%BC%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/20210602152202569.png" alt="在这里插入图片描述"><br>这里可以看出gbcf的6种分类类别的真实名称0，1，2，3，4，5。（说实话在这个数据集选取的并不好，因为是以数字来当作分类类别的，不太容易区分。）</p><h2 id="监督学习算法小结"><a href="#监督学习算法小结" class="headerlink" title="监督学习算法小结"></a>监督学习算法小结</h2><p>时间过得很快，三个多月前我开始接触机器学习，并且开始坚持一周学习一个算法，找其它数据练手，并且写下文章来记录。到目前为止，一些基础的监督学习算法已经全部完结了。接下来，我们来对这些算法做一个简单地总结，并附上链接，方便及时复习：</p><p><strong>最近邻</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法分类</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112005984">k邻近算法回归</a>）<br>适用于小型数据集,是很好的基准模型,很容易解释。</p><p><strong>线性模型</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/112271333">最小二乘法</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">岭回归</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192">lasso回归</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112277248">LogisticRegression算法</a>）<br>非常可靠的首选算法,适用于非常大的数据集,也适用于高维数据。</p><p><strong>朴素贝叶斯</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/115541911">朴素贝叶斯分类器</a>）<br>只适用于分类问题。比线性模型速度还快,适用于非常大的数据集和高维数据。精度通常要低于线性模型。</p><p><strong>决策树</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/115696198">决策树算法之讲解实操（上）</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/115939923">决策树算法之讲解实操（下）</a>）<br>速度很快,不需要数据缩放,可以进行可视化,很容易解释。</p><p><strong>随机森林</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/116231286">决策树集成-随机森林之分类实操</a>）<br>几乎总是比单棵决策树的表现要好,鲁棒性很好,非常强大。不需要数据缩放。不适用于高维稀疏数据。</p><p><strong>梯度提升决策树</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/116522395">决策树集成-梯度提升回归树之分类实操</a>）<br>精度通常比随机森林略高。与随机森林相比,训练速度更慢,但预测速度更快,需要的内存也更少。比随机森林需要更多的参数调节。</p><p><strong>支持向量机</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/115350097">支持向量机（SVM）</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/116704969">补充说明</a>）<br>对于特征含义相似的中等大小的数据集很强大。需要数据缩放,对参数敏感。</p><p><strong>神经网络</strong>（<a href="https://blog.csdn.net/weixin_43580339/article/details/117256873">神经网络（深度学习）算法之分类实操</a>）<br>可以构建非常复杂的模型,特别是对于大型数据集而言。对数据缩放敏感,对参数选取敏感。大型网络需要很长的训练时间</p><p>我们之前有讨论了模型复杂度,然后讨论了泛化,或者说学习一个能够在前所未见的新数据上表现良好的模型。这就引出了<strong>欠拟合</strong>和<strong>过拟合</strong>的概念,前者是指一个模型无法获取数据中的所有变化,后者是指模型过分关注训练数据,但对新数据的泛化性能不好。</p><p>在我们讨论了一系列用于分类和回归的机器学习模型之后，就会发现对于许多算法而言,设置正确的参数对模型性能至关重要。有些算法还对输入数据的表示方式很敏感,特别是特征的缩放。因此,如果盲目地将一个算法应用于数据集,而不去理解模型所做的假设以及参数设定的含义,不太可能会得到精度高的模型。</p><p>通常我们在面对新数据集的时候，最好先从简单的模型开始，比如线性模型，朴素贝叶斯或最近临分类器，看看能得到什么效果。在对数据有了进一步的了解之后，我们再进一步考虑用于构建更复杂模型的算法，比如随机森林，梯度提升决策树，SVM或神经网络。</p><h2 id="学习分享"><a href="#学习分享" class="headerlink" title="学习分享"></a>学习分享</h2><p>其实在日常生活中，博主很喜欢逛知乎，当时决定要转行学习数据分析之前，也是到知乎，论坛上找了很多资料，然后看了很多人提问题，比如说“数据分析的发展前景怎么样？”，“学数据分析需要看什么书？”，“有没有大佬说一下课程推荐，或者是学习计划？”，甚至还有“学数据分析要不要买台好一点的电脑？”这样的问题。而截至到今天我已经学习了3个多月了，每周坚持更新一篇博文，然后等到我今天再去逛知乎，论坛的时候，发现当时在问类似问题的人，现在还在问这些问题，而并没有开始行动。虽然我并不知道那些人是不是营销号，但我希望他们是。希望看到这篇文章的人可以先做再说。</p><p>最后我有一句话送给打算努力或者正在努力的人，这句话也是我目前学习的动力之一，那就是：<br><strong>“心不妥协，行不受限，德智之高，莫若于知行合一。”</strong></p><p>与君共勉</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>分类器不确定度估计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>算法小结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LogisticRegression算法</title>
    <link href="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/"/>
    <url>/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>先简单介绍一下机器学习里面的两个概念</p><p><strong>1.损失函数</strong></p><p>损失函数是机器学习里最基础也是最为关键的一个要素，它的作用就是衡量模型预测的好坏。<br>我们举个简单地例子来说明这个函数：</p><p>假设我们对一家公司的销售情况进行建模，分别得出了实际模型和预测模型，这两者之间的差距就是损失函数，可以用绝对损失函数来表示：</p><p><strong>L(Y-f(X))=|Y-f(X)|——公式Y-实际Y的绝对值</strong></p><p>对于不同的模型，损失函数也不尽相同，比如使用平方损失函数代替绝对损失函数：</p><p><strong>L(Y-f(X))=（Y-f(X))^2——公式Y-实际Y的平方</strong></p><p>损失函数是很好的反映模型与实际数据差距的工具，理解损失函数能够更好得对后续优化工具（梯度下降等）进行分析与理解。很多时候遇到复杂的问题，最难的一关就是如何写出损失函数。</p><p><strong>2.正则化</strong></p><p>正则化是指对模型做显式约束，以避免过拟合。正则化的具体原理就不在这里多叙述了，感兴趣的朋友可以看一下这篇文章：<a href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解。</a></p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><em><strong>LogisticRegression</strong></em></p><p>该模型的预测公式如下：</p><blockquote><p><em>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+……+w[p]x[p]+b&gt;0</em></p></blockquote><p>这个公式看起来和线性回归的公式非常相似。虽然LogisticRegression的名字中含有回归（Regression），但它是一种分类算法，并不是回归算法，不要与LinearRegression混淆。在这个公式中我们并没有返回特征的加权求和，而是为预测设置了阈值（0）。如果函数值小于0，我们就预测类别为-1，如果函数值大于0，我们就预测类别+1。对于所有用于分类的线性模型，这个预测规则都是通用的。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/andrewmvd/fetal-health-classification">胎儿健康分类：https://www.kaggle.com/andrewmvd/fetal-health-classification</a></p><p><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326150227435.jpg" alt="在这里插入图片描述"></p><p>该数据包含胎儿心电图，胎动，子宫收缩等特征值，而我们所需要做的就是通过这些特征值来对胎儿的健康状况(fetal_health)进行分类。</p><p><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326150323351.jpg" alt="在这里插入图片描述"></p><p>数据集包含从心电图检查中提取的2126条特征记录，然后由三名产科专家将其分为3类，并用数字来代表：1-普通的，2-疑似病理，3-确定病理。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#划分数据集与测试集</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<span class="hljs-comment">#导入算法模块</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<span class="hljs-comment">#导入评分模块</span><br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块。</p><p><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\avocado.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>glass=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/andrewmvd/fetal-health-classification</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p><p><strong>3.清洗数据</strong></p><p>查找缺失值：<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326150702818.jpg" alt="在这里插入图片描述"><br>从上面的结果来看，数据中没有缺失值。</p><p><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">###考虑到接下来可能需要进行其它的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">logistic=LogisticRegression(penalty=<span class="hljs-string">&#x27;l2&#x27;</span>,C=<span class="hljs-number">1</span>,solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>,max_iter=<span class="hljs-number">1000</span>)<br>logistic.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,logistic.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,logistic.predict(X_test))))<br></code></pre></td></tr></table></figure><p>引入LogisticRegression算法，并将算法中的参数依次设立好，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326163658784.jpg" alt="在这里插入图片描述"></p><p>可以看到，该模型的精度为88%左右。</p><p><strong>5.参数讲解</strong></p><p>在这里我们只讲解几个重要的参数，对于其它的参数，朋友们可以自行探究。</p><blockquote><p>LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0,fit_intercept=True, intercept_scaling=1, class_weight=None,random_state=None, solver=’liblinear’, max_iter=100,multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)</p></blockquote><p>1.<strong>penalty</strong>：正则化项的选择。正则化主要有两种：L1和L2，默认选择L2正则化。（相较于<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842?spm=1001.2014.3001.5502">ridge</a>和<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192?spm=1001.2014.3001.5502">lasso</a>这两个默认正则化的线性算法来说，自由选择正则化是这个算法的一大优点。）</p><p>2.<strong>C</strong> ：正则化强度（正则化系数λ）的倒数; 必须是大于0的浮点数。 与支持向量机一样，较小的值指定更强的正则化，通常默认为1。</p><p>3.<strong>solver</strong> ：‘newton-cg’，‘lbfgs’，‘liblinear’，‘sag’，’saga’。默认参数：liblinear<br>liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。<br>lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。<br>newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。<br>sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。<br>saga：线性收敛的随机优化算法。</p><p>4.<strong>max_iter</strong>：求解器收敛的最大迭代次数。（通常来说越大越好）</p><p><strong>6.调参</strong></p><p><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326164552794.png" alt="在这里插入图片描述"><br>1.在用这个算法进行分类的时候，偶尔会出现这样的警告，大概的意思就是最大迭代次数无法满足solver参数的收敛，这个时候我们除了选择另外一个参数之外，还可以通过增加最大迭代次数来解决这个问题。<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326165110326.png" alt="在这里插入图片描述"><br>从上图可以看到除了警告消失了，相较于之前的运行结果，模型的精度有所上升。当然增加最大迭代次数并不会无限提高模型精度。当提高到某个程度，满足了solver的参数的收敛之后，模型精度便不再提升。</p><p>2.参数C代表的是正则化的强度，数值越小，正则化越强。<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326165650125.png" alt="在这里插入图片描述"><br>3.solver参数</p><p>我觉得solver参数最大的优点，就是用一个参数很好的区分了二分类与多分类的问题，并且进行了优化。<br>简单解释一下这两类问题：判断某个女生是否喜欢你，它只会回答你喜欢或者不喜欢。这就是二分类问题。当然这对我们来说，显得太粗鲁了，要不希望，要不绝望，都不利于身心健康。那如果它可以告诉我，她很喜欢、有一点喜欢、不怎么喜欢或者一点都不喜欢，这就是多分类问题。而且相较于上一个问题来说，也许就会有不同的解决办法。</p><p>下面对二分类问题进行建模：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">health=health[health[<span class="hljs-string">&quot;fetal_health&quot;</span>]!=<span class="hljs-number">3</span>]<span class="hljs-comment">#去掉3，只保留1，2，将之前的多分类问题变成二分类问题</span><br>train=health.drop([<span class="hljs-string">&quot;fetal_health&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,health[<span class="hljs-string">&quot;fetal_health&quot;</span>],random_state=<span class="hljs-number">1</span>)<br>logistic=LogisticRegression(penalty=<span class="hljs-string">&#x27;l1&#x27;</span>,C=<span class="hljs-number">1</span>,solver=<span class="hljs-string">&#x27;liblinear&#x27;</span>)<br>logistic.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic训练模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_train,logistic.predict(X_train))))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logistic待测模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(accuracy_score(y_test,logistic.predict(X_test))))<br></code></pre></td></tr></table></figure><p>得到结果：<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326170539215.png" alt="在这里插入图片描述"><br>下面是我对solver参数适用范围的一个汇总，大家看一下：<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326170714780.png" alt="在这里插入图片描述"><br>这里要说明的一件事就是用于处理多分类的参数也同样可以用来处理二分类，只是效果不会很理想：<br><img src="/2021/06/15/LogisticRegression%E7%AE%97%E6%B3%95/20210326170823579.png" alt="在这里插入图片描述"><br>跟上一个参数所建立的模型相比，可以看出模型精度有所下降。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>目前为止，我们已经用过几个线性算法了（<a href="https://blog.csdn.net/weixin_43580339/article/details/112983192?spm=1001.2014.3001.5501">lasso</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842?spm=1001.2014.3001.5501">ridge</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112271333?spm=1001.2014.3001.5501">最小二乘</a>），这些算法的区别在于以下两点：</p><p>1.系数（w）和截距（b）的特定组合对训练数据拟合好坏的度量方法（损失函数）<br>2.是否使用正则化，以及使用哪种正则化方法。</p><p>不同的算法使用不同的方法来度量“对训练拟合的好坏”。由于数学上的技术原因，不可能调节w和b使得算法产生的误分类数量最少。所以对于我们的目的，以及对于许多应用而言，上面第一点的选择有时并不重要。</p><p>线性模型的一个优点就是训练速度非常快，预测速度也很快。这种模型可以推广到非常大的数据集，对稀疏数据也很有效。如果你的数据包含数十万甚至上百万个样本，你可能需要研究如何使用LogisticRegression和<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842?spm=1001.2014.3001.5501">Ridge</a>模型的solver=”sag”选项，在处理大型数据时，这一选项比默认值要更快。</p><p>线性模型的另一个优点在于，利用我们之前件过得用于分类和回归的公式，理解如何进行预测是相对比较容易的。不幸的是，往往并不完全清楚系数为什么是这样的。如果你的数据集中包含高度相关的特征，这一问题尤为突出。在这种情况下，可能很难对系数做出解释。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分类模型</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lasso回归算法</title>
    <link href="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
    <url>/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>正则化</strong></p><p><strong>正则化</strong>是指对模型做显式约束，以避免过拟合。本文用到的lasso回归就是L1正则化。（从数学的观点来看，lasso惩罚了系数向量的L1范数，换句话说，就是系数的绝对值之和。）</p><p>正则化的具体原理就不在这里多叙述了，感兴趣的朋友可以看一下这篇文章：<a href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a>。</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>lasso回归</strong></p><p>在了解lasso回归之前，建议朋友们先对普通最小二乘法和岭回归做一些了解，可以参考这两篇文章：<a href="https://blog.csdn.net/weixin_43580339/article/details/112271333">最小二乘法-回归实操</a>，<a href="https://blog.csdn.net/weixin_43580339/article/details/112931842">岭回归-回归实操</a>。</p><p>除了岭回归之外，lasso是另一种正则化的线性回归模型，因此它的模型公式与最小二乘法的相同，如下式所示：</p><blockquote><p><em>y=w[0]x[0]+w[1]x[1]+w[2]x[2]+……+w[p]x[p]+b</em></p></blockquote><p>与岭回归相同，使用lasso也是约束系数w使其接近于0，但用到的方法不同，叫做L1正则化。L1正则化的结果是，使用lasso时某些系数刚好是0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选择。某些系数刚好为0，这样模型更容易被理解，也可以呈现模型最重要的特征。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/altavish/boston-housing-dataset">波士顿房价：https://www.kaggle.com/altavish/boston-housing-dataset</a><br><del>非常经典的一个数据</del><br><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210121135007262.jpg" alt="在这里插入图片描述"></p><p>简单解释一下这个数据的几个主要指标：<br>ZN：25,000平方英尺以上的土地划为住宅用地的比例。<br>RM：每个住宅的平均房间数。<br>AGE：1940年之前建造的自有住房的比例<br>CHAS：有没有河流经过 (如果等于1，说明有，等于0就说明没有)<br>CRIM：犯罪率<br>MEDV：住房的价格<br><del>其它指标就不用说了，都是一些住房的其它指标，感兴趣的小伙伴可以自己查一下。</del> </p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> winreg<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Lasso<span class="hljs-comment">###导入岭回归算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块</p><p><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\HousingData.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>house_price=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/altavish/boston-housing-dataset</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。<br><del>其实到这一步都是在走流程，基本上每个数据挖掘都要来一遍，没什么好说的。</del> </p><p><strong>3.清洗数据</strong></p><p><em>1.查找缺失值</em><br><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210121135832795.png" alt="在这里插入图片描述"></p><p>可以看到这个数据并包括一些缺失值，并不是很多，所以直接删掉就好了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">house_price1=house_price.dropna().reset_index()<br><span class="hljs-keyword">del</span> house_price1[<span class="hljs-string">&quot;index&quot;</span>]<br></code></pre></td></tr></table></figure><p><em>2.突变值查找</em></p><p><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210121140155149.png" alt="在这里插入图片描述"><br>一般是看看特征值里面是否包含等于零的数据。其实说的直接一点就是看看数据里面是否包含不符合实际的数值，比如像是犯罪率，实际中不可能出现犯罪率等于0的片区。那么从上面的结果来看，这份数据并没有其它问题。<br><del>这份数据里面的ZN和CHAS都是利用0和1来当作一种指标，所以包含0是很正常的。</del> </p><p><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">train=house_price1.drop([<span class="hljs-string">&quot;MEDV&quot;</span>],axis=<span class="hljs-number">1</span>)<br>X_train,X_test,y_train,y_test=train_test_split(train,house_price1[<span class="hljs-string">&quot;MEDV&quot;</span>],random_state=<span class="hljs-number">23</span>)<br>lasso=Lasso(alpha=<span class="hljs-number">10</span>,max_iter=<span class="hljs-number">0</span>)<br>lasso.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Lasso训练模型得分：&quot;</span>+<span class="hljs-built_in">str</span>(r2_score(y_train,lasso.predict(X_train))))<span class="hljs-comment">#训练集</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Lasso待测模型得分：&quot;</span>+<span class="hljs-built_in">str</span>(r2_score(y_test,lasso.predict(X_test))))<span class="hljs-comment">#待测集</span><br></code></pre></td></tr></table></figure><p>引入lasso算法，进行建模后，对测试集进行精度评分，得到的结果如下：</p><p><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210122140849176.jpg" alt="在这里插入图片描述"><br>如结果所见，lasso在训练集和测试集上的表现很差。这表示存在过拟合。与岭回归类似，lasso也有一个正则化参数alpha，可以控制系数趋向于0的强度。在上一个模型中，我们使用的是alpha=10，为了降低欠拟合，我们尝试减小alpha。同时，我们还需要增加max_iter的值（运行迭代的最大次数）。结果如下所示：<br><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210122141843421.png" alt="在这里插入图片描述"></p><p>依次修改系数之后，可以看到，该模型的训练精度为79%左右，对于新的数据来说，模型精度在60%左右。<br>至此，这个数据集的将建模就算是完成了。<br>ps：如果max_iter取值过小的话，就会出现警告说需要取值取大一点，而且max_iter的取值过大并不会对模型的精度造成影响。</p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p><strong>与岭回归算法的比较</strong></p><p>我们通过变换约束参数的取值，来具体看一下lasso与岭回归的优缺点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Ridge<span class="hljs-comment">###导入岭回归算法</span><br>result=pd.DataFrame(columns=[<span class="hljs-string">&quot;参数&quot;</span>,<span class="hljs-string">&quot;lasso训练模型得分&quot;</span>,<span class="hljs-string">&quot;lasso待测模型得分&quot;</span>,<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>,<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>])<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>):<br>    alpha=i/<span class="hljs-number">10</span><br>    ridge=Ridge(alpha=alpha)<br>    lasso=Lasso(alpha=alpha,max_iter=<span class="hljs-number">10000</span>)<br>    ridge.fit(X_train,y_train)<br>    lasso.fit(X_train,y_train)<br>    result=result.append([&#123;<span class="hljs-string">&quot;参数&quot;</span>:alpha,<span class="hljs-string">&quot;lasso训练模型得分&quot;</span>:r2_score(y_train,lasso.predict(X_train)),<span class="hljs-string">&quot;lasso待测模型得分&quot;</span>:r2_score(y_test,lasso.predict(X_test)),<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>:r2_score(y_train,ridge.predict(X_train)),<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>:r2_score(y_test,ridge.predict(X_test))&#125;])<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210122142156490.png" alt="在这里插入图片描述"></p><p>可以看出，随着alpha的变化，两个算法无论是训练模型还是待测模型都会呈现一定的规律。接下来，我们通过一个折线图来更直观地表现上面的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>plt.style.use(<span class="hljs-string">&quot;fivethirtyeight&quot;</span>)<br>sns.set_style(&#123;<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>:[<span class="hljs-string">&#x27;SimHei&#x27;</span>,<span class="hljs-string">&#x27;Arial&#x27;</span>]&#125;)<span class="hljs-comment">#设定汉字字体，防止出现方框</span><br>%matplotlib inline<br><span class="hljs-comment">#在jupyter notebook上直接显示图表</span><br>fig= plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">5</span>))<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;lasso训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;lasso训练模型得分&quot;</span>)<span class="hljs-comment">#画折线图</span><br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;lasso待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;lasso待测模型得分&quot;</span>)<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>],label=<span class="hljs-string">&quot;岭回归训练模型得分&quot;</span>)<br>plt.plot(result[<span class="hljs-string">&quot;参数&quot;</span>],result[<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>],label=<span class="hljs-string">&quot;岭回归待测模型得分&quot;</span>)<br>plt.rcParams.update(&#123;<span class="hljs-string">&#x27;font.size&#x27;</span>: <span class="hljs-number">15</span>&#125;)<br>plt.legend()<br>plt.xticks(fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的刻度字体大小</span><br>plt.yticks(fontsize=<span class="hljs-number">15</span>)<br>plt.xlabel(<span class="hljs-string">&quot;参数&quot;</span>,fontsize=<span class="hljs-number">15</span>)<span class="hljs-comment">#设置坐标轴上的标签内容和字体</span><br>plt.ylabel(<span class="hljs-string">&quot;得分&quot;</span>,fontsize=<span class="hljs-number">15</span>)<br></code></pre></td></tr></table></figure><p>结果如下所示：<br><img src="/2021/06/15/lasso%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/20210122142449484.png" alt="在这里插入图片描述"></p><p>可以看出如果alpha很小，我们可以拟合一个更复杂的模型，在训练集和测试集上的表现也更好，模型的泛化能力比使用岭回归要略好一点（红线和绿线）。但随着alpha参数的增加，lasso算法模型的欠拟合现象会越来越明显（红线与蓝线），即模型精度和泛化能力都会逐渐降低。</p><p>但如果把alpha设得太小，那么就会消除正则化的效果，并出现过拟合，得到与最小二乘法类似的结果。</p><p>同时还可以看出当alpha取到某一个值的时候，岭回归的预测性能和lasso的模型类似（看两条线的交点）。</p><p>所以在实践中，对于这两个模型一般首选岭回归，从图中就可以看出来，随着参数的变化，模型得分的变化很平稳，甚至随着参数的增加，泛化能力也会有轻微的提高（绿线）。但如果特征很多，你认为只有其中几个是重要的，那么选择lasso可能更好。同样，如果你想要一个更容易解释的模型，lasso可以给出更容易理解的模型，因为它只选择了一部分特征值来做为输入。</p><p>以上就是关于lasso算法的实际操作与看法了，有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>线性模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>回归模型</tag>
      
      <tag>L1正则化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k临近算法-回归</title>
    <link href="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/"/>
    <url>/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>先简单介绍一下机器学习里面的两个概念</p><p><strong>1.分类与回归</strong></p><p>分类模型和回归模型本质一样，分类模型是将回归模型的输出离散化。</p><p>一般来说，回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。回归是对真实值的一种逼近预测。</p><p>分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。</p><p>简言之：</p><p>　　<strong>定量输出称为回归，或者说是连续变量预测，预测明天的气温是多少度，这是一个回归任务<br>定性输出称为分类，或者说是离散变量预测，预测明天是阴、晴还是雨，就是一个分类任务</strong></p><p><strong>2.拟合</strong><br><strong>泛化</strong>：如果一个模型能够对没见过的新数据作出准确预测，我们就能够说它能够从训练集<strong>泛化</strong>到测试集<br><strong>拟合</strong>：模型是否可以很好的描述某些样本，并且有较好的泛化能力<br><strong>欠拟合</strong>：测试样本的特性没有学到，或模型过于简单无法拟合<br><strong>过拟合</strong>：太过贴近于训练数据的特性，在训练集上优秀，但在测试集上不行，不具有泛化性</p><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p><strong>KNN回归</strong></p><p>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的某个（些）属性的平均值赋给该样本，就可以得到该样本对应属性的值。</p><p>knn分类实操可以参考这一篇文章：<a href="https://blog.csdn.net/weixin_43580339/article/details/111628241">k邻近算法-分类实操</a></p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="#https://www.kaggle.com/neuromusic/avocado-prices">鳄梨单价预测（Kaggle）：#https://www.kaggle.com/neuromusic/avocado-prices</a></p><p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231144706634.png" alt="在这里插入图片描述"><br>该数据包含2017年到2019年的鳄梨单价，每次出售的重量，鳄梨种类，产地等信息。<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231144907215.png" alt="在这里插入图片描述"><br>Tips：4046，4225，4770是国外进口水果的plu码，plu四位码代表鳄梨的产地，种类，大小等水果信息，所以4046，4225，4770就是代表三种鳄梨。<br><em><del>有一说一，平常买水果还真没注意到这个。又知道了一个无用的小知识。</del></em> </p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#导入划分数据集的模块</span><br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsRegressor<span class="hljs-comment">#导入knn回归算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score<br></code></pre></td></tr></table></figure><p>老规矩，上来先依次导入建模需要的各个模块，除了前四个库是数据挖掘必要的第三方库之外，重点说一下r2_score：</p><blockquote><p>sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=’uniform_average’)<br>y_true：观测值<br>y_pred：预测值<br>sample_weight：样本权重，默认None<br>multioutput：多维输入输出，可选‘raw_values’, ‘uniform_average’, ‘variance_weighted’或None。默认为’uniform_average’;<br>raw_values：分别返回各维度得分<br>uniform_average：各输出维度得分的平均<br>variance_weighted：对所有输出的分数进行平均，并根据每个输出的方差进行加权。</p></blockquote><p>r2_score评分是主要的回归模型评分方式，具体原理就不多做介绍了，感兴趣的朋友可以查看这篇文章：<a href="https://www.cnblogs.com/jpld/p/12022123.html">深度研究：回归模型评价指标R2_score</a></p><p><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\avocado.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>glass=pd.read_csv(file_origin)<span class="hljs-comment">#https://www.kaggle.com/neuromusic/avocado-prices</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。</p><p><strong>3.清洗数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">avocado.groupby(avocado[<span class="hljs-string">&quot;year&quot;</span>])[<span class="hljs-string">&quot;year&quot;</span>].count()<br>avocado_2017=avocado[avocado[<span class="hljs-string">&quot;year&quot;</span>]==<span class="hljs-number">2017</span>].reset_index()<br></code></pre></td></tr></table></figure><p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231150641784.png" alt="在这里插入图片描述"><br>这里我们选择2017年的数据作为建模数据。<del><em>（有的时候数据量太多反而会造成一些噪音干扰）</em></del> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">avocado_2017=avocado_2017.replace(&#123;<span class="hljs-string">&quot;type&quot;</span>:&#123;<span class="hljs-string">&quot;conventional&quot;</span>:<span class="hljs-number">0</span>&#125;&#125;)<br>avocado_2017=avocado_2017.replace(&#123;<span class="hljs-string">&quot;type&quot;</span>:&#123;<span class="hljs-string">&quot;organic&quot;</span>:<span class="hljs-number">1</span>&#125;&#125;)<br>a=pd.DataFrame(avocado_2017.groupby(avocado_2017[<span class="hljs-string">&quot;region&quot;</span>])[<span class="hljs-string">&quot;region&quot;</span>].count())<br>a[<span class="hljs-string">&quot;replace_num&quot;</span>]=<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(a.index))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(a.index)):<br>    avocado_2017=avocado_2017.replace(&#123;<span class="hljs-string">&quot;region&quot;</span>:&#123;a.index[i]:a.loc[a.index[i],<span class="hljs-string">&quot;replace_num&quot;</span>]&#125;&#125;)<br>    <span class="hljs-comment">###注意这里在利用.loc进行筛选时不能用数字索引进行筛选，因为当前a的行索引是一系列字符串</span><br></code></pre></td></tr></table></figure><p>因为type和region的文本数据有点复杂，所以利用replace函数替换成数字来代表不同的品种和产地。<br><del>其实这个替换并没有什么用，只是博主单纯地看不习惯文本数据而已。</del> </p><p><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231151917738.png" alt="在这里插入图片描述"><br>因为数据中的特征值Total Bags=Small Bags+Large Bags，XLarge Bags=0，且region，type皆与4位plu码有属性重合的情况，所以为了避免出现过拟合，我这里只选取AveragePrice，Total Volume，4046，4225，4770，Small Bags，Large Bags进行预测建模。</p><p><strong>4.建模</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train,X_test,y_train,y_test=train_test_split(avocado_2017[[<span class="hljs-string">&quot;Total Volume&quot;</span>,<span class="hljs-string">&quot;4225&quot;</span>,<span class="hljs-string">&quot;4046&quot;</span>,<span class="hljs-string">&quot;4770&quot;</span>,<span class="hljs-string">&quot;Small Bags&quot;</span>,<span class="hljs-string">&quot;Large Bags&quot;</span>]],avocado_2017[<span class="hljs-string">&quot;AveragePrice&quot;</span>],random_state=<span class="hljs-number">24</span>)<br><span class="hljs-comment">#注意特征值标签要放在前面，预测值标签要放在后面</span><br><span class="hljs-comment">#考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和预测值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">knn=KNeighborsRegressor(n_neighbors=<span class="hljs-number">1</span>)<br>knn.fit(X_train,y_train)<br>prediction=knn.predict(X_test)<br>r2_score(y_test,prediction)<br></code></pre></td></tr></table></figure><p>引入knn算法，并将算法中的邻居值设为1，进行建模后，对测试集进行精度评分，得到的结果如下：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231153653655.png" alt="在这里插入图片描述"><br>可以看到，该模型的精度为50%左右。</p><p><strong>5.简单的调参</strong></p><p>之前设立的邻居参数为1，接下来依次测试不同的参数，看看最优的邻居参数是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">result=&#123;&#125;<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<span class="hljs-comment">#一般n_neighbors的选取低于样本总数的平方根</span><br>    knn=KNeighborsRegressor(n_neighbors=(i+<span class="hljs-number">1</span>))<br>    knn.fit(X_train,y_train)<br>    prediction=knn.predict(X_test)<br>    score=r2_score(y_test,prediction)<br>    result[i+<span class="hljs-number">1</span>]=score*<span class="hljs-number">100</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> result.keys():<br>    <span class="hljs-keyword">if</span> result[i]==<span class="hljs-built_in">max</span>(result.values()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;最佳邻近数：&quot;</span>+<span class="hljs-built_in">str</span>(i))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型评分：&quot;</span>+<span class="hljs-built_in">str</span>(<span class="hljs-built_in">max</span>(result.values())))<br></code></pre></td></tr></table></figure><p>结果如下：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231154159122.jpg" alt="在这里插入图片描述"><br>可以看出在邻近数依次选择1～100的过程中，最佳邻近参数为4；模型的最佳精度评分是68分。（可以看到分数很低，博主认为有可能是算法本身的原因。毕竟是一个简单的算法，而不像森林或者树回归算法一样，可以调整权重等其它参数。）</p><p><strong>6.总结</strong></p><p>1.随着邻近参数的变化，模型精度也会跟随变化，并呈现一定的规律的规律：<br><img src="/2021/06/15/k%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95-%E5%9B%9E%E5%BD%92/20201231154542262.jpg" alt="在这里插入图片描述"></p><p>对于同一个数据集，随着邻近参数的逐渐增加，模型精度往往会到达一个临界点，之后便会逐渐降低。其他的knn回归模型也会呈现这种情况，有兴趣的朋友可以自行检验一下。</p><p>2.回归类算法是预测连续值的算法，如果打算通过对预测标签进行分箱（pd.cut），来对预测标签进行变化范围的预测是行不通的。ps:博主已经尝试过，会报错误。</p><p>3.对于不同类型的数据要选择不同类型的算法，每个算法都有各自的优缺点，并没有可以解决所有问题的算法，所以在以后的建模中不要钻牛角尖，要注意选择。</p><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>k邻近</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>回归模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k邻近算法-分类</title>
    <link href="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/"/>
    <url>/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p>k邻近算法是最简单的算法之一，该算法的思路是：在特征空间中，如果一个样本附近的k个最近(即特征空间中最邻近)样本的大多数属于某一个类别，则该样本也属于这个类别。该算法主要用于解决分类问题，无论是二分类或者是多分类都可以应用。<br>本文主要记录实际操作，想了解具体的算法原理可以自己查找，或者可以参考这篇博客：<a href="https://www.cnblogs.com/pinard/p/6061661.html">KNN原理小结</a></p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><a href="https://www.kaggle.com/uciml/glass">玻璃分类（Kaggle）：https://www.kaggle.com/uciml/glass</a></p><p><img src="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/20201224115620339.jpg" alt="在这里插入图片描述"><br>该数据按照玻璃的组成成分(RI,Na,Mg,Al,SI,K,Ca,Ba,Fe)的不同划分了7种玻璃，每种玻璃（Type)以数字1~7来命名。<br>依照这些数据建立一个knn多分类模型。</p><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><strong>1.导入第三方库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<span class="hljs-comment">#划分训练集和测试集</span><br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<span class="hljs-comment">#导入knn算法</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<span class="hljs-comment">#导入分类评分标准</span><br></code></pre></td></tr></table></figure><p>依次导入建模需要的各个模块，除了前四个库是数据挖掘必要的第三方库之外，重点说一下accuracy_score：</p><blockquote><p><em><strong>sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)<br>normalize：默认值为True，返回正确分类的比例；如果为False，返回正确分类的样本数</strong></em></p></blockquote><p>分类准确率分数是指所有分类正确的百分比。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。不过我只是简单地建立一个多分类模型，够用了。</p><p><strong>2.读取文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> winreg<br>real_address = winreg.OpenKey(winreg.HKEY_CURRENT_USER,<span class="hljs-string">r&#x27;Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders&#x27;</span>,)<br>file_address=winreg.QueryValueEx(real_address, <span class="hljs-string">&quot;Desktop&quot;</span>)[<span class="hljs-number">0</span>]<br>file_address+=<span class="hljs-string">&#x27;\\&#x27;</span><br>file_origin=file_address+<span class="hljs-string">&quot;\\源数据-分析\\glass.csv&quot;</span><span class="hljs-comment">#设立源数据文件的桌面绝对路径</span><br>glass=pd.read_csv(file_origin)<span class="hljs-comment">#读取csv文件</span><br></code></pre></td></tr></table></figure><p>因为之前每次下载数据之后都要将文件转移到python根目录里面，或者到下载文件夹里面去读取，很麻烦。所以我通过winreg库，来设立绝对桌面路径，这样只要把数据下载到桌面上，或者粘到桌面上的特定文件夹里面去读取就好了，不会跟其它数据搞混。</p><p><strong>3.建模</strong></p><p><img src="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/20201224135351342.jpg" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">y=<span class="hljs-built_in">list</span>(glass.columns)[:-<span class="hljs-number">1</span>]<br>X_train,X_test,y_train,y_test=train_test_split(glass[y],glass[<span class="hljs-string">&quot;Type&quot;</span>],random_state=<span class="hljs-number">1</span>)<br><span class="hljs-comment">#考虑到接下来可能需要进行其他的操作，所以定了一个随机种子，保证接下来的train和test是同一组数</span><br></code></pre></td></tr></table></figure><p>划分列索引为特征值和目标分类值，并将数据划分成训练集和测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">knn=KNeighborsClassifier(n_neighbors=<span class="hljs-number">1</span>)<br>knn.fit(X_train,y_train)<br></code></pre></td></tr></table></figure><p>引入knn算法，并将算法中的邻居值设为1，并进行建模，得到结果。<br><img src="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/2020122414004156.jpg" alt="在这里插入图片描述"></p><p><strong>4.评分</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">prediction=knn.predict(X_test)<span class="hljs-comment">#对测试值进行预测</span><br>accuracy_score(y_test,prediction)<span class="hljs-comment">#对结果进行评分</span><br></code></pre></td></tr></table></figure><p>利用knn.predict来对测试值进行预测，并与之前划分的测试标签进行比较评分，来看看模型准确度。<br><img src="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/20201224140714389.jpg" alt="在这里插入图片描述"><br>结果为0.72,也就是说测试结果与预测结果有72%是符合的，或者说该模型准度评分为72分。</p><p><strong>5.简单的调参</strong><br>之前设立的邻居参数为1，接下来依次测试不同的参数，看看最优的邻居参数是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">result=&#123;&#125;<span class="hljs-comment">#通过字典来记录每次的参数及对应的评分结果</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<span class="hljs-comment">#参数依次从1取到20</span><br>    knn=KNeighborsClassifier(n_neighbors=(i+<span class="hljs-number">1</span>))<br>    knn.fit(X_train,y_train)<br>    prediction=knn.predict(X_test)<br>    score=accuracy_score(y_test,prediction)<br>    result[i+<span class="hljs-number">1</span>]=score*<span class="hljs-number">100</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> result.keys():<br>    <span class="hljs-keyword">if</span> result[i]==<span class="hljs-built_in">max</span>(result.values()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;最佳邻近数：&quot;</span>+<span class="hljs-built_in">str</span>(i))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型评分&quot;</span>+<span class="hljs-built_in">str</span>(<span class="hljs-built_in">max</span>(result.values())))<br></code></pre></td></tr></table></figure><p>结果如下：<br><img src="/2021/06/15/k%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95-%E5%88%86%E7%B1%BB/20201224142437770.jpg" alt="在这里插入图片描述"><br>可以看出在参数依次选择1～20的过程中，最佳邻近参数为1，4，5；模型的最佳精度评分是72分。</p><p><strong>6.总结</strong><br>其实整个过程下来，与其说是在建模，倒不如说是体会了一遍建模的流程。而且k邻近算法不是很复杂的算法，暂时也没有涉及到数据标准化或者归一化的一些操作，不过还是有一些问题可以讨论：</p><blockquote><p>1.这个算法可不可以用于文本类数据的分类，如果可以的话，如何使用？<del>（我之前尝试了一些文本类的数据，但是都会报错，不管是将数据类型转换成float，还是将文本元素转换为ASC码，都不能成功建模）</del></p><p>2.knn算法邻近参数越小精度越高吗？对于其它数据的泛化能力呢？<br>3.有没有更全面的调参方法来提高模型精度？<br>4.knn算法可以用于回归类问题吗？</p></blockquote><p>有很多地方做的不是很好，欢迎网友来提出建议，也希望可以遇到些朋友来一起交流讨论。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>监督学习</category>
      
      <category>k邻近</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分类模型</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建</title>
    <link href="/2021/06/14/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <url>/2021/06/14/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h4 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h4><p>目前网络上有两种主流的搭建方法,一个是<strong>jekyll+github</strong>,另一个是<strong>hexo+github</strong>,这两种框架均是利用静态网页编辑器将生成的静态网页上传到github上,从而实现网页展示,跳转等功能.</p><p>本网站是采用hexo+github的方法搭建的,过程很简单,哪怕是完全不懂java,js的小白也可以轻松搭建出来.网络上有很多关于利用hexo搭建博客的文章,譬如:<a href="https://blog.csdn.net/qq_43270074/article/details/95392429">搭建GitHub免费个人网站（详细教程）</a>,有需要的朋友自行百度下就好了.</p><p>不过,虽然搭建过程很简单,但是里面的小坑是真不少,接下来我会简单说一下搭建过程中遇到的问题.</p><ol><li><p>有很多hexo主题的导航页并没有完全设置好,像是标签(tag),分类(catagories)这些属性的设立有可能是缺失的,这个时候就需要我们手动设置一下.具体过程可以参考这篇文章:<a href="https://blog.csdn.net/nineya_com/article/details/103322773">hexo博客添加标签、分类、归档、关于等页面</a>.</p></li><li><p>由于我们的博客是利用markdown编辑器生成,然后上传到github的,所以如果文档中涉及到图片插入的话,在网页展示中是<strong>需要设立正确的图片路径来去引用图片</strong>的.但如果每张图片都去设立引用路径的话是很麻烦的,而且一旦设立错误就会让<strong>本该展示图片的地方变成空白,即无法引用图片</strong>.这个问题的解决方法就是通过typora(markdown编辑器)来去自动复制并生成图片引用路径,具体方法可以参考这篇文章:<a href="https://blog.csdn.net/ayuayue/article/details/109198493">hexo 图片显示问题及使用typora设置图片路径</a>.</p></li><li><p>如果algolia搜索功能出现问题的话,可以参考<a href="https://blog.csdn.net/qq_35479468/article/details/107335663">为Hexo增加algolia搜索功能</a>.</p><p>除此之外,还要注意两点:</p><p>第一点:每次编写完新文章,并且重新部署之后,需要输入hexo algolia指令,来去更新algolia网站的新文章索引.</p><p>第二点:修改blog根目录下config_yml文件里面的默认url选项,要修改成项目所在的url网址(如</p><p><a href="https://yb705.github.io/">https://yb705.github.io</a>).</p></li><li><p>评论插件valine本身如果并没有什么问题,但是评论框没有显示的话,那么有可能是api和key设置错了,具体参数设置可以参考官方文档:<a href="https://valine.js.org/configuration.html">https://valine.js.org/configuration.html</a></p><p>PS:刚开始使用valine插件的时候,不显示评论框,博主调整了好久都没有成功,但是在leadcloud上新建了一个app之后,莫名其妙地就有了… ㄟ( ▔, ▔ )ㄏ</p></li><li><p>在博客搭建过程中,我们修改最多的就是blog根目录和theme目录下的<strong>config_yml</strong>文件.这个文件是遵从yml语言格式的,所以当我们在做属性修改的时候,一定要<strong>注意冒号后面加一个空格</strong>, 不然在网页部署的时候就会报错.这里建议用<strong>sumlime</strong>代码编辑器来去编辑,因为yml里面的属性会被不同的颜色标记出来,这样方便大家及时发现错误.当然我们也可以记事本来编辑,只是如果出现错误,会很难发现是哪里出现的问题.</p></li></ol><p>其实博客搭建的本质就是用markdown编辑器编写完文章之后,将文章上传托管到github的博客项目上,并自动生成url链接网址与gitpage,然后通过点击特定组合的url网址来去跳转各种不同的静态网页.只要了解了这个流程,以后再遇到什么问题便都有个大概的思考方向,通过检查网页源码便能解决大部分问题,而不会两眼一抹黑,或者是到网上硬搜答案,有时甚至还什么都没搜出来┑(￣Д ￣)┍.</p>]]></content>
    
    
    <categories>
      
      <category>博客搭建</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客搭建</tag>
      
      <tag>注意事项</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一篇博客</title>
    <link href="/2021/06/14/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2021/06/14/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h4 id="第一篇博客"><a href="#第一篇博客" class="headerlink" title="第一篇博客"></a>第一篇博客</h4><p> 从21年年初,我开始自学数据分析,数据挖掘方面的东西.为了能够坚持下去,同时也是想要将学到的东西记录下来,我便给自己定了一个规矩:每周至少要更新一篇博客.开始的时候是在CSDN和知乎上面更新,毕竟是知名技术网站,而且编辑器什么的也都是搭建好的,撰写发布很方便.</p><p>可时间久了,就发现这两个网站的markdown编辑器还是有一些限制的,譬如说标题,字体等种类很少,布局简陋等等.在加上自己平常有写日记的习惯,所以搭建一个属于自己的博客网站便提上了日程.最终,在经历了7个多小时的折磨之后,本网站就此诞生了.</p><p>以后,我会在CSDN和个人博客同步更新技术博客,偶尔也会在个人博客上分享一些生活的事情.</p><p>希望可以和各路大佬多多交流.</p>]]></content>
    
    
    <categories>
      
      <category>第一篇博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活记录</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
